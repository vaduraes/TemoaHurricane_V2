{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run this code use environment: geo_env\n",
    "#should take between 5-10min to run\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from haversine import haversine_vector, Unit, haversine\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "from shapely.ops import unary_union\n",
    "import sys\n",
    "import shutil\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import Image\n",
    "import glob\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "import sqlite3\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "sys.path.insert(0, './InputData/FragilityCurvesData/') #Fragility curves module\n",
    "from fragility_curves import fragility\n",
    "\n",
    "\n",
    "State=\"NC\"\n",
    "StatesSHP=\"./InputData/US_State_Boundaries/us-state-boundaries.shp\"\n",
    "\n",
    "GeneratorDataPath=\"./InputData/EIAData/february_generator2023.xlsx\"\n",
    "HazusDataPath=\"./InputData/HazusData/Hazus_NC_Hurricane.shp\"\n",
    "\n",
    "UserDataPath=\"./InputData/ExcelUserData/\"\n",
    "CensusDataPath=\"./InputData/CensusData/US_tract_cenpop_2020.shp\" # https://data2.nhgis.org/downloads\n",
    "\n",
    "EIA_930_BalancePath=\"./InputData/EIAData/EIA-930-Balance/\" # https://www.eia.gov/survey/#eia-930, Used for demand, ignore COVID years\n",
    "EIA_930_CFPath=\"./InputData/EIAData/EIA-930-Generation/\" # https://www.eia.gov/survey/#eia-930, Used for CFs include COVID years\n",
    "\n",
    "EIA_923_Path=\"./InputData/EIAData/EIA923_Schedules_2_3_4_5_M_12_2021_Final_Revision.xlsx\" # Fuel consumption and generation by plant\n",
    "EIA_923_Path_2020=\"./InputData/EIAData/EIA923_Schedules_2_3_4_5_M_12_2020_Final_Revision.xlsx\" # Fuel consumption and generation by plant\n",
    "\n",
    "BiomassDataFromDOE_Path=\"./InputData/ExcelUserData/Biomass/billionton_state_download20230630-115258.csv\" #Needs to be filtered for the State you are interesd\n",
    "\n",
    "MajorBAs=[\"DUK\", \"CPLE\"] #Major BA's in the region according to EIA-930 notation\n",
    "\n",
    "SAM_NREL_Path=\"./InputData/NREL_SAM/\"\n",
    "\n",
    "HydroDataPath=\"./InputData/EIAData/ORNL_EHAHydroPlant_FY2020revised.xlsx\" #most hydroplants in US with average generation and general data\n",
    "NCAverageStatisticsPath=\"./InputData/EIAData/NC Energy Average Data Statistics EIA.xlsx\" #NC average statistics from >2022 \n",
    "\n",
    "FragilityCurvesDataPath=\"./InputData/FragilityCurvesData/\" #Fragility curves for each type of building\n",
    "\n",
    "ConversionCPIRate=1.16 #Rate Used to convert values on excel tables to 2023 dollars (Our table is in 2020 dolars) -Jan 2020 to Jan 2023\n",
    "\n",
    "Vintages=np.arange(2022,1900,-1) #Exisiting years. [2022 to 2020), [2020 to 2018) Needs to start from earlier date to later date\n",
    "\n",
    "#For us to set a limit in the CO2 emissions by 2050 we need the future years to go to 2055\n",
    "FutureYears=[2023]+list(np.arange(2025,2056,5))#2023 to 2056\n",
    "#FutureYears=[2023,2025,2030,2045,2050,2055]\n",
    "\n",
    "\n",
    "Seasons=[\"S1\",\"S2\",\"S3\",\"S4\"] # Winter, Spring, Summer, Fall\n",
    "SeasonsMonthRange=[[12,1,2],[3,4,5],[6,7,8],[9,10,11]] #Months of each season [Start, End]\n",
    "NumHoursSeason={\"S1\":90.25*24,\"S2\":92*24,\"S3\":92*24,\"S4\":91*24} #Number of hours in each season\n",
    "\n",
    "\n",
    "TimeOfDay=[\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\",\"T8\",\"T9\",\"T10\",\"T11\",\"T12\",\"T13\",\"T14\",\"T15\",\"T16\",\"T17\",\"T18\",\"T19\",\"T20\",\"T21\",\"T22\",\"T23\",\"T24\"] # 24 hours of the day, cant be changed\n",
    "\n",
    "UserScenario=\"Conservative\" # (Conservative, Moderate, Advanced) from ATB cost perspectives\n",
    "RegionsDefinition=[\"R1\",\"R2\",\"R3\"]\n",
    "RegionNotes=[\"East\",\"Center\",\"West\"]\n",
    "\n",
    "SaveSqlName=\"NC_EnergySystem2023\"\n",
    "\n",
    "# #Assign scenarios to the stochastic model\n",
    "S2={\"Name\": \"High Impact Hurricane\",\n",
    "    \"HazusCodes\": [\"f200yr\",\"f500yr\",\"f1000yr\"],\n",
    "    }\n",
    "\n",
    "S1={\"Name\": \"Medium Impact Hurricane\",\n",
    "    \"HazusCodes\":[\"f100yr\",\"f50yr\"],\n",
    "    }\n",
    "\n",
    "S0={\"Name\": \"Low/No Impact Hurricane\",\n",
    "    \"HazusCodes\":[\"f10yr\",\"f20yr\"],\n",
    "    }\n",
    "\n",
    "Scenarios={\"S0\":S0,\"S1\":S1,\"S2\":S2}\n",
    "\n",
    "##Assign scenarios to the stochastic model\n",
    "# S1={\"Name\": \"High Impact Hurricane\",\n",
    "#     \"HazusCodes\": [\"f500yr\",\"f1000yr\"]\n",
    "#     }\n",
    "\n",
    "# S0={\"Name\": \"Low/No Impact Hurricane\",\n",
    "#     \"HazusCodes\":[\"f50yr\",\"f100yr\",\"f200yr\"]\n",
    "#     }\n",
    "\n",
    "# Scenarios={\"S0\":S0,\"S1\":S1}\n",
    "\n",
    "#Number of hours to represent in the model (sometime it is necessary to reduce the number of hours to control memory use in the stochastic model)\n",
    "NumHoursAjust={\"tm1\":[\"T1\"],\n",
    "                \"tm2\":[\"T2\"],\n",
    "                \"tm3\":[\"T3\"],\n",
    "                \"tm4\":[\"T4\"],\n",
    "                \"tm5\":[\"T5\"],\n",
    "                \"tm6\":[\"T6\"],\n",
    "                \"tm7\":[\"T7\"],\n",
    "                \"tm8\":[\"T8\"],\n",
    "                \"tm9\":[\"T9\"],\n",
    "                \"tm10\":[\"T10\"],\n",
    "                \"tm11\":[\"T11\"],\n",
    "                \"tm12\":[\"T12\"],\n",
    "                \"tm13\":[\"T13\"],\n",
    "                \"tm14\":[\"T14\"],\n",
    "                \"tm15\":[\"T15\"],\n",
    "                \"tm16\":[\"T16\"],\n",
    "                \"tm17\":[\"T17\"],\n",
    "                \"tm18\":[\"T18\"],\n",
    "                \"tm19\":[\"T19\"],\n",
    "                \"tm20\":[\"T20\"],\n",
    "                \"tm21\":[\"T21\"],\n",
    "                \"tm22\":[\"T22\"],\n",
    "                \"tm23\":[\"T23\"],\n",
    "                \"tm24\":[\"T24\"]}\n",
    "\n",
    "#----- Tasks ------\n",
    "\n",
    "\n",
    "#----- Extra Tasks (in progress need adjustments) ------\n",
    "#1. Define transmission/distribution system costs for each region (Investment Fixed and Variable)\n",
    "#   Need to verify Costs and existing capacity values - distribution is with no cost values and transmission/distribution is with no existing capacity values\n",
    "                 \n",
    "#----- Tasks next ------ P2\n",
    "\n",
    "#Do later\n",
    "#Interconnection transfers \n",
    "#Energy storage available on existing pumped hydro\n",
    "\n",
    "#Percentage residential and commercial solar existing cap (https://www.seia.org/state-solar-policy/north-carolina-solar)\n",
    "\n",
    "#----- Observations ------\n",
    "#Storage 24h representative day, loop on the season instead the year\n",
    "\n",
    "####################################-------------------------------------####################################\n",
    "################## Important code modifications  :) ##################\n",
    "####################################-------------------------------------####################################\n",
    "\n",
    "#1. for Emission limit regions can be set as R1+R2+R3 where the sum of the emissions in all regions is limited by emis_limit\n",
    "#Need to implement this in the temoa code (do it outside the stochatic model first on the  OEO model)\n",
    "#2. Same problem of 1. with PlanningReserveMargin\n",
    "\n",
    "#Min/Max activity or capacity for biomass and solar and wind\n",
    "#Alternative biomass max at 2% of total demand independent of the period?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Convert Excel Table to Sql data Part 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExistingCapacity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generation data from: Preliminary Monthly Electric Generator Inventory (based on Form EIA-860M as a supplement to Form EIA-860)\n",
    "\n",
    "#Read File\n",
    "df = pd.read_excel(GeneratorDataPath, sheet_name ='Operating',skiprows=2)\n",
    "InState=df[\"Plant State\"]==State #Filter for NC\n",
    "df=df[InState].reset_index()\n",
    "\n",
    "#NameplateCapacity should be larger than or equal to NetSummerCapacity and NetWinterCapacity\n",
    "NC=df[\"Nameplate Capacity (MW)\"]\n",
    "NSC=df[\"Net Summer Capacity (MW)\"]\n",
    "NWC=df[\"Net Winter Capacity (MW)\"]\n",
    "\n",
    "df[\"Nameplate Capacity (MW)\"]=np.max(np.vstack((NC,NSC,NWC)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On EIA 860 combined cycle tech are divided in CT and CA (steam part)\n",
    "#We need to combine them as CC: combine cycle\n",
    "df.loc[df[\"Prime Mover Code\"]==\"CT\",\"Prime Mover Code\"]=\"CC\"\n",
    "df.loc[df[\"Prime Mover Code\"]==\"CA\",\"Prime Mover Code\"]=\"CC\"\n",
    "df.loc[df[\"Prime Mover Code\"]==\"CT\",\"Technology\"]=\"Combined cycle\"\n",
    "df.loc[df[\"Prime Mover Code\"]==\"CA\",\"Technology\"]=\"Combined cycle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Some plants may have multiple generators commissioned at different times\n",
    "#   we are aggregating the capacity of these generators per year\n",
    "\n",
    "#   We can have multiple vintages (years) of the same plant. As generators may be comissioned at different times\n",
    "df_AggregateGen = pd.DataFrame(columns=[\"PlantID\",\"SourceCode\", \"MoverCode\", \"OperatingYear\",\"NameplateCapacity(MW)\",\"NetSummerCapacity(MW)\",\"NetWinterCapacity(MW)\",\n",
    " \"NameplateEnergyCapacity(MWh)\", \"Latitude\",\"Longitude\",\"Technology\"])\n",
    "\n",
    "for ID in df[\"Plant ID\"].unique().astype(int):\n",
    "    #Same Plant data\n",
    "    SameIds   = df[\"Plant ID\"]==ID\n",
    "    df_tmp1    = df[SameIds]\n",
    "\n",
    "    SourceCode=df_tmp1[\"Energy Source Code\"]\n",
    "    #Same Energy Source Code data\n",
    "    for UniqueSC in SourceCode.unique():\n",
    "        SameIds    = df_tmp1[\"Energy Source Code\"]==UniqueSC\n",
    "        df_tmp2    = df_tmp1[SameIds]\n",
    "\n",
    "        MoverCode=df_tmp2[\"Prime Mover Code\"]\n",
    "        #Same Mover Code data\n",
    "        for UniqueMC in MoverCode.unique():\n",
    "            SameIds    = df_tmp2[\"Prime Mover Code\"]==UniqueMC\n",
    "            df_tmp3    = df_tmp2[SameIds]\n",
    "\n",
    "            OperatingYear=df_tmp3[\"Operating Year\"]\n",
    "            OperatingYear=OperatingYear.sort_values(ascending=False)\n",
    "            #Same Year Code data\n",
    "            for UniqueY in OperatingYear.unique():\n",
    "                \n",
    "                SameIds    = df_tmp3[\"Operating Year\"]==UniqueY\n",
    "                df_tmp4    = df_tmp3[SameIds].reset_index()\n",
    "\n",
    "                if UniqueMC!=\"BA\":\n",
    "                    NameplateEnergyCapacity=np.sum(df_tmp4[\"Nameplate Energy Capacity (MWh)\"])\n",
    "                    NameplateCapacity=np.sum(df_tmp4[\"Nameplate Capacity (MW)\"])\n",
    "                    NetSummerCapacity=np.sum(df_tmp4[\"Net Summer Capacity (MW)\"])\n",
    "                    NetWinterCapacity=np.sum(df_tmp4[\"Net Winter Capacity (MW)\"])\n",
    "\n",
    "                    Latitude  = df_tmp4[\"Latitude\"].iloc[0]\n",
    "                    Longitude = df_tmp4[\"Longitude\"].iloc[0]\n",
    "                    Technology= df_tmp4[\"Technology\"].iloc[0]\n",
    "\n",
    "                    NewPlantID = UniqueSC + \"_\" + UniqueMC + \"_\" + str(ID) \n",
    "\n",
    "                    Data=[[NewPlantID, UniqueSC, UniqueMC, UniqueY, NameplateCapacity, NetSummerCapacity, NetWinterCapacity,NameplateEnergyCapacity,\n",
    "                    Latitude,Longitude,Technology]]\n",
    "\n",
    "                    df_AggregateGen=pd.concat([df_AggregateGen,pd.DataFrame(Data,columns=df_AggregateGen.columns)],ignore_index=True)\n",
    "                else:\n",
    "                    NameplateEnergyCapacity=df_tmp4[\"Nameplate Energy Capacity (MWh)\"]\n",
    "                    NameplateCapacity=df_tmp4[\"Nameplate Capacity (MW)\"]\n",
    "                    NetSummerCapacity=df_tmp4[\"Net Summer Capacity (MW)\"]\n",
    "                    NetWinterCapacity=df_tmp4[\"Net Winter Capacity (MW)\"]\n",
    "\n",
    "                    Latitude  = df_tmp4[\"Latitude\"].iloc[0]\n",
    "                    Longitude = df_tmp4[\"Longitude\"].iloc[0]\n",
    "                    Technology= df_tmp4[\"Technology\"].iloc[0]\n",
    "\n",
    "                    NewPlantID = UniqueSC + \"_\" + UniqueMC + \"_\" + str(ID) \n",
    "\n",
    "                    for k in range(len(df_tmp4)):\n",
    "                        Data=[[NewPlantID, UniqueSC, UniqueMC, UniqueY, NameplateCapacity.iloc[k], NetSummerCapacity.iloc[k], NetWinterCapacity.iloc[k],\n",
    "                                NameplateEnergyCapacity.iloc[k],Latitude,Longitude,Technology]]\n",
    "\n",
    "                        df_AggregateGen=pd.concat([df_AggregateGen,pd.DataFrame(Data,columns=df_AggregateGen.columns)],ignore_index=True)                    \n",
    "\n",
    "#Delete technologies that are after the maximum vintage period\n",
    "df_AggregateGen=df_AggregateGen[df_AggregateGen[\"OperatingYear\"]<=np.max(Vintages)]\n",
    "\n",
    "#Check for missing data\n",
    "if sum(df_AggregateGen[df_AggregateGen[\"MoverCode\"]==\"BA\"][\"NameplateEnergyCapacity(MWh)\"]==' '):\n",
    "    EliminateIdx=(df_AggregateGen[\"MoverCode\"]==\"BA\") * (df_AggregateGen[\"NameplateEnergyCapacity(MWh)\"]==' ')\n",
    "    EliminateId=df_AggregateGen[EliminateIdx][\"PlantID\"]\n",
    "    print(\"Check Battery Data, some elements do not have NameplateEnergyCapacity(MWh) and were removed:\\n Plant ID:\")\n",
    "    [print(EliminateId.iloc[i]) for i in range(EliminateId.shape[0])]\n",
    "\n",
    "    df_AggregateGen=df_AggregateGen[~EliminateIdx]\n",
    "\n",
    "df_AggregateGen = df_AggregateGen.reset_index(drop=True)\n",
    "\n",
    "df_AggregateGen.to_excel(UserDataPath+\"Temporary_ToHelpBuildDecks/TMP_ExistingCapacity_UniqueTechNameLv1.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PlotHurricaneSpeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shapefile_Hazus = gpd.read_file(HazusDataPath)\n",
    "Polygons=[shapefile_Hazus.iloc[j][\"geometry\"] for j in range(shapefile_Hazus.shape[0])]\n",
    "HazusWind_100y=[shapefile_Hazus.iloc[j][\"f100yr\"] for j in range(shapefile_Hazus.shape[0])]\n",
    "HazusWind_1000y=[shapefile_Hazus.iloc[j][\"f1000yr\"] for j in range(shapefile_Hazus.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PlotHurricaneSpeed(HazusWind,Polygons,L_Wind=np.min(HazusWind_100y),M_Wind=np.max(HazusWind_1000y), SingleRange=True,HideCbar=True, FileName=\"HurricaneSpeedY100\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import numpy as np\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    count=0\n",
    "    if SingleRange==True:\n",
    "        norm = mpl.colors.Normalize(vmin=L_Wind, vmax=M_Wind)\n",
    "    else:\n",
    "        norm = mpl.colors.Normalize(vmin=np.min(HazusWind), vmax=np.max(HazusWind))\n",
    "    \n",
    "    cmap = mpl.colormaps[\"jet\"]\n",
    "    \n",
    "    for Poly in Polygons:    \n",
    "        \n",
    "        if Poly.geom_type == 'Polygon':\n",
    "            \n",
    "            xs, ys = Poly.exterior.xy    \n",
    "            ax.fill(xs, ys, alpha=0.5, color=cmap(norm(HazusWind[count])), edgecolor='black', linewidth=0.2)\n",
    "            count=count+1\n",
    "            \n",
    "        if Poly.geom_type == 'MultiPolygon':\n",
    "            for Poly2 in list(Poly.geoms):   \n",
    "                xs, ys = Poly2.exterior.xy\n",
    "                ax.fill(xs, ys, alpha=0.5, color=cmap(norm(HazusWind[count])), edgecolor='black', linewidth=0.2)\n",
    "            count=count+1\n",
    "        \n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    #Colorbar\n",
    "    if HideCbar==False:\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cb_ax = fig.add_axes([.91,.124,.04,.754])\n",
    "        \n",
    "        \n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])\n",
    "        if SingleRange==True:\n",
    "            ticks=np.linspace(L_Wind,M_Wind,10)\n",
    "        else:\n",
    "            ticks=np.linspace(np.min(HazusWind),np.max(HazusWind),10)\n",
    "        \n",
    "        ticks=[np.ceil(ticks[i]) if i==0 else np.floor(ticks[i]) for i in range(len(ticks))]\n",
    "    \n",
    "        clb=fig.colorbar(sm,cax=cb_ax, ticks=ticks,anchor=(0,-1))\n",
    "        clb.ax.set_title('Wind Speed \\n[mph]',size=10)\n",
    "\n",
    "    \n",
    "    plt.savefig(\"OutputData/Figures/\"+FileName+\".png\",dpi=600,bbox_inches='tight')\n",
    "    return fig, ax\n",
    "\n",
    "fig1, ax1=PlotHurricaneSpeed(HazusWind_100y,Polygons,SingleRange=True,FileName=\"HurricaneSpeedY100\")\n",
    "fig2, ax2=PlotHurricaneSpeed(HazusWind_1000y,Polygons,SingleRange=True,HideCbar=False,FileName=\"HurricaneSpeedY1000\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create regions of interest based on the Hazus data and aggregate the capacity of the generators in each region\n",
    "\n",
    "shapefile_Hazus = gpd.read_file(HazusDataPath)\n",
    "Hazus1000=shapefile_Hazus.f1000yr\n",
    "\n",
    "#Define regions of interest\n",
    "Region1Idx=Hazus1000>=130 # Cat 4 hurricane and above 130mph\n",
    "Region2Idx=(Hazus1000>=111)*(Hazus1000<130) # Cat 3 hurricane 111-129mph\n",
    "Region3Idx=Hazus1000<111 # Cat 1-2 hurricane and bellow 110mph\n",
    "\n",
    "Region1Data=shapefile_Hazus[Region1Idx].reset_index()\n",
    "Region2Data=shapefile_Hazus[Region2Idx].reset_index() \n",
    "Region3Data=shapefile_Hazus[Region3Idx].reset_index()     \n",
    "\n",
    "new_polyR1 = unary_union([Region1Data.iloc[j][\"geometry\"] for j in range(Region1Data.shape[0])])\n",
    "new_polyR2 = unary_union([Region2Data.iloc[j][\"geometry\"] for j in range(Region2Data.shape[0])])\n",
    "new_polyR3 = unary_union([Region3Data.iloc[j][\"geometry\"] for j in range(Region3Data.shape[0])])\n",
    "\n",
    "df_AggregateGen[\"Region\"]=''\n",
    "#Assign region for each generation \n",
    "\n",
    "for i in range(df_AggregateGen.shape[0]):\n",
    "    Latitude=df_AggregateGen.iloc[i][\"Latitude\"]\n",
    "    Longitude=df_AggregateGen.iloc[i][\"Longitude\"]\n",
    "\n",
    "    if new_polyR1.contains(Point(Longitude,Latitude)):\n",
    "        df_AggregateGen.at[i,\"Region\"]='R1'\n",
    "\n",
    "    elif new_polyR2.contains(Point(Longitude,Latitude)):\n",
    "        df_AggregateGen.at[i,\"Region\"]='R2'\n",
    "    \n",
    "    elif new_polyR3.contains(Point(Longitude,Latitude)):\n",
    "        df_AggregateGen.at[i,\"Region\"]='R3'\n",
    "\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "\n",
    "if sum(df_AggregateGen.Region=='')==0:\n",
    "    print(\"All existing generators were properly mapped\")\n",
    "else:\n",
    "    print(\"Some generators were no properly mapped to the Hazus data\\\n",
    "    check Hazus and the location of generators\")\n",
    "\n",
    "df_AggregateGen3 = pd.DataFrame(columns=[\"PlantID\", \"SourceCode\", \"MoverCode\", \"Vintage\",\"NameplateCapacity(MW)\",\"NetSummerCapacity(MW)\",\"NetWinterCapacity(MW)\",\n",
    "\"NameplateEnergyCapacity(MWh)\",\"Technology\",\"Region\"])\n",
    "\n",
    "\n",
    "for Region in [\"R1\",\"R2\",\"R3\"]:\n",
    "\n",
    "    df_tmp1=df_AggregateGen[df_AggregateGen[\"Region\"]==Region]\n",
    "\n",
    "    SourceCode=df_tmp1[\"SourceCode\"]\n",
    "    #Same Energy Source Code data\n",
    "    for UniqueSC in SourceCode.unique():\n",
    "        SameIds    = df_tmp1[\"SourceCode\"]==UniqueSC\n",
    "        df_tmp2    = df_tmp1[SameIds]\n",
    "\n",
    "        MoverCode=df_tmp2[\"MoverCode\"]\n",
    "        #Same Mover Code data\n",
    "        for UniqueMC in MoverCode.unique():\n",
    "            SameIds    = df_tmp2[\"MoverCode\"]==UniqueMC\n",
    "            df_tmp3    = df_tmp2[SameIds]\n",
    "            \n",
    "            OperatingYear=df_tmp3[\"OperatingYear\"]\n",
    "            for i in range(len(Vintages[0:-1])):\n",
    "\n",
    "                IdxsIn=(OperatingYear<=Vintages[i]+(Vintages[0]-Vintages[1])/2) * (OperatingYear>Vintages[i]-(Vintages[0]-Vintages[1])/2)\n",
    "                if sum(IdxsIn)!=0:\n",
    "                    if UniqueMC!=\"BA\":\n",
    "                        df_tmp4    = df_tmp3[IdxsIn].reset_index()    \n",
    "\n",
    "                        NameplateCapacity=np.sum(df_tmp4[\"NameplateCapacity(MW)\"])\n",
    "                        NetSummerCapacity=np.sum(df_tmp4[\"NetSummerCapacity(MW)\"])\n",
    "                        NetWinterCapacity=np.sum(df_tmp4[\"NetWinterCapacity(MW)\"])\n",
    "                        NameplateEnergyCapacity=np.sum(df_tmp4[\"NameplateEnergyCapacity(MWh)\"])\n",
    "                        \n",
    "                        NewPlantID = Region + \"_\" + UniqueSC + \"_\" + UniqueMC\n",
    "                        Technology= df_tmp4[\"Technology\"].iloc[0]\n",
    "\n",
    "                        Data=[[NewPlantID, UniqueSC, UniqueMC, Vintages[i] ,\n",
    "                        NameplateCapacity, NetSummerCapacity, NetWinterCapacity, NameplateEnergyCapacity,\n",
    "                        Technology,Region]]\n",
    "\n",
    "                        df_AggregateGen3=pd.concat([df_AggregateGen3,pd.DataFrame(Data,columns=df_AggregateGen3.columns)],ignore_index=True)\n",
    "                    else:\n",
    "                        #We need to differentiate between the different battery technologies\n",
    "                        #Consider 1, 2 ,4, 6, 8 ,10h battery\n",
    "                        df_tmp4=df_tmp3[IdxsIn].reset_index()    \n",
    "                        HoursOfOperation=df_tmp4[\"NameplateEnergyCapacity(MWh)\"]/df_tmp4[\"NameplateCapacity(MW)\"] #h operation of each battery\n",
    "\n",
    "\n",
    "                        TmpBrack=[0, 1, 2 ,4, 6, 8 ,10]\n",
    "                        for h_id in range(len(TmpBrack)):\n",
    "                            \n",
    "                            IdxInBatteryH= (HoursOfOperation>TmpBrack[i]) * (HoursOfOperation<=TmpBrack[i+1])\n",
    "                            df_tmp5=df_tmp4[IdxInBatteryH].reset_index()    \n",
    "                            #if not empty\n",
    "                            if sum(IdxInBatteryH)!=0:\n",
    "                                \n",
    "                                #Need to ajust the power of the battery to match the energy capacity\n",
    "                                NameplateCapacity=np.sum(df_tmp5[\"NameplateEnergyCapacity(MWh)\"])/TmpBrack[i+1]\n",
    "                                NetSummerCapacity=NameplateCapacity\n",
    "                                NetWinterCapacity=NameplateCapacity\n",
    "                                NameplateEnergyCapacity=TmpBrack[i+1]*NameplateCapacity\n",
    "                                \n",
    "                                NewPlantID = Region + \"_\" + UniqueSC + \"_\" + UniqueMC +str(TmpBrack[i+1])+\"H\"\n",
    "                                Technology= df_tmp5[\"Technology\"].iloc[0]\n",
    "\n",
    "                                Data=[[NewPlantID, UniqueSC, UniqueMC, Vintages[i] ,\n",
    "                                NameplateCapacity, NetSummerCapacity, NetWinterCapacity, NameplateEnergyCapacity,\n",
    "                                Technology,Region]]\n",
    "\n",
    "                                df_AggregateGen3=pd.concat([df_AggregateGen3,pd.DataFrame(Data,columns=df_AggregateGen3.columns)],ignore_index=True)                                \n",
    "\n",
    "dfExistingCapacity=df_AggregateGen3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area of Each Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PercentageAreaR1=new_polyR1.area/(new_polyR1.area + new_polyR2.area + new_polyR3.area)\n",
    "PercentageAreaR2=new_polyR2.area/(new_polyR1.area + new_polyR2.area + new_polyR3.area)\n",
    "PercentageAreaR3=new_polyR3.area/(new_polyR1.area + new_polyR2.area + new_polyR3.area)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot regions and tech location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "count=0\n",
    "for geom in new_polyR1.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    count=count+1\n",
    "    if count==1:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='r', ec='none',label=\"R1\")\n",
    "    else:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='r', ec='none')\n",
    "\n",
    "count=0\n",
    "for geom in new_polyR2.geoms:    \n",
    "    count=count+1\n",
    "    xs, ys = geom.exterior.xy    \n",
    "    if count==1:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='yellow', ec='none',label=\"R2\")\n",
    "    else:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='yellow', ec='none')\n",
    "\n",
    "\n",
    "xs, ys = new_polyR3.exterior.xy    \n",
    "ax.fill(xs, ys, alpha=0.5, fc='green', ec='none',label=\"R3\")\n",
    "\n",
    "\n",
    "plt.scatter(df_AggregateGen[\"Longitude\"],df_AggregateGen[\"Latitude\"],color=\"black\",s=1, label=\"Ex. Generators\")\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig(\"OutputData/Figures/RegionsByRisk_ExTech.png\",dpi=600,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "count=0\n",
    "for geom in new_polyR1.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    count=count+1\n",
    "    if count==1:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='r', ec='none',label=\"R1\")\n",
    "    else:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='r', ec='none')\n",
    "\n",
    "count=0\n",
    "for geom in new_polyR2.geoms:    \n",
    "    count=count+1\n",
    "    xs, ys = geom.exterior.xy    \n",
    "    if count==1:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='yellow', ec='none',label=\"R2\")\n",
    "    else:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='yellow', ec='none')\n",
    "\n",
    "\n",
    "xs, ys = new_polyR3.exterior.xy    \n",
    "ax.fill(xs, ys, alpha=0.5, fc='green', ec='none',label=\"R3\")\n",
    "\n",
    "\n",
    "for Poly in Polygons:    \n",
    "    \n",
    "    if Poly.geom_type == 'Polygon':\n",
    "        \n",
    "        xs, ys = Poly.exterior.xy    \n",
    "        ax.plot(xs, ys, alpha=0.5, c='black', linewidth=0.2)\n",
    "          \n",
    "    if Poly.geom_type == 'MultiPolygon':\n",
    "        for Poly2 in list(Poly.geoms):   \n",
    "            xs, ys = Poly2.exterior.xy\n",
    "            ax.plot(xs, ys, alpha=0.5, c='black', linewidth=0.2)\n",
    "        count=count+1\n",
    "            \n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.legend(frameon=False,loc=\"lower left\")\n",
    "plt.savefig(\"OutputData/Figures/RegionsByRisk.png\",dpi=600,bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write df on sql format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExistingCapacity_sql = pd.DataFrame(columns=[\"regions\", \"tech\", \"vintage\", \"exist_cap\", \"exist_cap_units\", \"exist_cap_notes\"])\n",
    "ExistingCapacity_sql[\"regions\"]=dfExistingCapacity[\"Region\"]\n",
    "ExistingCapacity_sql[\"tech\"]=dfExistingCapacity[\"PlantID\"].str.split('_').str[1:3].str.join('_')\n",
    "\n",
    "\n",
    "ExistingCapacity_sql[\"vintage\"]=dfExistingCapacity[\"Vintage\"]\n",
    "ExistingCapacity_sql[\"exist_cap\"]=dfExistingCapacity[\"NameplateCapacity(MW)\"]/1000 #MW to GW\n",
    "ExistingCapacity_sql[\"exist_cap_units\"]=\"GW\"\n",
    "ExistingCapacity_sql[\"exist_cap_notes\"]=\"EIA 860 Generator + Code 2 aggregate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export unique tech name and description to facilitate the manual mapping\n",
    "data1=ExistingCapacity_sql[\"tech\"].unique()\n",
    "data2=[dfExistingCapacity[ dfExistingCapacity[\"PlantID\"].str.split('_').str[1:3].str.join('_')==tech][\"Technology\"].iloc[0] for tech in ExistingCapacity_sql[\"tech\"].unique()]\n",
    "\n",
    "ExistingCapacity_sql[\"tech\"]=ExistingCapacity_sql[\"tech\"]+\"_EXISTING\"\n",
    "data1=data1+\"_EXISTING\"\n",
    "data=zip(data1,data2)\n",
    "\n",
    "ExistingCapacity_UniqueTechNamedf=pd.DataFrame(columns=[\"tech\",\"description\"],data=data)\n",
    "ExistingCapacity_UniqueTechNamedf.to_excel(UserDataPath + \"Temporary_ToHelpBuildDecks/TMP_ExistingCapacity_UniqueTechNameLv2.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Existing Capacity by Operating Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get colors for each tech plot\n",
    "def GetColors(Tech):\n",
    "    Colors=[]\n",
    "    ColorsUser= pd.read_excel(UserDataPath+\"UserDataPart3.xlsx\", sheet_name ='TechColor')\n",
    "\n",
    "    for t in Tech:\n",
    "        pallet=ColorsUser.loc[ColorsUser[\"Tech\"]==t,\"Pallet\"].values[0]\n",
    "        idx=ColorsUser.loc[ColorsUser[\"Tech\"]==t,\"Idx\"].values[0]\n",
    "        Colors.append(sns.color_palette(pallet, n_colors=20)[idx])\n",
    "\n",
    "    return Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotStackBars(Data, ExcludeTech=[\"TRANSMISSION_REGIONAL\",\"DISTRIBUTION\"],DataColumn=\"vintage\",TechTermination=\"_EXISTING\",Regions=[\"R1\",\"R2\",\"R3\"],X_name=\"Vintage\",Y_name=\"Capacity [GW]\",scale_legend=1000, ScaleUnits=\"MW\",\n",
    "                  Title=\"Existing Capacity\", OutputName=\"ExistingCapacity\",Ticks=1):\n",
    "\n",
    "    #Filter for selected regions and technology terminations and eliminate excluded technologies\n",
    "    df_tmp=Data[Data[\"regions\"].isin(Regions)]  #Exclude regions\n",
    "    df_tmp=df_tmp[~df_tmp[\"tech\"].isin(ExcludeTech)]#Exclude technologies\n",
    "    df_tmp=df_tmp[df_tmp[\"tech\"].str.contains(TechTermination)] #Filter for technology termination\n",
    "    df_tmp=df_tmp.sort_values(by=['tech'])\n",
    "\n",
    "    Units=df_tmp[\"exist_cap_units\"].values[0]\n",
    "\n",
    "    #Sum Data by Tech and Vintage\n",
    "    df_tmp_G=df_tmp.groupby([\"tech\",DataColumn]).sum().reset_index()\n",
    "\n",
    "    #Plot stacked tech bars for each vintage\n",
    "    df_pivot=df_tmp_G.pivot(index=DataColumn,columns='tech',values='exist_cap')\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    #Choose from a large pallet of collors and plot\n",
    "    Tech=df_pivot.columns.values\n",
    "    colors =GetColors(Tech)\n",
    "\n",
    "    #Add total \"capacity\" to the legend\n",
    "    Legend=Tech.tolist()\n",
    "    TotalCapacity=df_tmp_G.groupby([\"tech\"]).sum().reset_index()\n",
    "    TotalCapacity=(TotalCapacity[\"exist_cap\"]*scale_legend).astype(float).round(1).astype(str).values.tolist()\n",
    "    Legend=[Legend[i]+\" (\"+TotalCapacity[i]+ScaleUnits+\")\" for i in range(len(Legend))]\n",
    "    df_pivot.plot.bar(stacked=True,ax=ax,color=colors,legend=False)\n",
    "    ax.legend(Legend,loc='center left', bbox_to_anchor=(1, 0.5),frameon=False)\n",
    "\n",
    "    ax.set_ylabel(Y_name)\n",
    "    ax.set_xlabel(X_name)\n",
    "    ax.set_title(Title)\n",
    "\n",
    "    UniqueYears=df_tmp[DataColumn].unique()\n",
    "    MaxY=UniqueYears.max()\n",
    "    MinY=UniqueYears.min()\n",
    "\n",
    "\n",
    "    # step=int((MaxY-MinY+1)/25)\n",
    "    # if step>1:\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(25))\n",
    "\n",
    "    #Enlongate the graph horizontally\n",
    "    fig.set_size_inches(10, 5)\n",
    "\n",
    "    plt.savefig(\"OutputData/Figures/\"+OutputName+\".png\",dpi=1000, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=ExistingCapacity_sql.copy()\n",
    "#Add zero values for all vintages to make sure the ticks on the plot are correct\n",
    "tech=\"BLQ_ST_EXISTING\"\n",
    "MaxVintage=Data[\"vintage\"].unique().max()\n",
    "MinVintage=Data[\"vintage\"].unique().min()\n",
    "for vintage in range(MinVintage,MaxVintage+1,1):\n",
    "    Data=pd.concat([Data,pd.DataFrame({\"regions\":\"R1\",\"tech\":tech,\"vintage\":vintage,\"exist_cap\":0,\"exist_cap_units\":\"GW\",\"exist_cap_notes\":\"EIA 860 Generator + Code 2 aggregate\"},index=[0])],ignore_index=True)\n",
    "\n",
    "_=PlotStackBars(Data,Title=\"Existing Capacity (\"+str(np.round(Data[\"exist_cap\"].sum(),2))+\" GW)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LifetimeTech & LifetimeLoanTech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Average Lifetime from EIA 860 Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read File\n",
    "df = pd.read_excel(GeneratorDataPath, sheet_name ='Retired',skiprows=2)\n",
    "df=df[df[\"Nameplate Capacity (MW)\"]>=0]\n",
    "\n",
    "# StatesInside=[\"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"DC\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\n",
    "# \"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\n",
    "# \"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\"]\n",
    "\n",
    "#StatesInside=[\"ME\",\"NH\",\"MA\",\"RI\",\"CT\",\"NY\",\"NJ\",\"DE\",\"MD\",\"VA\",\"NC\",\"SC\",\"GA\",\"FL\"]\n",
    "StatesInside=[\"NC\"] #If Capacity >100MW use NC othrewise east coast\n",
    "InState=df[\"Plant State\"].isin(StatesInside) #Filter for South \n",
    "df=df[InState].reset_index()\n",
    "\n",
    "#On EIA 860 combined cycle tech are divided in CT and CA (steam part)\n",
    "#We need to combine them as CC: combine cycle\n",
    "df.loc[df[\"Prime Mover Code\"]==\"CT\",\"Prime Mover Code\"]=\"CC\"\n",
    "df.loc[df[\"Prime Mover Code\"]==\"CA\",\"Prime Mover Code\"]=\"CC\"\n",
    "df.loc[df[\"Prime Mover Code\"]==\"CT\",\"Technology\"]=\"Combined cycle\"\n",
    "df.loc[df[\"Prime Mover Code\"]==\"CA\",\"Technology\"]=\"Combined cycle\"\n",
    "df[\"LifetimeWC\"]=(df[\"Retirement Year\"]-df[\"Operating Year\"]+1)*df[\"Nameplate Capacity (MW)\"]\n",
    "\n",
    "#Get average Lifetime. Group by Energy source and Prime mover\n",
    "df_Lifetime=df.groupby([\"Energy Source Code\",\"Prime Mover Code\"])[[\"LifetimeWC\",\"Nameplate Capacity (MW)\"]].sum().reset_index()\n",
    "df_Lifetime[\"Lifetime\"]=df_Lifetime[\"LifetimeWC\"]/df_Lifetime[\"Nameplate Capacity (MW)\"]\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "df_Lifetime\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Energy Capacity With Specific Retirement Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read File\n",
    "df_Retirement = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='LifeTimeSpecific')\n",
    "\n",
    "#On EIA 860 combined cycle tech are divided in CT and CA (steam part)\n",
    "#We need to combine them as CC: combine cycle\n",
    "df_Retirement.loc[df_Retirement[\"MoverCode\"]==\"CT\",\"MoverCode\"]=\"CC\"\n",
    "df_Retirement.loc[df_Retirement[\"MoverCode\"]==\"CA\",\"MoverCode\"]=\"CC\"\n",
    "\n",
    "for i in range(df_Retirement.shape[0]):\n",
    "    Latitude=df_Retirement.iloc[i][\"Latitude\"]\n",
    "    Longitude=df_Retirement.iloc[i][\"Longitude\"]\n",
    "\n",
    "    if new_polyR1.contains(Point(Longitude,Latitude)):\n",
    "        df_Retirement.at[i,\"Region\"]='R1'\n",
    "\n",
    "    elif new_polyR2.contains(Point(Longitude,Latitude)):\n",
    "        df_Retirement.at[i,\"Region\"]='R2'\n",
    "    \n",
    "    elif new_polyR3.contains(Point(Longitude,Latitude)):\n",
    "        df_Retirement.at[i,\"Region\"]='R3'\n",
    "\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "df_Retirement[\"tech\"]=df_Retirement[\"SourceCode\"]+\"_\"+df_Retirement[\"MoverCode\"] + \"_EXISTING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the retirement data (df_AggregateRetirement)\n",
    "df_AggregateRetirement = pd.DataFrame(columns=[\"PlantID\",\"SourceCode\", \"MoverCode\", \"RetireYear\", \"NameplateCapacity(MW)\",\"NetSummerCapacity(MW)\",\"NetWinterCapacity(MW)\", \"Region\"])\n",
    "\n",
    "for Region in [\"R1\",\"R2\",\"R3\"]:\n",
    "\n",
    "    df_tmp1=df_Retirement[df_Retirement[\"Region\"]==Region]\n",
    "\n",
    "    SourceCode=df_tmp1[\"SourceCode\"]\n",
    "    #Same Energy Source Code data\n",
    "    for UniqueSC in SourceCode.unique():\n",
    "        SameIds    = df_tmp1[\"SourceCode\"]==UniqueSC\n",
    "        df_tmp2    = df_tmp1[SameIds]\n",
    "\n",
    "        MoverCode=df_tmp2[\"MoverCode\"]\n",
    "        #Same Mover Code data\n",
    "        for UniqueMC in MoverCode.unique():\n",
    "            SameIds    = df_tmp2[\"MoverCode\"]==UniqueMC\n",
    "            df_tmp3    = df_tmp2[SameIds]\n",
    "            \n",
    "            RetireYear=df_tmp3[\"RetireYear\"]\n",
    "            for RefRetireYear in RetireYear.unique():\n",
    "\n",
    "                IdxsIn=df_tmp3[\"RetireYear\"]==RefRetireYear\n",
    "                if sum(IdxsIn)!=0:\n",
    "                    #So far only considering data for non BA technology\n",
    "                    if UniqueMC!=\"BA\":\n",
    "                        df_tmp4    = df_tmp3[IdxsIn]\n",
    "\n",
    "                        NameplateCapacity=np.sum(df_tmp4[\"NameplateCapacity(MW)\"])\n",
    "                        NetSummerCapacity=np.sum(df_tmp4[\"NetSummerCapacity(MW)\"])\n",
    "                        NetWinterCapacity=np.sum(df_tmp4[\"NetWinterCapacity(MW)\"])\n",
    "                        \n",
    "                        NewPlantID = Region + \"_\" + UniqueSC + \"_\" + UniqueMC + \"_EXISTING\"\n",
    "\n",
    "\n",
    "                        Data=[[NewPlantID, UniqueSC, UniqueMC, RefRetireYear,\n",
    "                        NameplateCapacity, NetSummerCapacity, NetWinterCapacity,Region]]\n",
    "\n",
    "                        df_AggregateRetirement=pd.concat([df_AggregateRetirement,pd.DataFrame(Data,columns=df_AggregateRetirement.columns)],ignore_index=True)\n",
    "                        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a sql for the LifetimeTech & LifetimeLoanTech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read File\n",
    "df = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='LifeTimesLoanTechDefault')\n",
    "\n",
    "LifetimeTech_sql = pd.DataFrame(columns=[\"Regions\",\"tech\", \"life\", \"life_notes\"])\n",
    "LifetimeLoanTech_sql = pd.DataFrame(columns=[\"Regions\",\"tech\", \"loan\", \"loan_notes\"])\n",
    "\n",
    "\n",
    "Regions=[\"R1\"]*len(df)+[\"R2\"]*len(df)+[\"R3\"]*len(df)\n",
    "tech=np.tile(df[\"tech\"],3)\n",
    "life=np.tile(df[\"life tech\"],3)\n",
    "loan=np.tile(df[\"loan\"],3)\n",
    "Observations=np.tile(df[\"Observations\"],3)\n",
    "\n",
    "LifetimeTech_sql[\"Regions\"]=Regions\n",
    "LifetimeTech_sql[\"tech\"]=tech\n",
    "LifetimeTech_sql[\"life\"]=life\n",
    "LifetimeTech_sql[\"life_notes\"]=Observations\n",
    "\n",
    "LifetimeLoanTech_sql[\"Regions\"]=Regions\n",
    "LifetimeLoanTech_sql[\"tech\"]=tech\n",
    "LifetimeLoanTech_sql[\"loan\"]=loan\n",
    "LifetimeLoanTech_sql[\"loan_notes\"]=Observations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExistingCapacity Changes to Match Known Retirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the operation year and allocate capacity to match known retirements and LifetimeTech\n",
    "# On temoa there is only one LifetimeTech value per technology. Since we grouped the technologies part of it \n",
    "# may retire at different times. We only know the expected retirements for a few generators (~60%) we need to make sure retirements happen at the right time\n",
    "ExistingCapacity_sql_adjusted = ExistingCapacity_sql.copy()\n",
    "\n",
    "for i in range(df_Retirement.shape[0]):\n",
    "    #Retirement data\n",
    "    OperatingYear=df_Retirement.iloc[i][\"OperatingYear\"]\n",
    "    RetireYear=df_Retirement.iloc[i][\"RetireYear\"]\n",
    "\n",
    "    Region=df_Retirement.iloc[i][\"Region\"]\n",
    "    SourceCode=df_Retirement.iloc[i][\"SourceCode\"]\n",
    "    MoverCode=df_Retirement.iloc[i][\"MoverCode\"]\n",
    "    tech=df_Retirement.iloc[i][\"tech\"]\n",
    "    NameplateCapacity=df_Retirement.iloc[i][\"NameplateCapacity(MW)\"]/1000 #MW to GW\n",
    "    NetSummerCapacity=df_Retirement.iloc[i][\"NetSummerCapacity(MW)\"]/1000 #MW to GW\n",
    "    NetWinterCapacity=df_Retirement.iloc[i][\"NetWinterCapacity(MW)\"]/1000 #MW to GW\n",
    "\n",
    "    #Defined in the sql file is the same for all tehcnologies of the same type\n",
    "    LifeOfTech=LifetimeTech_sql[(LifetimeTech_sql[\"tech\"]==tech) * (LifetimeTech_sql[\"Regions\"]==Region)][\"life\"].iloc[0]\n",
    "\n",
    "    #Check for tech and regions that are the same\n",
    "    IdxCandidate= (ExistingCapacity_sql[\"regions\"]==Region) * (ExistingCapacity_sql[\"tech\"]==tech)\n",
    "    \n",
    "    if sum(IdxCandidate)!=0:\n",
    "        \n",
    "        #Check for years that are the same of very close\n",
    "        Targets=(ExistingCapacity_sql[IdxCandidate][\"vintage\"]-OperatingYear).abs()\n",
    "        EC_Av_Discount=np.sum(ExistingCapacity_sql[IdxCandidate][\"exist_cap\"][Targets==0]) #Existing capacity available for discount\n",
    "        \n",
    "        if EC_Av_Discount>=NameplateCapacity: #a buffer of 0 years between retirements and existing capacity is allowed (change Targets==0 to Targets<=1 to allow for a 1 year buffer)            \n",
    "            NewVintageLocation=RetireYear-LifeOfTech #Year that the resource should be placed to match retirements\n",
    "            if NewVintageLocation>=FutureYears[0]:\n",
    "                print(\"Inconsistency between retirements and existing capacity.\\nLikely lifetimes too small for the target retirements, and/or need to separate technologies with different lifetimes\")\n",
    "                #ignore this error as the differences are small only a few years for some technologies with retirement >2050\n",
    "                print(\"On Retirement Info: Tech (%s)  NewVintage (%s) Retirement (%s) NameplateCapacity (%s)\"%(tech,NewVintageLocation,RetireYear,NameplateCapacity))\n",
    "                print(\"Error of %s years\"%(NewVintageLocation-Vintages[0]))\n",
    "                if NewVintageLocation>=FutureYears[1]:\n",
    "                    print(\"Fix the inconsistency or increase relaxation for the error, by setting FutureYears[2,3,..]\")\n",
    "                    break\n",
    "                print(\"Ignoring the inconsistency as it seems to be small\\n\\n\")\n",
    "                NewVintageLocation=Vintages[0] #Since the year need to be larger than Vintages[0] for adequeate retirement we set it to Vintages[0] and ignore the error\n",
    "            #add\n",
    "            else:\n",
    "                Data=[[Region,tech, NewVintageLocation, NameplateCapacity, \"GW\", \"New add for retirement ballance\"]]\n",
    "                ExistingCapacity_sql_adjusted=pd.concat([ExistingCapacity_sql_adjusted,pd.DataFrame(Data,columns=ExistingCapacity_sql.columns)],ignore_index=True) \n",
    "\n",
    "                #subtract\n",
    "                Data=[[Region,tech, OperatingYear, -NameplateCapacity, \"GW\", \"New add for retirement ballance\"]]\n",
    "                ExistingCapacity_sql_adjusted=pd.concat([ExistingCapacity_sql_adjusted,pd.DataFrame(Data,columns=ExistingCapacity_sql.columns)],ignore_index=True)            \n",
    "\n",
    "        else:\n",
    "            print(\"Error- Inconsistency between retirements and existing capacity. Not enought capacity on the EIA 860 forms for the target retire\")\n",
    "            print(\"On Retirement Info: Tech (%s)  OperatingYear (%s)  NameplateCapacity (%s)\"%(tech,OperatingYear,NameplateCapacity))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExistingCapacity_sql_adjusted_tmp=ExistingCapacity_sql_adjusted.groupby([\"regions\",\"tech\",\"vintage\"])[\"exist_cap\"].sum().reset_index()\n",
    "ExistingCapacity_sql_adjusted_tmp[\"exist_cap_units\"]=\"GW\"\n",
    "ExistingCapacity_sql_adjusted_tmp[\"exist_cap_notes\"]=\"EIA 860 Generator + Code 2 aggregate\"\n",
    "\n",
    "#Check for negative values\n",
    "#pop error\n",
    "if np.sum(ExistingCapacity_sql_adjusted_tmp[ExistingCapacity_sql_adjusted_tmp[\"exist_cap\"]<0][\"exist_cap\"])<=-1/1000: #1MW of inconsistency:\n",
    "    print(\"Error- Inconsistency between retirements and existing capacity. Negative values existing capacity\")\n",
    "else:\n",
    "    ExistingCapacity_sql_adjusted_tmp=ExistingCapacity_sql_adjusted_tmp[ExistingCapacity_sql_adjusted_tmp[\"exist_cap\"]>0]\n",
    "    ExistingCapacity_sql_adjusted=ExistingCapacity_sql_adjusted_tmp\n",
    "    print(\"Known retirements properly allocated to existing capacity.\")\n",
    "\n",
    "#ExistingCapacity_sql_adjusted: corrected ExistingCapacityValues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Retirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=ExistingCapacity_sql_adjusted.copy().reset_index(drop=True)\n",
    "#Compute retirement year\n",
    "Data[\"RetireYear\"]=0\n",
    "for i in range(len(Data)):\n",
    "    Life=LifetimeTech_sql[LifetimeTech_sql[\"tech\"]==Data.at[i,\"tech\"]].iloc[0][\"life\"]\n",
    "    Data.at[i,\"RetireYear\"]=int(Life + Data.at[i,\"vintage\"])\n",
    "\n",
    "#Add zero values for all vintages to make sure the ticks on the plot are correct\n",
    "tech=\"BLQ_ST_EXISTING\" #Needs to be a technology that existis <2023 and after 2023\n",
    "MaxRetireYear=Data[\"RetireYear\"].unique().max()\n",
    "MinRetireYear=Data[\"RetireYear\"].unique().min()\n",
    "for RetireYear in range(MinRetireYear,MaxRetireYear+5,1):\n",
    "    Data=pd.concat([Data,pd.DataFrame({\"regions\":\"R1\",\"tech\":tech,\"vintage\":0,\"RetireYear\":RetireYear,\"exist_cap\":0,\"exist_cap_units\":\"\",\"exist_cap_notes\":\"\"},index=[0])],ignore_index=True)\n",
    "\n",
    "Data1=Data[Data[\"RetireYear\"]<=2023]\n",
    "\n",
    "\n",
    "\n",
    "_=PlotStackBars(Data1, ExcludeTech=[\"TRANSMISSION_REGIONAL\",\"DISTRIBUTION\"],TechTermination=\"_EXISTING\",Regions=[\"R1\",\"R2\",\"R3\"],X_name=\"Year of Retirement\",Y_name=\"Capacity [GW]\",scale_legend=1000, ScaleUnits=\"MW\",\n",
    "                  Title=\"*Retirement Years of Existing Capacity (\"+str(np.round(Data1[\"exist_cap\"].sum(),2))+\" GW)\", OutputName=\"ExistingCapacityRetirementsBefore2023\",DataColumn=\"RetireYear\")\n",
    "\n",
    "Data2=Data[Data[\"RetireYear\"]>2023]\n",
    "\n",
    "_=PlotStackBars(Data2, ExcludeTech=[\"TRANSMISSION_REGIONAL\",\"DISTRIBUTION\"],TechTermination=\"_EXISTING\",Regions=[\"R1\",\"R2\",\"R3\"],X_name=\"Year of Retirement\",Y_name=\"Capacity [GW]\",scale_legend=1000, ScaleUnits=\"MW\",\n",
    "                  Title=\"*Retirement Years of Existing Capacity (\"+str(np.round(Data2[\"exist_cap\"].sum(),2))+\" GW)\", OutputName=\"ExistingCapacityRetirementsAfter2023\",DataColumn=\"RetireYear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of Existing Capacity retired Before 2023: %.2f %% (%.2f MW)\"%((np.sum(Data1[\"exist_cap\"])/np.sum(Data[\"exist_cap\"])*100), np.sum(Data1[\"exist_cap\"]*1000)))\n",
    "#Please double check WH_ST lifetime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Ajustment in Existing Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capacity that is mathematically already retired (due to lifetime values) but not rerited according to EIA 860 are elimited from the model\n",
    "#However, you can explicitly imput the retirement date of this generation (of any generation) on the user data (LifeTimeSpecific) excel file is you want to keep it.\n",
    "\n",
    "df=ExistingCapacity_sql_adjusted.copy().reset_index(drop=True)\n",
    "#Compute retirement year\n",
    "df[\"RetireYear\"]=0\n",
    "for i in range(len(df)):\n",
    "    Life=LifetimeTech_sql[LifetimeTech_sql[\"tech\"]==df.at[i,\"tech\"]].iloc[0][\"life\"]\n",
    "    df.at[i,\"RetireYear\"]=int(Life + df.at[i,\"vintage\"])\n",
    "\n",
    "\n",
    "#also remove entries that have very low capacity less than 100kw or 0.1MW or 1e-4 GW\n",
    "ExistingCapacity_sql_adjusted=df[(df[\"RetireYear\"]>np.min(FutureYears))*(df[\"exist_cap\"]>=1e-4)]\n",
    "ExistingCapacity_sql=ExistingCapacity_sql_adjusted.drop(columns=[\"RetireYear\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Total Existing Capacity Through Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TotAccumulatedStackTime(Data, ListGroups, FutureYears=FutureYears, DataColumn=\"exist_cap\", TimeColumn=\"RetireYear\", X_name=\"Year\", Y_name=\"Capacity [GW]\",\n",
    "                  Title=\"Existing Capacity\", OutputName=\"ExistingCapacityInSimulation\"):\n",
    "    \n",
    "    X_Data=np.arange(min(FutureYears),max(FutureYears)+1,1)\n",
    "    Y_Data=np.zeros(( len(Excel_tmp.iloc[:,1:].keys()), len(X_Data) ),dtype=float)\n",
    "    df=Data.groupby([\"tech\",TimeColumn])[[DataColumn]].sum().reset_index() #Merge regions\n",
    "\n",
    "    i_group=0\n",
    "    for groups in ListGroups.keys():\n",
    "        TechIn=ListGroups[groups]\n",
    "        df_group=df[df[\"tech\"].isin(TechIn)]#Filter for the group\n",
    "        \n",
    "        i_year=0\n",
    "        for year in X_Data:\n",
    "            \n",
    "            Y_Data[i_group,i_year]=df_group[df_group[TimeColumn]>year][DataColumn].sum()#Filter get total capacity for the group\n",
    "            i_year+=1\n",
    "            \n",
    "        i_group+=1\n",
    "\n",
    "    colors =GetColors(list(ListGroups.keys()))\n",
    "    plt.stackplot(X_Data,Y_Data,labels=ListGroups.keys(),colors=colors )\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xticks(FutureYears,rotation=90) \n",
    "    plt.xlim(min(FutureYears),max(FutureYears))\n",
    "    plt.ylabel(Y_name)\n",
    "    plt.xlabel(X_name)\n",
    "    plt.legend(frameon=False)\n",
    "    plt.savefig(\"OutputData/Figures/\"+OutputName+\".png\",bbox_inches='tight',dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=ExistingCapacity_sql_adjusted\n",
    "#Get groups\n",
    "Excel_tmp=pd.read_excel(UserDataPath+\"UserDataPart3.xlsx\",sheet_name='SummaryGroupsExistingTech')\n",
    "ListGroups={}\n",
    "for groups in Excel_tmp.iloc[:,1:].keys(): #order alphabetically\n",
    "    ListGroups[groups]=list(Excel_tmp[groups].dropna())\n",
    "    \n",
    "TotAccumulatedStackTime(Data, ListGroups)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Population at Each Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demand we get total energy demand of the sate from the IRPs\n",
    "#and allocate the demand to R1, R2, R3 based on the population of each region\n",
    "shapefile_Pop = gpd.read_file(CensusDataPath)\n",
    "PointsPop=[Point(shapefile_Pop[\"LONGITUDE\"][i],shapefile_Pop[\"LATITUDE\"][i]) for i in range(len(shapefile_Pop))]\n",
    "PopulationR1=0\n",
    "PopulationR2=0\n",
    "PopulationR3=0\n",
    "\n",
    "ListCensusPop=[]\n",
    "ListCensusOnR1,ListCensusOnR2,ListCensusOnR3=[],[],[]\n",
    "\n",
    "for i in range(len(PointsPop)):\n",
    "    point=PointsPop[i]\n",
    "    ListCensusPop.append(shapefile_Pop[\"POPULATION\"][i])\n",
    "    if new_polyR1.contains(point):\n",
    "        PopulationR1=PopulationR1+shapefile_Pop[\"POPULATION\"][i]\n",
    "        ListCensusOnR1.append(i)\n",
    "        \n",
    "    elif new_polyR2.contains(point):\n",
    "        PopulationR2=PopulationR2+shapefile_Pop[\"POPULATION\"][i]\n",
    "        ListCensusOnR2.append(i)\n",
    "\n",
    "    elif new_polyR3.contains(point):\n",
    "        PopulationR3=PopulationR3+shapefile_Pop[\"POPULATION\"][i]\n",
    "        ListCensusOnR3.append(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Demand From User and Regionalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from excel\n",
    "NonRegionalizedDemand_df = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='Demand')\n",
    "\n",
    "#Get demand in each region\n",
    "DemandR1=PopulationR1/(PopulationR1+PopulationR2+PopulationR3)*NonRegionalizedDemand_df[\"Demand [PJ]\"]\n",
    "DemandR2=PopulationR2/(PopulationR1+PopulationR2+PopulationR3)*NonRegionalizedDemand_df[\"Demand [PJ]\"]\n",
    "DemandR3=PopulationR3/(PopulationR1+PopulationR2+PopulationR3)*NonRegionalizedDemand_df[\"Demand [PJ]\"]\n",
    "\n",
    "#Filter for the future years\n",
    "DemandR1=DemandR1[np.searchsorted(NonRegionalizedDemand_df[\"periods\"], FutureYears[:-1])]\n",
    "DemandR2=DemandR2[np.searchsorted(NonRegionalizedDemand_df[\"periods\"], FutureYears[:-1])]\n",
    "DemandR3=DemandR3[np.searchsorted(NonRegionalizedDemand_df[\"periods\"], FutureYears[:-1])]\n",
    "\n",
    "\n",
    "#Create sql dataframe\n",
    "Demand_sql = pd.DataFrame(columns=[\"regions\", \"periods\", \"demand_comm\", \"demand\", \"demand_units\", \"demand_notes\"])\n",
    "\n",
    "Demand_sql[\"regions\"]=[\"R1\"]*len(FutureYears[:-1])+[\"R2\"]*len(FutureYears[:-1])+[\"R3\"]*len(FutureYears[:-1])\n",
    "Demand_sql[\"periods\"]=np.tile(FutureYears[:-1],3)#need to ignore last year\n",
    "Demand_sql[\"demand\"]=np.concatenate((DemandR1,DemandR2,DemandR3),axis=0)\n",
    "\n",
    "Demand_sql[\"demand_units\"]=\"PJ\"\n",
    "Demand_sql[\"demand_comm\"]=\"ELCDMD\" #Electricity demand\n",
    "Demand_sql[\"demand_notes\"]=NonRegionalizedDemand_df[\"Comments\"][0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time (_of_day, _period_labels, _periods, _renewable, _season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_of_day\n",
    "time_of_day_sql=pd.DataFrame(columns=[\"t_day\"])\n",
    "time_of_day_sql[\"t_day\"]=TimeOfDay\n",
    "\n",
    "#time_period_labels\n",
    "time_period_labels_sql=pd.DataFrame(columns=[\"t_period_labels\",\"t_period_labels_desc\"])\n",
    "time_period_labels_sql[\"t_period_labels\"]=[\"e\",\"f\"]\n",
    "time_period_labels_sql[\"t_period_labels_desc\"]=[\"existing vintages\",\"future\"]\n",
    "\n",
    "#time_periods\n",
    "time_periods_sql=pd.DataFrame(columns=[\"t_periods\",\"flag\"])\n",
    "Exisist=Vintages[0:np.where(Vintages==ExistingCapacity_sql[\"vintage\"].min())[0][0]+1]\n",
    "Exisist=Exisist[::-1]\n",
    "Fut=FutureYears\n",
    "Periods=np.concatenate((Exisist,Fut))\n",
    "\n",
    "time_periods_sql[\"t_periods\"]=Periods\n",
    "time_periods_sql.loc[time_periods_sql[\"t_periods\"]<=ExistingCapacity_sql[\"vintage\"].max(),\"flag\"]=\"e\"\n",
    "time_periods_sql.loc[time_periods_sql[\"t_periods\"]>ExistingCapacity_sql[\"vintage\"].max(),\"flag\"]=\"f\"\n",
    "\n",
    "#time_renewable (Empty)\n",
    "time_renewable_sql=pd.DataFrame(columns=[\"Field1\"])\n",
    "\n",
    "#time_season\n",
    "time_season_sql=pd.DataFrame(columns=[\"t_season\"])\n",
    "time_season_sql[\"t_season\"]=Seasons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sector_labels, technology_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sector_labels\n",
    "sector_labels_sql=pd.DataFrame(columns=[\"sector\"])\n",
    "sector_labels_sql[\"sector\"]=[\"supply\",\"electric\",\"electric_misc\",\"transport\",\"commercial\", \"residential\",\"industrial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technology_labels_sql=pd.DataFrame(columns=[\"tech_labels\",\"tech_labels_desc\"])\n",
    "technology_labels_sql[\"tech_labels\"]=[\"r\",\"p\",\"pb\",\"ps\"]\n",
    "technology_labels_sql[\"tech_labels_desc\"]=[\"resource technology\",\"production technology\",\"baseload production technology\",\"storage production technology\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from excel\n",
    "technologies_sql = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='technologies')\n",
    "technologies_sql=technologies_sql.drop(columns=[\"Observations\"])\n",
    "technologies_sql[\"tech_category\"]=\"\"\n",
    "\n",
    "# #verify content\n",
    "if set(technologies_sql.columns)!=(set([\"tech\", \"flag\", \"sector\", \"tech_desc\", \"tech_category\"])):\n",
    "    print(\"Error: There are heads in the technologies sheet not valid\")\n",
    "    print(\"Look at the path: \" + UserDataPath+\"UserDataPart1.xlsx\")\n",
    "    sys.exit()\n",
    "\n",
    "if set(technologies_sql[\"flag\"].unique()).issubset(set(technology_labels_sql[\"tech_labels\"]))!=True:\n",
    "    print(\"Error: There are elements flag in the technologies sheet not valid\")\n",
    "    print(\"Look at the path: \" + UserDataPath+\"UserDataPart1.xlsx\")\n",
    "    sys.exit()\n",
    "\n",
    "if set(technologies_sql[\"sector\"].unique()).issubset(set(sector_labels_sql[\"sector\"]))!=True:\n",
    "    print(\"Error: There are elements sector in the technologies sheet not valid\")\n",
    "    print(\"Look at the path: \" + UserDataPath+\"UserDataPart1.xlsx\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate _EXISTING TECHNOLOGIES not in ExistingCapacity_sql[\"tech\"]. Some tech may be eliminated if no capacity existing after filtering \n",
    "IdxExistingTech=technologies_sql[\"tech\"].str.contains(\"_EXISTING\")\n",
    "IdxExistingNotEliminated=technologies_sql[\"tech\"].isin(ExistingCapacity_sql[\"tech\"].unique())\n",
    "technologies_sql=technologies_sql[~(IdxExistingTech * ~IdxExistingNotEliminated)] #guys need to keep\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get region and name of unique existing and future tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueExistingTech=ExistingCapacity_sql.groupby([\"regions\",\"tech\"]).size().reset_index().rename(columns={0:'Occurances'}) \n",
    "UniqueExistingTech=UniqueExistingTech.drop(columns=[\"Occurances\"])\n",
    "\n",
    "df = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='NewTechRegions')\n",
    "\n",
    "regions=[]\n",
    "tech=[]\n",
    "for i in range(len(df)):\n",
    "    if df.iloc[i][\"R1\"]==1:\n",
    "        regions.append(\"R1\")\n",
    "        tech.append(df.iloc[i][\"tech\"])\n",
    "\n",
    "    if df.iloc[i][\"R2\"]==1:\n",
    "        regions.append(\"R2\")\n",
    "        tech.append(df.iloc[i][\"tech\"])\n",
    "\n",
    "    if df.iloc[i][\"R3\"]==1:\n",
    "        regions.append(\"R3\")\n",
    "        tech.append(df.iloc[i][\"tech\"])\n",
    "\n",
    "UniqueFutureTech = pd.DataFrame(list(zip(regions, tech)),columns=[\"regions\",\"tech\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust LifeTimeTech & Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueEFTech = pd.concat([UniqueExistingTech,UniqueFutureTech])\n",
    "KeepRows=[]\n",
    "\n",
    "for tech,region in zip(LifetimeLoanTech_sql[\"tech\"],LifetimeLoanTech_sql[\"Regions\"]):\n",
    "\n",
    "    if (region, tech) not in set(zip(UniqueEFTech[\"regions\"], UniqueEFTech[\"tech\"])):\n",
    "        KeepRows.append(False)\n",
    "    else:\n",
    "        KeepRows.append(True)\n",
    "\n",
    "#Only keep the rows that of values relevant to the model\n",
    "LifetimeLoanTech_sql=LifetimeLoanTech_sql[KeepRows]\n",
    "LifetimeTech_sql=LifetimeTech_sql[KeepRows]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CostInvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='CostInvest')\n",
    "\n",
    "CostInvest_sql = pd.DataFrame(columns=[\"regions\",\"tech\",\"vintage\",\"cost_invest\",\"cost_invest_units\",\"cost_invest_notes\"])\n",
    "\n",
    "for region, tech in zip(UniqueFutureTech[\"regions\"], UniqueFutureTech[\"tech\"]):\n",
    "\n",
    "    df_tmp=df[df[\"tech\"]==tech]\n",
    "\n",
    "    if np.sum(df_tmp[\"Stage\"]==UserScenario)!=0:\n",
    "        df_tmp=df_tmp[df_tmp[\"Stage\"]==UserScenario]\n",
    "\n",
    "        #If you cant find UserScenario you assign \"Single\" as the scenario\n",
    "    else:\n",
    "        df_tmp=df_tmp[df_tmp[\"Stage\"]==\"Single\"]\n",
    "\n",
    "    if len(df_tmp)==0:\n",
    "        print(\"Error: There are no values for the technology \"+tech+\" in the CostInvest sheet\")\n",
    "        print(\"Look at the path: \" + UserDataPath+\"UserDataPart1.xlsx\")\n",
    "        break\n",
    "\n",
    "    for vintage in FutureYears[:-1]:\n",
    "\n",
    "        cost_invest=df_tmp[vintage].iloc[0]\n",
    "        cost_invest_units=df_tmp[\"cost_invest_units\"].iloc[0]\n",
    "        cost_invest_notes=df_tmp[\"cost_invest_notes\"].iloc[0]\n",
    "        Data=[[region, tech, vintage, cost_invest, cost_invest_units, cost_invest_notes]]\n",
    "\n",
    "        CostInvest_sql=pd.concat([CostInvest_sql,pd.DataFrame(Data,columns=CostInvest_sql.columns)],ignore_index=True)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CostInvest_sql.to_excel('CostInvest_sql.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CostFixed and CostVariable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost only depends on the vintage> it does not change with the period\n",
    "\n",
    "df_Fixed    = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='CostFixed')\n",
    "df_Variable = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='CostVariable')\n",
    "\n",
    "CostFixed_sql    = pd.DataFrame(columns=[\"regions\",\"periods\",\"tech\",\"vintage\",\"cost_fixed\",\"cost_fixed_units\",\"cost_fixed_notes\"])\n",
    "CostVariable_sql = pd.DataFrame(columns=[\"regions\",\"periods\",\"tech\",\"vintage\",\"cost_variable\",\"cost_variable_units\",\"cost_variable_notes\"])\n",
    "\n",
    "\n",
    "#Future Tech\n",
    "for region, tech in zip(UniqueFutureTech[\"regions\"], UniqueFutureTech[\"tech\"]):\n",
    "\n",
    "    df_tmp_CostFixed_sql    = df_Fixed[df_Fixed[\"tech\"]==tech]\n",
    "    df_tmp_CostVariable_sql = df_Variable[df_Variable[\"tech\"]==tech]\n",
    "\n",
    "\n",
    "    if np.sum(df_tmp_CostFixed_sql[\"Stage\"]==UserScenario)!=0:\n",
    "        df_tmp_CostFixed_sql=df_tmp_CostFixed_sql[df_tmp_CostFixed_sql[\"Stage\"]==UserScenario]\n",
    "\n",
    "        #If you cant find UserScenario you assign \"Single\" as the scenario\n",
    "    else:\n",
    "        df_tmp_CostFixed_sql=df_tmp_CostFixed_sql[df_tmp_CostFixed_sql[\"Stage\"]==\"Single\"]\n",
    "        \n",
    "\n",
    "    if np.sum(df_tmp_CostVariable_sql[\"Stage\"]==UserScenario)!=0:\n",
    "        df_tmp_CostVariable_sql=df_tmp_CostVariable_sql[df_tmp_CostVariable_sql[\"Stage\"]==UserScenario]\n",
    "\n",
    "        #If you cant find UserScenario you assign \"Single\" as the scenario\n",
    "    else:\n",
    "        df_tmp_CostVariable_sql=df_tmp_CostVariable_sql[df_tmp_CostVariable_sql[\"Stage\"]==\"Single\"]\n",
    "    \n",
    "\n",
    "    if len(df_tmp_CostFixed_sql)==0:\n",
    "        print(\"Error: There are no values for the technology \"+tech+\" in the CostFixed sheet\")\n",
    "        print(\"Look at the path: \" + UserDataPath+\"UserDataPart1.xlsx\")\n",
    "        break\n",
    "    if len(df_tmp_CostVariable_sql)==0:\n",
    "        print(\"Error: There are no values for the technology \"+tech+\" in the CostVariable sheet\")\n",
    "        print(\"Look at the path: \" + UserDataPath+\"UserDataPart1.xlsx\")\n",
    "        break\n",
    "\n",
    "    #Fixed Cost\n",
    "    for i in range(len(FutureYears[:-1])):\n",
    "        \n",
    "        vintage=FutureYears[i]\n",
    "        cost_fixed=df_tmp_CostFixed_sql[vintage].iloc[0]\n",
    "        cost_fixed_units    = df_tmp_CostFixed_sql   [\"cost_fixed_units\"].iloc[0]\n",
    "        cost_fixed_notes    = df_tmp_CostFixed_sql   [\"cost_fixed_notes\"].iloc[0]\n",
    "\n",
    "        for period in FutureYears[i:-1]:#Does not vary with the  period\n",
    "            Data=[[region, period, tech, vintage, cost_fixed, cost_fixed_units, cost_fixed_notes]]\n",
    "            CostFixed_sql=pd.concat([CostFixed_sql,pd.DataFrame(Data,columns=CostFixed_sql.columns)],ignore_index=True)  \n",
    "    \n",
    "    #Variable Cost .\n",
    "    if df_tmp_CostVariable_sql[\"Costs For: Vintage=1\\nPeriod=0\"].iloc[0]==1:#Same costs for all periods\n",
    "        for i in range(len(FutureYears[:-1])):\n",
    "            \n",
    "            vintage=FutureYears[i]\n",
    "            cost_variable=df_tmp_CostVariable_sql[vintage].iloc[0] #Get costs as vintage costs\n",
    "            cost_variable_units = df_tmp_CostVariable_sql[\"cost_variable_units\"].iloc[0]\n",
    "            cost_variable_notes = df_tmp_CostVariable_sql[\"cost_variable_notes\"].iloc[0]\n",
    "\n",
    "            for period in FutureYears[i:-1]:#Does not vary with the  period\n",
    "                Data=[[region, period, tech, vintage, cost_variable, cost_variable_units, cost_variable_notes]]\n",
    "                CostVariable_sql=pd.concat([CostVariable_sql,pd.DataFrame(Data,columns=CostVariable_sql.columns)],ignore_index=True)  \n",
    "                \n",
    "    if df_tmp_CostVariable_sql[\"Costs For: Vintage=1\\nPeriod=0\"].iloc[0]==0:#Cost only depends on the period (eg. Fuel technology)\n",
    "        for i in range(len(FutureYears[:-1])):\n",
    "            vintage=FutureYears[i]\n",
    "            cost_variable_units = df_tmp_CostVariable_sql[\"cost_variable_units\"].iloc[0]\n",
    "            cost_variable_notes = df_tmp_CostVariable_sql[\"cost_variable_notes\"].iloc[0]\n",
    "\n",
    "            for period in FutureYears[i:-1]:\n",
    "\n",
    "                cost_variable=df_tmp_CostVariable_sql[period].iloc[0] # Get costs as period costs\n",
    "                Data=[[region, period, tech, vintage, cost_variable, cost_variable_units, cost_variable_notes]]\n",
    "                CostVariable_sql=pd.concat([CostVariable_sql,pd.DataFrame(Data,columns=CostVariable_sql.columns)],ignore_index=True)              \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existing Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existing Tech\n",
    "UniqueExistingTech=ExistingCapacity_sql.groupby([\"regions\",\"tech\",\"vintage\"]).size().reset_index().rename(columns={0:'Occurances'}) \n",
    "UniqueExistingTech=UniqueExistingTech.drop(columns=[\"Occurances\"])\n",
    "\n",
    "\n",
    "for region, tech, vintage in zip(UniqueExistingTech[\"regions\"], UniqueExistingTech[\"tech\"],UniqueExistingTech[\"vintage\"]):\n",
    "\n",
    "    df_tmp_CostFixed_sql    = df_Fixed[df_Fixed[\"tech\"]==tech]\n",
    "    df_tmp_CostVariable_sql = df_Variable[df_Variable[\"tech\"]==tech]\n",
    "\n",
    "\n",
    "    if np.sum(df_tmp_CostFixed_sql[\"Stage\"]==UserScenario)!=0:\n",
    "        df_tmp_CostFixed_sql=df_tmp_CostFixed_sql[df_tmp_CostFixed_sql[\"Stage\"]==UserScenario]\n",
    "\n",
    "        #If you cant find UserScenario you assign \"Single\" as the scenario\n",
    "    else:\n",
    "        df_tmp_CostFixed_sql=df_tmp_CostFixed_sql[df_tmp_CostFixed_sql[\"Stage\"]==\"Single\"]\n",
    "        \n",
    "\n",
    "    if np.sum(df_tmp_CostVariable_sql[\"Stage\"]==UserScenario)!=0:\n",
    "        df_tmp_CostVariable_sql=df_tmp_CostVariable_sql[df_tmp_CostVariable_sql[\"Stage\"]==UserScenario]\n",
    "\n",
    "        #If you cant find UserScenario you assign \"Single\" as the scenario\n",
    "    else:\n",
    "        df_tmp_CostVariable_sql=df_tmp_CostVariable_sql[df_tmp_CostVariable_sql[\"Stage\"]==\"Single\"]\n",
    "    \n",
    "\n",
    "    if len(df_tmp_CostFixed_sql)==0:\n",
    "        print(\"Error: There are no values for the technology \"+tech+\" in the CostFixed sheet\")\n",
    "        print(\"Look at the path: \" + UserDataPath+\"UserDataPart1.xlsx\")\n",
    "        break\n",
    "    if len(df_tmp_CostVariable_sql)==0:\n",
    "        print(\"Error: There are no values for the technology \"+tech+\" in the CostVariable sheet\")\n",
    "        print(\"Look at the path: \" + UserDataPath+\"UserDataPart1.xlsx\")\n",
    "        break\n",
    "\n",
    "    \n",
    "    #Fixed Cost:  #Does not vary with the  period only depends on the vintage\n",
    "    cost_fixed=df_tmp_CostFixed_sql.iloc[:,4].values[0] #For existing tech the cost is the costs of the earliest vintage year (2020 for the NC database)\n",
    "    cost_fixed_units    = df_tmp_CostFixed_sql   [\"cost_fixed_units\"].iloc[0]\n",
    "    cost_fixed_notes    = df_tmp_CostFixed_sql   [\"cost_fixed_notes\"].iloc[0]\n",
    "\n",
    "    for period in FutureYears[:-1]:\n",
    "        Data=[[region, period, tech, vintage, cost_fixed, cost_fixed_units, cost_fixed_notes]]\n",
    "        CostFixed_sql=pd.concat([CostFixed_sql,pd.DataFrame(Data,columns=CostFixed_sql.columns)],ignore_index=True)  \n",
    "\n",
    "\n",
    "    #Variable Cost:\n",
    "    cost_variable_notes = df_tmp_CostVariable_sql[\"cost_variable_notes\"].iloc[0]\n",
    "    cost_variable_units = df_tmp_CostVariable_sql[\"cost_variable_units\"].iloc[0]\n",
    "    \n",
    "    if df_tmp_CostVariable_sql[\"Costs For: Vintage=1\\nPeriod=0\"].iloc[0]==1:#Same costs for all periods, only depends on the vintage\n",
    "        cost_variable=df_tmp_CostVariable_sql.iloc[:,5].values[0]#the cost is the costs of the earliest vintage year (2020 for the NC database)\n",
    "        for period in FutureYears[:-1]:\n",
    "            Data=[[region, period, tech, vintage, cost_variable, cost_variable_units, cost_variable_notes]]\n",
    "            CostVariable_sql=pd.concat([CostVariable_sql,pd.DataFrame(Data,columns=CostVariable_sql.columns)],ignore_index=True)  \n",
    "\n",
    "    if df_tmp_CostVariable_sql[\"Costs For: Vintage=1\\nPeriod=0\"].iloc[0]==0:#Costs only depends on the period (eg. Fuel technology)\n",
    "        for period in FutureYears[:-1]:\n",
    "            cost_variable=df_tmp_CostVariable_sql.iloc[period].iloc[0] # Get costs as period costs\n",
    "            Data=[[region, period, tech, vintage, cost_variable, cost_variable_units, cost_variable_notes]]\n",
    "            CostVariable_sql=pd.concat([CostVariable_sql,pd.DataFrame(Data,columns=CostVariable_sql.columns)],ignore_index=True)  \n",
    "    \n",
    "\n",
    "#Convert $/MWh to M$/PJ\n",
    "CostVariable_sql.loc[CostVariable_sql[\"cost_variable_units\"]==\"$/MWh\",\"cost_variable\"]=CostVariable_sql.loc[CostVariable_sql[\"cost_variable_units\"]==\"$/MWh\",\"cost_variable\"]*1/3.6\n",
    "CostVariable_sql.loc[CostVariable_sql[\"cost_variable_units\"]==\"$/MWh\",\"cost_variable_units\"]=\"M$/PJ\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for Consistency in Life Tech (Eliminate costs for tech tech \"retired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for year and lifetime\n",
    "KeepRows=[]\n",
    "for (region, period, tech, vintage) in zip(CostVariable_sql[\"regions\"], CostVariable_sql[\"periods\"], CostVariable_sql[\"tech\"],CostVariable_sql[\"vintage\"]):\n",
    "    LifeOfTech=LifetimeTech_sql[(LifetimeTech_sql[\"tech\"]==tech) * (LifetimeTech_sql[\"Regions\"]==region)][\"life\"].iloc[0]\n",
    "\n",
    "    if (vintage+LifeOfTech)<=period:\n",
    "        KeepRows.append(False)\n",
    "\n",
    "    else:\n",
    "        KeepRows.append(True)\n",
    "\n",
    "CostVariable_sql=CostVariable_sql[KeepRows]\n",
    "CostFixed_sql=CostFixed_sql[KeepRows]\n",
    "\n",
    "#Filter for zeros (transmission has default values of zero) If you run this line you will get an error\n",
    "# CostVariable_sql=CostVariable_sql[CostVariable_sql[\"cost_variable\"]>0] \n",
    "# CostFixed_sql=CostFixed_sql[CostFixed_sql[\"cost_fixed\"]>0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transmission/Distribution System Costs and configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a system with three regions (R1-R2-R3) we consider the theoretical scenario where a single transmission system connects R1->R2 and R2->R3.\n",
    "# As such the following connections must be defined:\n",
    "# R1->R1 # R2->R2 # R3->R3\n",
    "# R1->R2 # R2->R3\n",
    "\n",
    "# On this theoretical framework we can use the current demand values of each region and the existing generation capacity and costs\n",
    "# to calculate the size of each transmission system: This is done by running TEMOA with a single future period (2020-2022) \n",
    "\n",
    "# Additional data you need: \n",
    "# Total Transmission System Cost North Carolina\n",
    "# Transmission system cost of interconnections ($/GW) R1>R2, R2>R3\n",
    "\n",
    "# On temoa the transmission system costs R1<>R2 is divided by 2 to account for the fact that the transmission system is bidirectional\n",
    "# As such we have two transmission lines R1->R2 and R2->R1. There is a constraint in the model to enforce that the two lines have the same capacity\n",
    "\n",
    "#We represent the transmission in the same region by using an intermediate commodity. R1>R1, R2>R2, R3>R3\n",
    "\n",
    "#------------------------------------------------------------------------------------------- Flow Diagram for the Transmission System ----------------------------------------------------------------------------------\n",
    "#\n",
    "Image(UserDataPath+\"TransmissionConfiguration.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Load Center in Each Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load center is defined based on the population of each county on the region (it is the same of the centroid of the population)\n",
    "#shapefile_Pop and PointsPop is available at Demand chapter\n",
    "PointsPopLongLat=np.array([[shapefile_Pop[\"LONGITUDE\"][i],shapefile_Pop[\"LATITUDE\"][i]] for i in range(len(shapefile_Pop))])\n",
    "ListCensusPop=np.array(ListCensusPop)\n",
    "\n",
    "CentroidLoadR1=np.sum(np.reshape(ListCensusPop[ListCensusOnR1]/np.sum(ListCensusPop[ListCensusOnR1]),(len(ListCensusOnR1),1))*PointsPopLongLat[ListCensusOnR1],axis=0)#Long Lat\n",
    "CentroidLoadR2=np.sum(np.reshape(ListCensusPop[ListCensusOnR2]/np.sum(ListCensusPop[ListCensusOnR2]),(len(ListCensusOnR2),1))*PointsPopLongLat[ListCensusOnR2],axis=0)#Long Lat\n",
    "CentroidLoadR3=np.sum(np.reshape(ListCensusPop[ListCensusOnR3]/np.sum(ListCensusPop[ListCensusOnR3]),(len(ListCensusOnR3),1))*PointsPopLongLat[ListCensusOnR3],axis=0)#Long Lat\n",
    "\n",
    "#Computing distance between centroids\n",
    "DistanceR1R2=haversine((CentroidLoadR1[1], CentroidLoadR1[0]), (CentroidLoadR2[1], CentroidLoadR2[0]), unit='mi') #Haversine works on lat long\n",
    "DistanceR2R3=haversine((CentroidLoadR2[1], CentroidLoadR2[0]), (CentroidLoadR3[1], CentroidLoadR3[0]), unit='mi') #Haversine works on lat long\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "count=0\n",
    "for geom in new_polyR1.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    count=count+1\n",
    "    if count==1:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='r', ec='none',label=\"R1\")\n",
    "    else:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='r', ec='none')\n",
    "\n",
    "count=0\n",
    "for geom in new_polyR2.geoms:    \n",
    "    count=count+1\n",
    "    xs, ys = geom.exterior.xy    \n",
    "    if count==1:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='yellow', ec='none',label=\"R2\")\n",
    "    else:\n",
    "        ax.fill(xs, ys, alpha=0.5, fc='yellow', ec='none')\n",
    "\n",
    "\n",
    "xs, ys = new_polyR3.exterior.xy    \n",
    "ax.fill(xs, ys, alpha=0.5, fc='green', ec='none',label=\"R3\")\n",
    "\n",
    "LongLatCentroids=np.concatenate([np.array([CentroidLoadR1]),np.array([CentroidLoadR2]),np.array([CentroidLoadR3])],axis=0)\n",
    "\n",
    "plt.scatter(df_AggregateGen[\"Longitude\"],df_AggregateGen[\"Latitude\"],color=\"black\",s=1, label=\"Ex. Generators\")\n",
    "plt.scatter(LongLatCentroids[:,0],LongLatCentroids[:,1],color=\"blue\",s=15,marker=\"v\", label=\"Load Centers\")\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.legend()\n",
    "plt.savefig(\"OutputData/Figures/Load Centroids.png\",dpi=600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CentroidLoadR1: close to Wilmington\n",
    "#CentroidLoadR2: Goldsboro\n",
    "#CentroidLoadR3: Lexington"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs (Investment Fixed and Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from excel\n",
    "#The transmission cost between regions is defined based on ReEDS data and DUKE data (Miles & KV of transmission line)\n",
    "#Tp compute it we use the diststance between centroids and the average transmission cost per mile per MW\n",
    "TransmissionExcel = pd.read_excel(UserDataPath+\"UserDataPart1.xlsx\", sheet_name ='TransmissionCalculations')\n",
    "AverageTranmsissionCost=TransmissionExcel.iloc[10,1] #[$/(MW*Mile)]\n",
    "InvestmentCost_TransmissionR1R2=DistanceR1R2*AverageTranmsissionCost/1000 #[M$/GW]=[$/kW]\n",
    "InvestmentCost_TransmissionR2R3=DistanceR2R3*AverageTranmsissionCost/1000 #[M$/GW]\n",
    "InvestmentCost_TransmissionR1R3=InvestmentCost_TransmissionR1R2+InvestmentCost_TransmissionR2R3 #Installing capacity in this line to connect R1 and R3 is the same as installing capacity in R1R2 and R2R3. We use this\n",
    "#representation to facilitate the model formulation on temoa\n",
    "\n",
    "#Regional Investment Cost\n",
    "InvestmentCost_TransmissionR1R1=TransmissionExcel.iloc[11,1] #[M$/GW]=[$/kW]\n",
    "InvestmentCost_TransmissionR2R2=TransmissionExcel.iloc[11,1] #[M$/GW]=[$/kW]\n",
    "InvestmentCost_TransmissionR3R3=TransmissionExcel.iloc[11,1] #[M$/GW]=[$/kW]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change CostInvest_sql (Regional transmission)\n",
    "CostInvest_sql.loc[(CostInvest_sql[\"tech\"]==\"TRANSMISSION_REGIONAL\") & (CostInvest_sql[\"regions\"]==\"R1\"),\"cost_invest\"]=InvestmentCost_TransmissionR1R1\n",
    "CostInvest_sql.loc[(CostInvest_sql[\"tech\"]==\"TRANSMISSION_REGIONAL\") & (CostInvest_sql[\"regions\"]==\"R2\"),\"cost_invest\"]=InvestmentCost_TransmissionR2R2\n",
    "CostInvest_sql.loc[(CostInvest_sql[\"tech\"]==\"TRANSMISSION_REGIONAL\") & (CostInvest_sql[\"regions\"]==\"R3\"),\"cost_invest\"]=InvestmentCost_TransmissionR3R3\n",
    "\n",
    "\n",
    "#CostInvest_sql (Interregional transmission)\n",
    "#Delete Transmission_Interregional on CostInvest_sql and rebuild properly on TEMOA notation\n",
    "CostInvest_sql=CostInvest_sql[CostInvest_sql[\"tech\"]!=\"TRANSMISSION_INTERREGIONAL\"]\n",
    "\n",
    "Data=[]\n",
    "for vintage in FutureYears[:-1]:\n",
    "    Data.append([\"R1-R2\", \"TRANSMISSION_INTERREGIONAL\", vintage, InvestmentCost_TransmissionR1R2, \"M$/GW\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "    Data.append([\"R2-R1\", \"TRANSMISSION_INTERREGIONAL\", vintage, InvestmentCost_TransmissionR1R2, \"M$/GW\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "    Data.append([\"R2-R3\", \"TRANSMISSION_INTERREGIONAL\", vintage, InvestmentCost_TransmissionR2R3, \"M$/GW\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "    Data.append([\"R3-R2\", \"TRANSMISSION_INTERREGIONAL\", vintage, InvestmentCost_TransmissionR2R3, \"M$/GW\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "    Data.append([\"R1-R3\", \"TRANSMISSION_INTERREGIONAL\", vintage, InvestmentCost_TransmissionR1R3, \"M$/GW\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "    Data.append([\"R3-R1\", \"TRANSMISSION_INTERREGIONAL\", vintage, InvestmentCost_TransmissionR1R3, \"M$/GW\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "\n",
    "CostInvest_sql=pd.concat([CostInvest_sql,pd.DataFrame(Data,columns=CostInvest_sql.columns)],ignore_index=True)      \n",
    "\n",
    "#CostVariable_sql (Interregional transmission)\n",
    "#Delete Transmission_Interregional on CostVariable_sql and rebuild properly on TEMOA notation\n",
    "CostVariable_sql=CostVariable_sql[CostVariable_sql[\"tech\"]!=\"TRANSMISSION_INTERREGIONAL\"]\n",
    "VariableCost_InterTransmission=0 #[M$/PJ]=1/3.2*[$/Whr]\n",
    "Data=[]\n",
    "\n",
    "YearsToConsider=list([Vintages[0]])+list(FutureYears[:-1])\n",
    "for i in range(len(YearsToConsider)):\n",
    "    vintage=YearsToConsider[i]\n",
    "\n",
    "    PeriodsToConsider=np.array(FutureYears[:-1])\n",
    "    PeriodsToConsider=PeriodsToConsider[PeriodsToConsider>=vintage]\n",
    "\n",
    "    for period in list(PeriodsToConsider):\n",
    "        Data.append([\"R1-R2\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, VariableCost_InterTransmission, \"M$/PJ\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R2-R1\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, VariableCost_InterTransmission, \"M$/PJ\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R2-R3\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, VariableCost_InterTransmission, \"M$/PJ\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R3-R2\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, VariableCost_InterTransmission, \"M$/PJ\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R1-R3\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, VariableCost_InterTransmission, \"M$/PJ\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R3-R1\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, VariableCost_InterTransmission, \"M$/PJ\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "\n",
    "CostVariable_sql=pd.concat([CostVariable_sql,pd.DataFrame(Data,columns=CostVariable_sql.columns)],ignore_index=True)      \n",
    "\n",
    "#CostFixed_sql (Interregional transmission)\n",
    "#Delete Transmission_Interregional on CostVariable_sql and rebuild properly on TEMOA notation\n",
    "CostFixed_sql=CostFixed_sql[CostFixed_sql[\"tech\"]!=\"TRANSMISSION_INTERREGIONAL\"]\n",
    "FixedCost_InterTransmission=0\n",
    "Data=[]\n",
    "for i in range(len(YearsToConsider)):\n",
    "    vintage=YearsToConsider[i]\n",
    "    PeriodsToConsider=np.array(FutureYears[:-1])\n",
    "    PeriodsToConsider=PeriodsToConsider[PeriodsToConsider>=vintage]\n",
    "    for period in list(PeriodsToConsider):\n",
    "        Data.append([\"R1-R2\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, FixedCost_InterTransmission, \"M$/GWyr\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R2-R1\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, FixedCost_InterTransmission, \"M$/GWyr\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R2-R3\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, FixedCost_InterTransmission, \"M$/GWyr\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R3-R2\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, FixedCost_InterTransmission, \"M$/GWyr\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R1-R3\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, FixedCost_InterTransmission, \"M$/GWyr\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "        Data.append([\"R3-R1\",period, \"TRANSMISSION_INTERREGIONAL\", vintage, FixedCost_InterTransmission, \"M$/GWyr\", \"Modified Automatically from the Code (Values on the right are Default Value)\"])\n",
    "\n",
    "CostFixed_sql=pd.concat([CostFixed_sql,pd.DataFrame(Data,columns=CostFixed_sql.columns)],ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Lifetech for Interregional Transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For interregional transmission we need to properly define the regions using TEMOA \"dash\" notation\n",
    "ExcelData=LifetimeTech_sql[(LifetimeTech_sql[\"tech\"]==\"TRANSMISSION_INTERREGIONAL\") & (LifetimeTech_sql[\"Regions\"]==\"R3\")]\n",
    "life=ExcelData[\"life\"].values[0]\n",
    "life_notes=ExcelData[\"life_notes\"].values[0]\n",
    "\n",
    "LifetimeTech_sql=LifetimeTech_sql[LifetimeTech_sql[\"tech\"]!=\"TRANSMISSION_INTERREGIONAL\"]\n",
    "Data=[]\n",
    "Data.append([\"R1-R2\", \"TRANSMISSION_INTERREGIONAL\", life, life_notes])\n",
    "Data.append([\"R2-R1\", \"TRANSMISSION_INTERREGIONAL\", life, life_notes])\n",
    "Data.append([\"R2-R3\", \"TRANSMISSION_INTERREGIONAL\", life, life_notes])\n",
    "Data.append([\"R3-R2\", \"TRANSMISSION_INTERREGIONAL\", life, life_notes])\n",
    "Data.append([\"R1-R3\", \"TRANSMISSION_INTERREGIONAL\", life, life_notes])\n",
    "Data.append([\"R3-R1\", \"TRANSMISSION_INTERREGIONAL\", life, life_notes])\n",
    "\n",
    "LifetimeTech_sql=pd.concat([LifetimeTech_sql,pd.DataFrame(Data,columns=LifetimeTech_sql.columns)],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for year and lifetime\n",
    "KeepRows=[]\n",
    "for (region, period, tech, vintage) in zip(CostVariable_sql[\"regions\"], CostVariable_sql[\"periods\"], CostVariable_sql[\"tech\"],CostVariable_sql[\"vintage\"]):\n",
    "    LifeOfTech=LifetimeTech_sql[(LifetimeTech_sql[\"tech\"]==tech) * (LifetimeTech_sql[\"Regions\"]==region)][\"life\"].iloc[0]\n",
    "\n",
    "    if (vintage+LifeOfTech)<=period and period<vintage:\n",
    "        KeepRows.append(False)\n",
    "\n",
    "    else:\n",
    "        KeepRows.append(True)\n",
    "\n",
    "CostVariable_sql=CostVariable_sql[KeepRows]\n",
    "CostFixed_sql=CostFixed_sql[KeepRows]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existing Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All existing transmission capacity is assumed to be in place in the earlies vintage year (all capacity on 2022)\n",
    "\n",
    "#TEMOA has a constrain to enforce CapacityR1R2=CapacityR2R1,...\n",
    "#Transmission_Interregional\n",
    "ExistingCap_TransmissionR1R2=0.1 #[GW]\n",
    "ExistingCap_TransmissionR2R3=0.1 #[GW]\n",
    "ExistingCap_TransmissionR1R3=0.1 #[GW]\n",
    "\n",
    "#Transmission_Regional\n",
    "ExistingCap_TransmissionR1R1=0.1 #[GW]\n",
    "ExistingCap_TransmissionR2R2=0.1 #[GW]\n",
    "ExistingCap_TransmissionR3R3=0.1 #[GW]\n",
    "\n",
    "#Need to define\n",
    "Data=[]\n",
    "Data.append([\"R1-R2\", \"TRANSMISSION_INTERREGIONAL\", Vintages[0], ExistingCap_TransmissionR1R2,\t\"GW\",\t\"From Database Code\"])\n",
    "Data.append([\"R2-R1\", \"TRANSMISSION_INTERREGIONAL\", Vintages[0], ExistingCap_TransmissionR1R2,\t\"GW\",\t\"From Database Code\"])\n",
    "\n",
    "Data.append([\"R1-R3\", \"TRANSMISSION_INTERREGIONAL\", Vintages[0], ExistingCap_TransmissionR2R3,\t\"GW\",\t\"From Database Code\"])\n",
    "Data.append([\"R3-R1\", \"TRANSMISSION_INTERREGIONAL\", Vintages[0], ExistingCap_TransmissionR2R3,\t\"GW\",\t\"From Database Code\"])\n",
    "\n",
    "Data.append([\"R2-R3\", \"TRANSMISSION_INTERREGIONAL\", Vintages[0], ExistingCap_TransmissionR1R3,\t\"GW\",\t\"From Database Code\"])\n",
    "Data.append([\"R3-R2\", \"TRANSMISSION_INTERREGIONAL\", Vintages[0], ExistingCap_TransmissionR1R3,\t\"GW\",\t\"From Database Code\"])\n",
    "\n",
    "Data.append([\"R1\", \"TRANSMISSION_REGIONAL\", Vintages[0], ExistingCap_TransmissionR1R1,\t\"GW\",\t\"From Database Code\"])\n",
    "Data.append([\"R2\", \"TRANSMISSION_REGIONAL\", Vintages[0], ExistingCap_TransmissionR2R2,\t\"GW\",\t\"From Database Code\"])\n",
    "Data.append([\"R3\", \"TRANSMISSION_REGIONAL\", Vintages[0], ExistingCap_TransmissionR3R3,\t\"GW\",\t\"From Database Code\"])\n",
    "\n",
    "ExistingCapacity_sql=pd.concat([ExistingCapacity_sql,pd.DataFrame(Data,columns=ExistingCapacity_sql.columns)],ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tech_exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_exchange_sql=pd.DataFrame([[\"TRANSMISSION_INTERREGIONAL\", \"Transmission Interregions eg. R1>R2\"]],columns=[\"tech\",\"notes\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Convert Excel Table to Sql data Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## commodity_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from excel\n",
    "ExcelDataP2 = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='commodity_labels')\n",
    "commodity_labels_sql=pd.DataFrame(ExcelDataP2.values,columns=[\"comm_labels\",\"comm_labels_desc\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from excel\n",
    "ExcelDataP2 = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='commodities')\n",
    "commodities_sql=pd.DataFrame(ExcelDataP2.values,columns=[\"comm_name\",\"flag\",\"comm_desc\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Efficiencies from Excel user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the df for all valid combinations\n",
    "#You can get all valid combinations for the future tech using CostInvest_sql\n",
    "DummyString=np.array(['']*CostInvest_sql.shape[0])\n",
    "DummyNumb=np.ones((CostInvest_sql.shape[0]))*-1\n",
    "DataFuture=np.stack((CostInvest_sql[\"regions\"].values,DummyString,CostInvest_sql[\"tech\"].values,CostInvest_sql[\"vintage\"].values,DummyString,DummyNumb,DummyString),axis=1)\n",
    "\n",
    "#You can get all valid combinations for exising tech using ExistingCapacity_sql\n",
    "DummyString=np.array(['']*ExistingCapacity_sql.shape[0])\n",
    "DummyNumb=np.ones((ExistingCapacity_sql.shape[0]))*-1\n",
    "DataPast=np.stack((ExistingCapacity_sql[\"regions\"].values,DummyString,ExistingCapacity_sql[\"tech\"].values,ExistingCapacity_sql[\"vintage\"].values,DummyString,DummyNumb,DummyString),axis=1)\n",
    "\n",
    "\n",
    "Efficiency_sql=pd.DataFrame(np.concatenate((DataFuture,DataPast),axis=0),columns=[\"regions\",\"input_comm\",\"tech\",\"vintage\",\"output_comm\",\"efficiency\",\"eff_notes\"]) #Skelleton for the efficiency table\n",
    "Efficiency_sql[\"units\"]=\"\"\n",
    "\n",
    "#Transfer the data from the excel file to the dataframe\n",
    "ExcelDataP2 = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='Efficiency')\n",
    "ExcelDataP2=ExcelDataP2[(ExcelDataP2[\"Stage\"]==\"single\") + (ExcelDataP2[\"Stage\"]==UserScenario)]\n",
    "\n",
    "#Transfer Input and output commodities\n",
    "for tech, imput_comm, output_comm, units in zip(ExcelDataP2[\"tech\"],ExcelDataP2[\"input_comm\"],ExcelDataP2[\"output_comm\"],ExcelDataP2[\"eff_units\"]):\n",
    "    Efficiency_sql.loc[Efficiency_sql[\"tech\"]==tech,\"input_comm\"]=imput_comm\n",
    "    Efficiency_sql.loc[Efficiency_sql[\"tech\"]==tech,\"output_comm\"]=output_comm\n",
    "    Efficiency_sql.loc[Efficiency_sql[\"tech\"]==tech,\"units\"]=units\n",
    "\n",
    "for index, ExcelRowEffData in ExcelDataP2.iterrows():\n",
    "    region=ExcelRowEffData[\"Region\"]\n",
    "    tech=ExcelRowEffData[\"tech\"]\n",
    "    eff_note=ExcelRowEffData[\"eff_notes\"]\n",
    "\n",
    "    if region==\"single\":\n",
    "        Tech_vitages=Efficiency_sql.loc[(Efficiency_sql[\"tech\"]==tech),\"vintage\"].values\n",
    "        LastYearInEfficiencyTable=ExcelDataP2.columns[7]\n",
    "        Tech_vitages[Tech_vitages<LastYearInEfficiencyTable]=LastYearInEfficiencyTable\n",
    "        \n",
    "        efficiency=[ExcelRowEffData[v] for v in Tech_vitages]\n",
    "        Efficiency_sql.loc[(Efficiency_sql[\"tech\"]==tech),\"efficiency\"]=efficiency\n",
    "        Efficiency_sql.loc[(Efficiency_sql[\"tech\"]==tech),\"eff_notes\"]=eff_note\n",
    "\n",
    "    if region!=\"single\":\n",
    "        Tech_vitages=Efficiency_sql.loc[(Efficiency_sql[\"tech\"]==tech) * (Efficiency_sql[\"regions\"]==region),\"vintage\"].values\n",
    "        LastYearInEfficiencyTable=ExcelDataP2.columns[7]\n",
    "        Tech_vitages[Tech_vitages<LastYearInEfficiencyTable]=LastYearInEfficiencyTable\n",
    "\n",
    "        efficiency=[ExcelRowEffData[v] for v in Tech_vitages]\n",
    "        Efficiency_sql.loc[(Efficiency_sql[\"tech\"]==tech) * (Efficiency_sql[\"regions\"]==region) ,\"efficiency\"]=efficiency\n",
    "        Efficiency_sql.loc[(Efficiency_sql[\"tech\"]==tech) * (Efficiency_sql[\"regions\"]==region) ,\"eff_notes\"]=eff_note\n",
    "\n",
    "    if tech==\"TRANSMISSION_INTERREGIONAL\": #(Exception on Interregional Transmission) Same in both directions\n",
    "        InvertRegion=region.split(\"-\")[1]+\"-\"+region.split(\"-\")[0]\n",
    "        Efficiency_sql.loc[(Efficiency_sql[\"tech\"]==tech) * (Efficiency_sql[\"regions\"]==InvertRegion) ,\"efficiency\"]=efficiency\n",
    "        Efficiency_sql.loc[(Efficiency_sql[\"tech\"]==tech) * (Efficiency_sql[\"regions\"]==InvertRegion) ,\"eff_notes\"]=eff_note\n",
    "\n",
    "\n",
    "#Verify that all the data is filled\n",
    "if np.sum(Efficiency_sql[\"efficiency\"]==-1)>0:\n",
    "    print(\"ERROR: There are some efficiency values that are not filled\")\n",
    "    print(Efficiency_sql.loc[Efficiency_sql[\"efficiency\"]==-1,:])\n",
    "    sys.exit()\n",
    "\n",
    "#Convert \"MWh/MMBTUs\" to \"PJ/MMBTUs\"\n",
    "Efficiency_sql.loc[Efficiency_sql[\"units\"]==\"MWh/MMBTUs\",\"efficiency\"]=Efficiency_sql.loc[Efficiency_sql[\"units\"]==\"MWh/MMBTUs\",\"efficiency\"]*3.41214\n",
    "Efficiency_sql.loc[Efficiency_sql[\"units\"]==\"MWh/MMBTUs\",\"units\"]=\"PJ/PJ\"\n",
    "\n",
    "Efficiency_sql[\"eff_notes\"]=Efficiency_sql[\"eff_notes\"]+\"__\"+Efficiency_sql[\"units\"]\n",
    "Efficiency_sql=Efficiency_sql.drop(columns=[\"units\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Efficiencies for existing tech using EIA 923 BTUs to MWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have to enter the value by hand on the efficiency table\n",
    "EIA923_df = pd.read_excel(EIA_923_Path,sheet_name ='Page 1 Generation and Fuel Data',skiprows=5)\n",
    "EIA923_State=EIA923_df[\"Plant State\"]\n",
    "EIA923_df=EIA923_df[EIA923_State==State]\n",
    "\n",
    "EIA923_df=EIA923_df[(EIA923_df[\"Net Generation\\n(Megawatthours)\"]>0)*(EIA923_df[\"Elec Fuel Consumption\\nMMBtu\"]>0)] #If no fuel was consumed you cannot calculate the efficiency\n",
    "\n",
    "#On EIA 860 combined cycle tech are divided in CT and CA (steam part)\n",
    "#We need to combine them as CC: combine cycle\n",
    "EIA923_df.loc[EIA923_df[\"Reported\\nPrime Mover\"]==\"CT\",\"Reported\\nPrime Mover\"]=\"CC\"\n",
    "EIA923_df.loc[EIA923_df[\"Reported\\nPrime Mover\"]==\"CA\",\"Reported\\nPrime Mover\"]=\"CC\"\n",
    "\n",
    "\n",
    "EIA923_Mover=EIA923_df[\"Reported\\nPrime Mover\"]\n",
    "EIA923_FuelType=EIA923_df[\"Reported\\nFuel Type Code\"]\n",
    "EIA923_FuelConsuption=EIA923_df[\"Elec Fuel Consumption\\nMMBtu\"]\n",
    "EIA923_EnergyGen=EIA923_df[\"Net Generation\\n(Megawatthours)\"]\n",
    "\n",
    "UniqueMoverFuel = EIA923_df[[\"Reported\\nFuel Type Code\",\"Reported\\nPrime Mover\"]].drop_duplicates()\n",
    "\n",
    "UniqueMoverFuel[\"MMBtu\"]=0\n",
    "UniqueMoverFuel[\"MWh\"]=0\n",
    "for Mover, Fuel in zip(UniqueMoverFuel.iloc[:,1],UniqueMoverFuel.iloc[:,0]):\n",
    "    Idx=(UniqueMoverFuel[\"Reported\\nPrime Mover\"]==Mover) * (UniqueMoverFuel[\"Reported\\nFuel Type Code\"]==Fuel)\n",
    "\n",
    "    UniqueMoverFuel.loc[Idx,\"MMBtu\"]=np.sum(EIA923_FuelConsuption[(EIA923_Mover==Mover) * (EIA923_FuelType==Fuel)])\n",
    "    UniqueMoverFuel.loc[Idx,\"MWh\"]=np.sum(EIA923_EnergyGen[(EIA923_Mover==Mover) * (EIA923_FuelType==Fuel)])\n",
    "\n",
    "UniqueMoverFuel[\"MWh/MMBtu\"]=UniqueMoverFuel[\"MWh\"]/UniqueMoverFuel[\"MMBtu\"]\n",
    "UniqueMoverFuel[\"MMBtu/MWh\"]=UniqueMoverFuel[\"MMBtu\"]/UniqueMoverFuel[\"MWh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueMoverFuel.sort_values(['Reported\\nFuel Type Code','Reported\\nPrime Mover'], ascending=(True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean given all biomass fuels\n",
    "BioFuel=[\"AB\",\"BLQ\",\"OBG\",\"WDS\"]\n",
    "BTU=0\n",
    "MWh=0\n",
    "for Fuel in BioFuel:\n",
    "    Idx=UniqueMoverFuel[\"Reported\\nFuel Type Code\"]==Fuel\n",
    "    BTU=BTU+np.sum(UniqueMoverFuel.loc[Idx,\"MMBtu\"])\n",
    "    MWh=MWh+np.sum(UniqueMoverFuel.loc[Idx,\"MWh\"])\n",
    "\n",
    "print(\"Average Biomass Efficiency MMBTU/MWh: %.4f\" % (BTU/MWh))\n",
    "\n",
    "#Mean of Tech Specific:\n",
    "Coal =[\"BIT\"]\n",
    "NG   =[\"NG\"]\n",
    "Petroleum=[\"DFO\",\"KER\", \"RFO\"]\n",
    "\n",
    "RefFuel=NG\n",
    "BTU=0\n",
    "MWh=0\n",
    "for Fuel in RefFuel:\n",
    "    Idx=UniqueMoverFuel[\"Reported\\nFuel Type Code\"]==Fuel\n",
    "    BTU=BTU+np.sum(UniqueMoverFuel.loc[Idx,\"MMBtu\"])\n",
    "    MWh=MWh+np.sum(UniqueMoverFuel.loc[Idx,\"MWh\"])\n",
    "\n",
    "print(\"\\n\\nAverage Tech Specific Efficiency MMBTU/MWh: %.4f\\nTech Specific Total MMBTUs: %.2f\" % (BTU/MWh,BTU))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmissionActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#From https://www.pfpi.net/wp-content/uploads/2018/04/PFPIetal.CapandTradeComments4.9.2018.pdf lb/MMBTU of Co2\n",
    "\n",
    "BioFuel_LbPerMMBTU={\"OBS\":233, #Other Biomass solid,\n",
    "                    \"OBG\":127, #Other Biomass Gas\n",
    "                    \"BLQ\":222, #Black Liquor\n",
    "                    \"WDS\":207, #Wood\n",
    "                    \"LFG\":130, #Landfill Gas\n",
    "                    \"AB\":207     #Agricultural Byproducts\n",
    "                    }\n",
    "\n",
    "TotalMMBtu=0\n",
    "AverageLbPerMMBTU=0\n",
    "for FT in list(BioFuel_LbPerMMBTU.keys()):\n",
    "    TotalMMBtu+=UniqueMoverFuel.loc[UniqueMoverFuel[\"Reported\\nFuel Type Code\"]==FT,\"MMBtu\"].sum()\n",
    "    AverageLbPerMMBTU+=UniqueMoverFuel.loc[UniqueMoverFuel[\"Reported\\nFuel Type Code\"]==FT,\"MMBtu\"].sum()*BioFuel_LbPerMMBTU[FT]\n",
    "AverageLbPerMMBTU=AverageLbPerMMBTU/TotalMMBtu\n",
    "\n",
    "print(\"Average Biomass Lb co2 /MMBTU: %.2f\" % (AverageLbPerMMBTU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the technologies we need to model efficiency from EmissionActivity on the excel file\n",
    "#An get the sql structure from Efficiency_sql\n",
    "ExcelDataP2 = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='EmissionActivity')\n",
    "\n",
    "EmissionActivity_sql=pd.DataFrame(columns=[\"regions\",\"emis_comm\",\"input_comm\",\"tech\",\"vintage\",\"output_comm\",\"emis_act\",\"emis_act_units\",\"emis_act_notes\"])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(ExcelDataP2[\"tech\"])):\n",
    "    tech=ExcelDataP2[\"tech\"][i]\n",
    "    MMBtus2Lbs=ExcelDataP2[\"emission_act\"][i]\n",
    "    units=ExcelDataP2[\"units\"][i]\n",
    "    emission_comm=ExcelDataP2[\"emission_comm\"][i]\n",
    "    IdxIn=Efficiency_sql[\"tech\"]==tech\n",
    "    Filtered_df_Emissions=Efficiency_sql[IdxIn]\n",
    "\n",
    "    #Get data we need from emissions considering the tech\n",
    "    regions=Filtered_df_Emissions[\"regions\"]\n",
    "    input_comm=Filtered_df_Emissions[\"input_comm\"]\n",
    "    tech=Filtered_df_Emissions[\"tech\"]\n",
    "    vintage=Filtered_df_Emissions[\"vintage\"]\n",
    "    output_comm=Filtered_df_Emissions[\"output_comm\"]\n",
    "    efficiency=Filtered_df_Emissions[\"efficiency\"]/3.41214 #Convert from PJ/PJ to[MWh/MMBtUs] >> 3.6/1.05505585262=3.41214\n",
    "\n",
    "    if units==\"lbs/MMBtu\":\n",
    "\n",
    "        emissions=MMBtus2Lbs*1/efficiency #[lbs/MWh]\n",
    "        emissions=emissions*0.453592/10**6 #[kt/MWh]\n",
    "        emissions=emissions/(3.6*10**-6) #[kt/PJ] #\n",
    "        new_units=\"kt/PJ\"\n",
    "        EmissionActivity_tmp_sql = pd.DataFrame({\"regions\":regions,\n",
    "                                    \"emis_comm\":emission_comm,\n",
    "                                    \"input_comm\":input_comm,\n",
    "                                    \"tech\":tech,\n",
    "                                    \"vintage\":vintage,\n",
    "                                    \"output_comm\":output_comm,\n",
    "                                    \"emis_act\":emissions,\n",
    "                                    \"emis_act_units\":new_units,\n",
    "                                    \"emis_act_notes\":\"Check Code\"})\n",
    "        \n",
    "        EmissionActivity_sql = pd.concat([EmissionActivity_sql, EmissionActivity_tmp_sql], ignore_index=True, sort=False)\n",
    "\n",
    "    if units==\"kt/kt\":\n",
    "        kt_kt=MMBtus2Lbs\n",
    "        emissions=kt_kt\n",
    "        new_units=\"kt/kt\"\n",
    "        EmissionActivity_tmp_sql = pd.DataFrame({\"regions\":regions,\n",
    "                                    \"emis_comm\":emission_comm,\n",
    "                                    \"input_comm\":input_comm,\n",
    "                                    \"tech\":tech,\n",
    "                                    \"vintage\":vintage,\n",
    "                                    \"output_comm\":output_comm,\n",
    "                                    \"emis_act\":emissions,\n",
    "                                    \"emis_act_units\":new_units,\n",
    "                                    \"emis_act_notes\":\"Check Code\"})\n",
    "        \n",
    "        EmissionActivity_sql = pd.concat([EmissionActivity_sql, EmissionActivity_tmp_sql], ignore_index=True, sort=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DemandSpecificDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from EIA-930 form (https://www.eia.gov/electricity/gridmonitor/dashboard/electric_overview/US48/US48)\n",
    "# Get hourly demand data from EIA froms to know how the demand changes for each season an hour of the day\n",
    "EIA_930_Paths = glob.glob(EIA_930_BalancePath + \"\\*.csv\")\n",
    "\n",
    "EIA_Data=pd.DataFrame(columns=[\"Year\",\"Season\",\"Hour\", \"Demand (MW)\"])\n",
    "for FilePath in EIA_930_Paths:\n",
    "    df = pd.read_csv(FilePath,usecols = [\"Balancing Authority\",'Data Date',\"Hour Number\",'Demand (MW)'],low_memory=False)\n",
    "\n",
    "    BAsIn=df[\"Balancing Authority\"]==MajorBAs[0] # Initialize the boolean array to select the BAs\n",
    "    for BA in MajorBAs:\n",
    "        BAsIn=(df[\"Balancing Authority\"]==BA) + BAsIn\n",
    "    df=df[BAsIn]\n",
    "\n",
    "    FileData=pd.DataFrame(columns=[\"Year\",\"Season\",\"Hour\", \"Demand (MW)\"])\n",
    "    FileData[\"Demand (MW)\"]=df[\"Demand (MW)\"].str.replace(',', '').astype(float)\n",
    "    FileData[\"Year\"]=pd.to_datetime(df[\"Data Date\"],format='%m/%d/%Y').dt.year\n",
    "    FileData[\"Hour\"]=df[\"Hour Number\"]\n",
    "\n",
    "    Month=pd.to_datetime(df[\"Data Date\"],format='%m/%d/%Y').dt.month\n",
    "    SeasonDf=[]\n",
    "    for month in Month:\n",
    "        for j in range(len(Seasons)):\n",
    "            if month in SeasonsMonthRange[j]:\n",
    "                SeasonDf.append(Seasons[j])\n",
    "                break\n",
    "    FileData[\"Season\"]=SeasonDf\n",
    "\n",
    "    EIA_Data = pd.concat([EIA_Data,FileData], ignore_index = True)\n",
    "\n",
    "    EIA_Data=EIA_Data[EIA_Data[\"Hour\"]!=25] #Remove the 25th hour. On the EIA some days have 25 hours. I don't know why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand_Season_Hour_agg=pd.DataFrame(columns=[\"Year\",\"Season\",\"Hour\", \"Demand (MW)\"])\n",
    "\n",
    "for Season in Seasons:\n",
    "    for Hour in EIA_Data[\"Hour\"].unique():\n",
    "        for Year in EIA_Data[\"Year\"].unique():\n",
    "            IdxIn=(EIA_Data[\"Season\"]==Season)*(EIA_Data[\"Hour\"]==Hour)*(EIA_Data[\"Year\"]==Year)\n",
    "            df=EIA_Data[IdxIn]\n",
    "            Demand=df[\"Demand (MW)\"].sum()\n",
    "            Demand_Season_Hour_agg=pd.concat([Demand_Season_Hour_agg,pd.DataFrame([{\"Year\":Year,\"Season\":Season,\"Hour\":Hour,\"Demand (MW)\":Demand}])], ignore_index=True)\n",
    "\n",
    "#Make demand as a percentage of the total demand on the year\n",
    "for Year in Demand_Season_Hour_agg[\"Year\"].unique():\n",
    "    IdxIn=Demand_Season_Hour_agg[\"Year\"]==Year\n",
    "    Demand_Season_Hour_agg.loc[IdxIn,\"Demand (MW)\"]=Demand_Season_Hour_agg.loc[IdxIn,\"Demand (MW)\"]/Demand_Season_Hour_agg.loc[IdxIn,\"Demand (MW)\"].sum()\n",
    "\n",
    "Demand_Season_Hour_agg=Demand_Season_Hour_agg.rename(columns={\"Demand (MW)\": \"RatioOfYearDemand\"})\n",
    "\n",
    "#Get the average Trend\n",
    "GeneralDemandDistribuion=pd.DataFrame(columns=[\"Season\",\"Hour\", \"RatioOfDemand\"])\n",
    "\n",
    "for Season in Seasons:\n",
    "    for Hour in Demand_Season_Hour_agg[\"Hour\"].unique():\n",
    "        IdxIn=(Demand_Season_Hour_agg[\"Season\"]==Season)*(Demand_Season_Hour_agg[\"Hour\"]==Hour)\n",
    "        df=Demand_Season_Hour_agg[IdxIn]\n",
    "        RatioOfDemand=df[\"RatioOfYearDemand\"].mean()#Mean of all years\n",
    "        GeneralDemandDistribuion=pd.concat([GeneralDemandDistribuion,pd.DataFrame([{\"Season\":Season,\"Hour\":Hour,\"RatioOfDemand\":RatioOfDemand}])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Demand Specific Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "G_S1Data=GeneralDemandDistribuion[GeneralDemandDistribuion[\"Season\"]==\"S1\"]\n",
    "G_S2Data=GeneralDemandDistribuion[GeneralDemandDistribuion[\"Season\"]==\"S2\"]\n",
    "G_S3Data=GeneralDemandDistribuion[GeneralDemandDistribuion[\"Season\"]==\"S3\"]\n",
    "G_S4Data=GeneralDemandDistribuion[GeneralDemandDistribuion[\"Season\"]==\"S4\"]\n",
    "x=G_S2Data[\"Hour\"].unique()\n",
    "\n",
    "MinY=-0.0005+min(G_S1Data[\"RatioOfDemand\"].min(),G_S2Data[\"RatioOfDemand\"].min(),G_S3Data[\"RatioOfDemand\"].min(),G_S4Data[\"RatioOfDemand\"].min())\n",
    "MaxY=0.001+max(G_S1Data[\"RatioOfDemand\"].max(),G_S2Data[\"RatioOfDemand\"].max(),G_S3Data[\"RatioOfDemand\"].max(),G_S4Data[\"RatioOfDemand\"].max())\n",
    "\n",
    "axs[0, 0].set_title('S1-Winter')\n",
    "axs[0, 1].set_title('S2-Spring')\n",
    "axs[1, 0].set_title('S3- Summer')\n",
    "axs[1, 1].set_title('S4-Autumn')\n",
    "\n",
    "axs[0, 0].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[0, 1].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[1, 0].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[1, 1].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "\n",
    "axs[0, 0].plot(x, G_S1Data[\"RatioOfDemand\"],\"k--\")\n",
    "axs[0, 1].plot(x, G_S2Data[\"RatioOfDemand\"],\"k--\")\n",
    "axs[1, 0].plot(x, G_S3Data[\"RatioOfDemand\"],\"k--\")\n",
    "axs[1, 1].plot(x, G_S4Data[\"RatioOfDemand\"],\"k--\",label=\"Avg Year\")\n",
    "\n",
    "Colors=list(mcolors.BASE_COLORS.keys())\n",
    "for i in range(len(Demand_Season_Hour_agg[\"Year\"].unique())):\n",
    "    year=Demand_Season_Hour_agg[\"Year\"].unique()[i]\n",
    "    G_S1Data=Demand_Season_Hour_agg[(Demand_Season_Hour_agg[\"Season\"]==\"S1\")*(Demand_Season_Hour_agg[\"Year\"]==year)]\n",
    "    G_S2Data=Demand_Season_Hour_agg[(Demand_Season_Hour_agg[\"Season\"]==\"S2\")*(Demand_Season_Hour_agg[\"Year\"]==year)]\n",
    "    G_S3Data=Demand_Season_Hour_agg[(Demand_Season_Hour_agg[\"Season\"]==\"S3\")*(Demand_Season_Hour_agg[\"Year\"]==year)]\n",
    "    G_S4Data=Demand_Season_Hour_agg[(Demand_Season_Hour_agg[\"Season\"]==\"S4\")*(Demand_Season_Hour_agg[\"Year\"]==year)]\n",
    "\n",
    "    axs[0, 0].plot(x, G_S1Data[\"RatioOfYearDemand\"],Colors[i]+\"--\")\n",
    "    axs[0, 1].plot(x, G_S2Data[\"RatioOfYearDemand\"],Colors[i]+\"--\")\n",
    "    axs[1, 0].plot(x, G_S3Data[\"RatioOfYearDemand\"],Colors[i]+\"--\")\n",
    "    axs[1, 1].plot(x, G_S4Data[\"RatioOfYearDemand\"],Colors[i]+\"--\",label=year)\n",
    "\n",
    "axs[0, 0].set_ylim(MinY, MaxY)\n",
    "axs[0, 1].set_ylim(MinY, MaxY)\n",
    "axs[1, 0].set_ylim(MinY, MaxY)\n",
    "axs[1, 1].set_ylim(MinY, MaxY)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=\"Local Hour\", ylabel='Perc of Demand')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Load Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season=\"S1\"\n",
    "year=2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "G_S1Data=GeneralDemandDistribuion[GeneralDemandDistribuion[\"Season\"]==\"S1\"]\n",
    "G_S2Data=GeneralDemandDistribuion[GeneralDemandDistribuion[\"Season\"]==\"S2\"]\n",
    "G_S3Data=GeneralDemandDistribuion[GeneralDemandDistribuion[\"Season\"]==\"S3\"]\n",
    "G_S4Data=GeneralDemandDistribuion[GeneralDemandDistribuion[\"Season\"]==\"S4\"]\n",
    "x=np.arange(0, 24+1, 1)\n",
    "\n",
    "MinY=-0.0005+min(G_S1Data[\"RatioOfDemand\"].min(),G_S2Data[\"RatioOfDemand\"].min(),G_S3Data[\"RatioOfDemand\"].min(),G_S4Data[\"RatioOfDemand\"].min())\n",
    "MaxY=0.001+max(G_S1Data[\"RatioOfDemand\"].max(),G_S2Data[\"RatioOfDemand\"].max(),G_S3Data[\"RatioOfDemand\"].max(),G_S4Data[\"RatioOfDemand\"].max())\n",
    "\n",
    "axs[0, 0].set_title('S1-Winter')\n",
    "axs[0, 1].set_title('S2-Spring')\n",
    "axs[1, 0].set_title('S3- Summer')\n",
    "axs[1, 1].set_title('S4-Autumn')\n",
    "\n",
    "axs[0, 0].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[0, 1].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[1, 0].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[1, 1].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "\n",
    "\n",
    "\n",
    "Colors=list(mcolors.BASE_COLORS.keys())\n",
    "\n",
    "for i in range(len(EIA_Data[\"Year\"].unique())):\n",
    "    year=EIA_Data[\"Year\"].unique()[i]\n",
    "    for season in Seasons:\n",
    "        MaxLoad=[]\n",
    "        MeanLoad=[]\n",
    "        MinLoad=[]\n",
    "        for hour in list(np.arange(0,24+1,1)):\n",
    "            MaxLoad.append(EIA_Data.loc[(EIA_Data[\"Hour\"]==hour)*(EIA_Data[\"Season\"]==season)*(EIA_Data[\"Year\"]==year),\"Demand (MW)\"].max())\n",
    "            MinLoad.append(EIA_Data.loc[(EIA_Data[\"Hour\"]==hour)*(EIA_Data[\"Season\"]==season)*(EIA_Data[\"Year\"]==year),\"Demand (MW)\"].min())\n",
    "            MeanLoad.append(EIA_Data.loc[(EIA_Data[\"Hour\"]==hour)*(EIA_Data[\"Season\"]==season)*(EIA_Data[\"Year\"]==year),\"Demand (MW)\"].mean())\n",
    "            y=MeanLoad\n",
    "        if season==\"S1\":\n",
    "            axs[0, 0].plot(x, y,Colors[i]+\"--\",label = str(year))\n",
    "        if season==\"S2\":\n",
    "            axs[0, 1].plot(x, y,Colors[i]+\"--\",label = str(year))\n",
    "        if season==\"S3\":\n",
    "            axs[1, 0].plot(x, y,Colors[i]+\"--\",label = str(year))\n",
    "        if season==\"S4\":\n",
    "            axs[1, 1].plot(x, y,Colors[i]+\"--\",label = str(year))\n",
    "\n",
    "MinY, MaxY = axs[1, 0].get_ylim()\n",
    "MinY=MinY*0.9\n",
    "MaxY=MaxY*1\n",
    "axs[0, 0].set_ylim(MinY, MaxY)\n",
    "axs[0, 1].set_ylim(MinY, MaxY)\n",
    "axs[1, 0].set_ylim(MinY, MaxY)\n",
    "axs[1, 1].set_ylim(MinY, MaxY)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=\"Local Hour\", ylabel='Demand (MW)')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write df on sql format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds_notes=\"From EIA-930 Forms\"\n",
    "\n",
    "Data=[]\n",
    "for region in RegionsDefinition:\n",
    "    for hour in range(len(TimeOfDay)):\n",
    "        for season in Seasons:\n",
    "            demand_name=\"ELCDMD\"\n",
    "            dds=GeneralDemandDistribuion[(GeneralDemandDistribuion[\"Season\"]==season)*(GeneralDemandDistribuion[\"Hour\"]==(hour+1))][\"RatioOfDemand\"].values[0]\n",
    "            Data.append([region, season, TimeOfDay[hour],demand_name,dds,dds_notes])\n",
    "        \n",
    "DemandSpecificDistribution_sql=pd.DataFrame(Data, columns=[\"regions\",\"season_name\",\"time_of_day_name\", \"demand_name\",\"dds\",\"dds_notes\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapacityFactorProcess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate CF based on EIA 930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate CF based on EIA 930\n",
    "TechEIA_SummaryFile=[\"Petroleum\",\"Pumped storage\",\"Other biomass\",\"Solar\"] #technology name on EIA summary file\n",
    "Tech2EIA_930= {\"Coal_EIA930\":           \"Net Generation (MW) from Coal\",\n",
    "                \"NG_EIA930\":             \"Net Generation (MW) from Natural Gas\",\n",
    "                \"Nuclear_EIA930\":        \"Net Generation (MW) from Nuclear\",\n",
    "                \"Petroleum_EIA930\":      \"Net Generation (MW) from All Petroleum Products\",\n",
    "                \"WATER_EIA930\":          \"Net Generation (MW) from Hydropower and Pumped Storage\",\n",
    "                \"WIND_EIA930\":           \"Net Generation (MW) from Wind\",\n",
    "                \"SOL_EIA930\":            \"Net Generation (MW) from Solar (Adjusted)\"}\n",
    "\n",
    "def EIA930_CFs(Tech, AvgY_CF):\n",
    "    EIA930_Column=Tech2EIA_930[Tech]\n",
    "    EIA_930_Paths = glob.glob(EIA_930_CFPath + \"\\*.csv\")\n",
    "\n",
    "    EIA_Data=pd.DataFrame(columns=[\"Year\",\"Season\",\"Hour\", \"Generation (MW)\"])\n",
    "    for FilePath in EIA_930_Paths:\n",
    "        df = pd.read_csv(FilePath,usecols = [\"Balancing Authority\",'Data Date',\"Hour Number\",EIA930_Column],low_memory=False)\n",
    "\n",
    "        BAsIn=df[\"Balancing Authority\"]==MajorBAs[0] # Initialize the boolean array to select the BAs\n",
    "        for BA in MajorBAs:\n",
    "            BAsIn=(df[\"Balancing Authority\"]==BA) + BAsIn\n",
    "        df=df[BAsIn]\n",
    "\n",
    "        FileData=pd.DataFrame(columns=[\"Year\",\"Season\",\"Hour\", \"Generation (MW)\"])\n",
    "        FileData[\"Generation (MW)\"]=df[EIA930_Column].str.replace(',', '').astype(float)\n",
    "        FileData[\"Year\"]=pd.to_datetime(df[\"Data Date\"],format='%m/%d/%Y').dt.year\n",
    "        FileData[\"Hour\"]=df[\"Hour Number\"]\n",
    "\n",
    "        Month=pd.to_datetime(df[\"Data Date\"],format='%m/%d/%Y').dt.month\n",
    "        SeasonDf=[]\n",
    "        for month in Month:\n",
    "            for j in range(len(Seasons)):\n",
    "                if month in SeasonsMonthRange[j]:\n",
    "                    SeasonDf.append(Seasons[j])\n",
    "                    break\n",
    "        FileData[\"Season\"]=SeasonDf\n",
    "\n",
    "        EIA_Data = pd.concat([EIA_Data,FileData], ignore_index = True)\n",
    "\n",
    "\n",
    "    EIA_Data=EIA_Data[EIA_Data[\"Hour\"]!=25] #Remove the 25th hour. On the EIA some days have 25 hours. I don't know why\n",
    "\n",
    "    PercentageNegative=-np.sum(EIA_Data.loc[EIA_Data[\"Generation (MW)\"]<0,\"Generation (MW)\"])/np.sum(EIA_Data.loc[EIA_Data[\"Generation (MW)\"]>0,\"Generation (MW)\"])*100\n",
    "    if PercentageNegative>1:\n",
    "        print(\"Warning: Negative generation is more than 1% of total generation, It is not advisable to use this data\")\n",
    "        print(\"EIA 930 Tech With Problem: %s  \\n Percentage Negative %.1f %\" % (Tech,PercentageNegative))\n",
    "        sys.exit()\n",
    "\n",
    "    EIA_Data.loc[EIA_Data[\"Generation (MW)\"]<0,\"Generation (MW)\"]=0 #Remove negative values, assume they are only a very small fraction of the total generation\n",
    "\n",
    "    AverageGen=pd.DataFrame(columns=[\"Season\",\"Hour\", \"AvgGen\",\"CF\"])\n",
    "    for Season in Seasons:\n",
    "        for Hour in EIA_Data[\"Hour\"].unique():\n",
    "            IdxIn=(EIA_Data[\"Season\"]==Season)*(EIA_Data[\"Hour\"]==Hour)\n",
    "            df=EIA_Data[IdxIn]\n",
    "            AvgGen=df[\"Generation (MW)\"].mean()#Mean of all years\n",
    "            AverageGen=pd.concat([AverageGen,pd.DataFrame([{\"Season\":Season,\"Hour\":Hour,\"AvgGen\":AvgGen, \"CF\":-1}])], ignore_index=True)\n",
    "\n",
    "    #Expected sum gen in a year\n",
    "    TGen=0\n",
    "    for Season in Seasons:\n",
    "        NumDaysSeason=NumHoursSeason[Season]/24\n",
    "        IdxIn=(AverageGen[\"Season\"]==Season)\n",
    "        df=AverageGen[IdxIn]\n",
    "        TGen+=df[\"AvgGen\"].sum()*NumDaysSeason #Proxy for Total energy generation in a year\n",
    "\n",
    "    for Season in Seasons:\n",
    "        NumDaysSeason=NumHoursSeason[Season]/24\n",
    "\n",
    "        IdxIn=(AverageGen[\"Season\"]==Season)\n",
    "        df=AverageGen[IdxIn]\n",
    "        DistributionBySeason=df[\"AvgGen\"].sum()*NumDaysSeason/TGen\n",
    "\n",
    "\n",
    "        CF=AvgY_CF*365.25*24*(df[\"AvgGen\"]/df[\"AvgGen\"].sum())*DistributionBySeason/(NumHoursSeason[Season]/24)\n",
    "        CF[CF<0.005]=0 # For some reason we have some small generation at night\n",
    "        AverageGen.loc[IdxIn,\"CF\"]=CF #For a specific season, what is the fraction of the total generation in the season that happens at each hour of the day (CSP)\n",
    "\n",
    "    return AverageGen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CFPlotEIA930(AverageGen):\n",
    "\n",
    "    R_S1Data=AverageGen[AverageGen[\"Season\"]==\"S1\"]\n",
    "    R_S2Data=AverageGen[AverageGen[\"Season\"]==\"S2\"]\n",
    "    R_S3Data=AverageGen[AverageGen[\"Season\"]==\"S3\"]\n",
    "    R_S4Data=AverageGen[AverageGen[\"Season\"]==\"S4\"]\n",
    "\n",
    "\n",
    "    x=R_S4Data[\"Hour\"].unique()\n",
    "\n",
    "    MinY=max(AverageGen[\"CF\"].min()-0.05,0)\n",
    "    MaxY=min(AverageGen[\"CF\"].max()+0.05,1)\n",
    "\n",
    "    #set ticks x axis\n",
    "    plt.xticks(np.arange(0, 24+1, 4))\n",
    "\n",
    "\n",
    "    Colors=list(mcolors.BASE_COLORS.keys())\n",
    "    plt.plot(x, R_S1Data[\"CF\"].values,Colors[0]+\"--\",label='S1-Winter')\n",
    "    plt.plot(x, R_S2Data[\"CF\"].values,Colors[1]+\"--\",label='S2-Spring')\n",
    "    plt.plot(x, R_S3Data[\"CF\"].values,Colors[2]+\"--\",label='S3-Summer')\n",
    "    plt.plot(x, R_S4Data[\"CF\"].values,Colors[3]+\"--\",label='S4-Autumn')\n",
    "    plt.xlabel(\"Local Hour\")\n",
    "    plt.ylabel('CF')\n",
    "\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "Tech=\"SOL_EIA930\"\n",
    "AvgY_CF=0.21 #CF from 2021 NC summary statistics\n",
    "AverageGen=EIA930_CFs(Tech, AvgY_CF)\n",
    "CFPlotEIA930(AverageGen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For solar energy we donwload irradiation data from SAM model of NREL at the 3 centroid locations determined by the load (CentroidLoadR1,CentroidLoadR2,CentroidLoadR3)\n",
    "# And define the distribution CF of the solar energy using irradiation ratios\n",
    "\n",
    "#Read All Data On Region R1,R2,R3 for solar DNI-Direct Normal Irradiation\n",
    "def GetAverageDNI_GHI(Region):\n",
    "    SAMPaths = glob.glob(SAM_NREL_Path+\"Sun/\"+Region + \"\\*.csv\")\n",
    "    FileData=pd.DataFrame(columns=[\"Year\",\"Season\",\"Hour\", \"DNI\",\"GHI\"])\n",
    "\n",
    "    for FilePath in SAMPaths:\n",
    "        df = pd.read_csv(FilePath,skiprows=2,low_memory=False,usecols = [\"Year\",\"Month\",'Hour', \"DNI\",\"GHI\",\"Temperature\"])\n",
    "        \n",
    "        SeasonDf=[]\n",
    "        for month in df[\"Month\"]:\n",
    "            for j in range(len(Seasons)):\n",
    "                if month in SeasonsMonthRange[j]:\n",
    "                    SeasonDf.append(Seasons[j])\n",
    "                    break\n",
    "\n",
    "        FileData = pd.concat([FileData,pd.DataFrame({\"Year\":df[\"Year\"],\"Hour\":df[\"Hour\"],\"DNI\":df[\"DNI\"],\"Season\":SeasonDf,\"GHI\":df[\"GHI\"]})], ignore_index = True)\n",
    "        #Hour 0 is 24:00 of the previous day, we change it to 24h of the current day (which is not fully correct but it is a good approximation given the data size and the averages we are going to take)\n",
    "        #Improve it later\n",
    "    FileData.loc[FileData[\"Hour\"]==0,\"Hour\"]=24 \n",
    "\n",
    "    AverageDNI_GHI=pd.DataFrame(columns=[\"Season\",\"Hour\", \"DNI\",\"GHI\",\"RatioOfDNI\",\"RatioOfGHI\"])\n",
    "    for Season in Seasons:\n",
    "        for Hour in FileData[\"Hour\"].unique():\n",
    "            IdxIn=(FileData[\"Season\"]==Season)*(FileData[\"Hour\"]==Hour)\n",
    "            df=FileData[IdxIn]\n",
    "            DNIValue=df[\"DNI\"].mean()#Mean of all years\n",
    "            GHIValue=df[\"GHI\"].mean()#Mean of all years\n",
    "            AverageDNI_GHI=pd.concat([AverageDNI_GHI,pd.DataFrame([{\"Season\":Season,\"Hour\":Hour,\"DNI\":DNIValue, \"GHI\":GHIValue, \"RatioOfDNI\":-1,\"RatioOfGHI\":-1}])], ignore_index=True)\n",
    "\n",
    "    #Expected sum total Irradiation in a year\n",
    "    TIrr_DNI=0\n",
    "    TIrr_GHI=0\n",
    "    for Season in Seasons:\n",
    "        NumDaysSeason=NumHoursSeason[Season]/24\n",
    "        IdxIn=(AverageDNI_GHI[\"Season\"]==Season)\n",
    "        df=AverageDNI_GHI[IdxIn]\n",
    "        TIrr_DNI+=df[\"DNI\"].sum()*NumDaysSeason #Proxy for Total energy generation in a year\n",
    "        TIrr_GHI+=df[\"GHI\"].sum()*NumDaysSeason\n",
    "\n",
    "    DistributionBySeason_DHI=[]\n",
    "    DistributionBySeason_GHI=[]\n",
    "    for Season in Seasons:\n",
    "        NumDaysSeason=NumHoursSeason[Season]/24\n",
    "        IdxIn=(AverageDNI_GHI[\"Season\"]==Season)\n",
    "        df=AverageDNI_GHI[IdxIn]\n",
    "\n",
    "        RatioOfDNI=df[\"DNI\"]/df[\"DNI\"].sum()\n",
    "        AverageDNI_GHI.loc[IdxIn,\"RatioOfDNI\"]=RatioOfDNI #For a specific season, what is the fraction of the total generation in the season that happens at each hour of the day (CSP)\n",
    "\n",
    "        RatioOfGHI=df[\"GHI\"]/df[\"GHI\"].sum()\n",
    "        AverageDNI_GHI.loc[IdxIn,\"RatioOfGHI\"]=RatioOfGHI #For a specific season, what is the fraction of the total generation in the season that happens at each hour of the day (PV)\n",
    "\n",
    "        DistributionBySeason_DHI.append(df[\"DNI\"].sum()*NumDaysSeason/TIrr_DNI)\n",
    "        DistributionBySeason_GHI.append(df[\"GHI\"].sum()*NumDaysSeason/TIrr_GHI)\n",
    "\n",
    "    DistributionBySeason=pd.DataFrame({\"Season\":Seasons,\"RatioOfDNI\":DistributionBySeason_DHI,\"RatioOfGHI\":DistributionBySeason_GHI})\n",
    "\n",
    "    return AverageDNI_GHI,DistributionBySeason\n",
    "\n",
    "AverageDNI_GHI_R1, DistributionBySeason_R1 = GetAverageDNI_GHI(\"R1\")\n",
    "AverageDNI_GHI_R2, DistributionBySeason_R2 = GetAverageDNI_GHI(\"R2\")\n",
    "AverageDNI_GHI_R3, DistributionBySeason_R3 = GetAverageDNI_GHI(\"R3\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Distribution of Solar Radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "R_S1Data=[AverageDNI_GHI_R1[AverageDNI_GHI_R1[\"Season\"]==\"S1\"],AverageDNI_GHI_R2[AverageDNI_GHI_R2[\"Season\"]==\"S1\"],AverageDNI_GHI_R3[AverageDNI_GHI_R3[\"Season\"]==\"S1\"]]\n",
    "R_S2Data=[AverageDNI_GHI_R1[AverageDNI_GHI_R1[\"Season\"]==\"S2\"],AverageDNI_GHI_R2[AverageDNI_GHI_R2[\"Season\"]==\"S2\"],AverageDNI_GHI_R3[AverageDNI_GHI_R3[\"Season\"]==\"S2\"]]\n",
    "R_S3Data=[AverageDNI_GHI_R1[AverageDNI_GHI_R1[\"Season\"]==\"S3\"],AverageDNI_GHI_R2[AverageDNI_GHI_R2[\"Season\"]==\"S3\"],AverageDNI_GHI_R3[AverageDNI_GHI_R3[\"Season\"]==\"S3\"]]\n",
    "R_S4Data=[AverageDNI_GHI_R1[AverageDNI_GHI_R1[\"Season\"]==\"S4\"],AverageDNI_GHI_R2[AverageDNI_GHI_R2[\"Season\"]==\"S4\"],AverageDNI_GHI_R3[AverageDNI_GHI_R3[\"Season\"]==\"S4\"]]\n",
    "\n",
    "R_S1=[DistributionBySeason_R1[DistributionBySeason_R1[\"Season\"]==\"S1\"],DistributionBySeason_R2[DistributionBySeason_R2[\"Season\"]==\"S1\"],DistributionBySeason_R3[DistributionBySeason_R3[\"Season\"]==\"S1\"]]\n",
    "R_S2=[DistributionBySeason_R1[DistributionBySeason_R1[\"Season\"]==\"S2\"],DistributionBySeason_R2[DistributionBySeason_R2[\"Season\"]==\"S2\"],DistributionBySeason_R3[DistributionBySeason_R3[\"Season\"]==\"S2\"]]\n",
    "R_S3=[DistributionBySeason_R1[DistributionBySeason_R1[\"Season\"]==\"S3\"],DistributionBySeason_R2[DistributionBySeason_R2[\"Season\"]==\"S3\"],DistributionBySeason_R3[DistributionBySeason_R3[\"Season\"]==\"S3\"]]\n",
    "R_S4=[DistributionBySeason_R1[DistributionBySeason_R1[\"Season\"]==\"S4\"],DistributionBySeason_R2[DistributionBySeason_R2[\"Season\"]==\"S4\"],DistributionBySeason_R3[DistributionBySeason_R3[\"Season\"]==\"S4\"]]\n",
    "\n",
    "R1_S3Data=AverageDNI_GHI_R1[AverageDNI_GHI_R1[\"Season\"]==\"S3\"]\n",
    "R1_S3=DistributionBySeason_R1[DistributionBySeason_R1[\"Season\"]==\"S3\"]\n",
    "x=R1_S3Data[\"Hour\"].unique()\n",
    "\n",
    "MinY=0\n",
    "MaxY=0.005+(R1_S3Data[\"RatioOfGHI\"]*R1_S3[\"RatioOfGHI\"].values[0]).max()\n",
    "\n",
    "axs[0, 0].set_title('S1-Winter')\n",
    "axs[0, 1].set_title('S2-Spring')\n",
    "axs[1, 0].set_title('S3- Summer')\n",
    "axs[1, 1].set_title('S4-Autumn')\n",
    "\n",
    "axs[0, 0].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[0, 1].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[1, 0].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[1, 1].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "\n",
    "\n",
    "\n",
    "Colors=list(mcolors.BASE_COLORS.keys())\n",
    "for i in range(3):\n",
    "    axs[0, 0].plot(x[1:], R_S1Data[i][\"RatioOfGHI\"][1:]*R_S1[i][\"RatioOfGHI\"].values[0],Colors[i]+\"--\",label=\"Region \"+str(i+1))\n",
    "    axs[0, 1].plot(x[1:], R_S2Data[i][\"RatioOfGHI\"][1:]*R_S2[i][\"RatioOfGHI\"].values[0],Colors[i]+\"--\",label=\"Region \"+str(i+1))\n",
    "    axs[1, 0].plot(x[1:], R_S3Data[i][\"RatioOfGHI\"][1:]*R_S3[i][\"RatioOfGHI\"].values[0],Colors[i]+\"--\",label=\"Region \"+str(i+1))\n",
    "    axs[1, 1].plot(x[1:], R_S4Data[i][\"RatioOfGHI\"][1:]*R_S4[i][\"RatioOfGHI\"].values[0],Colors[i]+\"--\",label=\"Region \"+str(i+1))\n",
    "\n",
    "axs[0, 0].set_ylim(MinY, MaxY)\n",
    "axs[0, 1].set_ylim(MinY, MaxY)\n",
    "axs[1, 0].set_ylim(MinY, MaxY)\n",
    "axs[1, 1].set_ylim(MinY, MaxY)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=\"Local Hour\", ylabel='Perc of Irradiation')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AverageGHI_Year_R1=AverageDNI_GHI_R1[\"GHI\"].sum()/len(Seasons)/1000 #Kwh/m2/day\n",
    "AverageGHI_Year_R2=AverageDNI_GHI_R2[\"GHI\"].sum()/len(Seasons)/1000  #Kwh/m2/day\n",
    "AverageGHI_Year_R3=AverageDNI_GHI_R3[\"GHI\"].sum()/len(Seasons)/1000  #Kwh/m2/day\n",
    "\n",
    "AverageDNI_Year_R1=AverageDNI_GHI_R1[\"DNI\"].sum()/len(Seasons)/1000  #Kwh/m2/day\n",
    "AverageDNI_Year_R2=AverageDNI_GHI_R2[\"DNI\"].sum()/len(Seasons)/1000  #Kwh/m2/day\n",
    "AverageDNI_Year_R3=AverageDNI_GHI_R3[\"DNI\"].sum()/len(Seasons)/1000  #Kwh/m2/day\n",
    "\n",
    "#NC was a GHI arond 4.5-4.75 which is Class 6 on the Utility-Scale/Commercial/Residential PV notation on NREL ATB-22\n",
    "#NC was a DNI around 5 which is Class 12 on the Concentrating Solar Power notation on NREL ATB-22 - The CFs of this technology are not mapped on the ATB-22. Since it is a poor resource we did not map it on NC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Existing Solar Residential and Comercial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get turbine data\n",
    "#some supply curves: https://www.nrel.gov/gis/wind-supply-curves.html\n",
    "def ReadTrubineData(NameDistribution):\n",
    "    FilePath=SAM_NREL_Path+\"Wind/Turbines.xlsx\"\n",
    "    df = pd.read_excel(FilePath,sheet_name=NameDistribution)\n",
    "    HubHeight=df.iloc[0,0]\n",
    "    Efficiency=df.iloc[0,1]\n",
    "    WS_PowerCurve=df.iloc[:,4]\n",
    "    CF_PowerCurve=df.iloc[:,5]\n",
    "    WS_2_CF = lambda WS : np.interp(WS, WS_PowerCurve, CF_PowerCurve)*Efficiency\n",
    "\n",
    "    return HubHeight,Efficiency,WS_2_CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CF_Wind(Region, NameDistribution):\n",
    "#Read Excel with wind data\n",
    "\n",
    "    if NameDistribution==\"OFFSHORE\":\n",
    "        SAMPaths = glob.glob(SAM_NREL_Path+\"Wind/Offshore\" + \"\\*.csv\")\n",
    "    if NameDistribution==\"LAND\":\n",
    "        SAMPaths = glob.glob(SAM_NREL_Path+\"Wind/\"+Region + \"\\*.csv\")\n",
    "        \n",
    "    FileData=pd.DataFrame(columns=[\"Season\",\"Hour\", \"CF\"])\n",
    "    HubHeight,Efficiency,WS_2_CF=ReadTrubineData(NameDistribution)\n",
    "\n",
    "    for FilePath in SAMPaths:\n",
    "        df = pd.read_csv(FilePath,skiprows=1,low_memory=False,usecols = [\"Year\",\"Month\",'Hour', \"wind speed at 100m (m/s)\",\"wind speed at 120m (m/s)\"])\n",
    "        SeasonDf=[]\n",
    "        for month in df[\"Month\"]:\n",
    "            for j in range(len(Seasons)):\n",
    "                if month in SeasonsMonthRange[j]:\n",
    "                    SeasonDf.append(Seasons[j])\n",
    "                    break\n",
    "                \n",
    "        WS100=df[\"wind speed at 100m (m/s)\"]\n",
    "        WS120=df[\"wind speed at 120m (m/s)\"]\n",
    "        WindSpeedAdjusted = WS100 + (HubHeight-100)*(WS120-WS100)/(120-100) #Linear interpolation between 100m and 120m at the hub height\n",
    "        WindSpeed110 = WS100 + (110-100)*(WS120-WS100)/(120-100) #Linear interpolation between 100m and 120m at the hub height\n",
    "        CF=WS_2_CF(WindSpeedAdjusted)\n",
    "        \n",
    "        FileData = pd.concat([FileData,pd.DataFrame({\"Hour\":df[\"Hour\"],\"Season\":SeasonDf,\"CF\":CF,\"WS110\":WindSpeed110})], ignore_index = True)\n",
    "\n",
    "    #Hour 0 is 24:00 of the previous day, we change it to 24h of the current day (which is not fully correct but it is a good approximation given the data size and the averages we are going to take)\n",
    "    #Improve it later\n",
    "    FileData.loc[FileData[\"Hour\"]==0,\"Hour\"]=24 \n",
    "    WS_Statistics={\"Mean\":FileData[\"WS110\"].mean(),\n",
    "                   \"Q75\":FileData[\"WS110\"].quantile(0.75),\n",
    "                   \"Q25\":FileData[\"WS110\"].quantile(0.25)}\n",
    "\n",
    "    AverageCF=pd.DataFrame(columns=[\"Season\",\"Hour\", \"CF\"])\n",
    "    for Season in Seasons:\n",
    "        for Hour in FileData[\"Hour\"].unique():\n",
    "            IdxIn=(FileData[\"Season\"]==Season)*(FileData[\"Hour\"]==Hour)\n",
    "            df=FileData[IdxIn]\n",
    "            CF_AVG=df[\"CF\"].mean()#Mean of all years\n",
    "\n",
    "            AverageCF=pd.concat([AverageCF,pd.DataFrame([{\"Hour\":Hour,\"Season\":Season,\"CF\":CF_AVG}])], ignore_index=True)\n",
    "            \n",
    "    return AverageCF, WS_Statistics\n",
    "\n",
    "AverageCF_R1,WS_Statistics_R1=CF_Wind(\"R1\", \"LAND\")\n",
    "AverageCF_R2,WS_Statistics_R2=CF_Wind(\"R2\", \"LAND\")\n",
    "AverageCF_R3,WS_Statistics_R3=CF_Wind(\"R3\", \"LAND\")\n",
    "AverageCF_Off,WS_Statistics_Off=CF_Wind(\"R3\", \"OFFSHORE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Distribution of Wind CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "R_S1Data=[AverageCF_R1[AverageCF_R1[\"Season\"]==\"S1\"],AverageCF_R2[AverageCF_R2[\"Season\"]==\"S1\"],AverageCF_R3[AverageCF_R3[\"Season\"]==\"S1\"],AverageCF_Off[AverageCF_Off[\"Season\"]==\"S1\"]]\n",
    "R_S2Data=[AverageCF_R1[AverageCF_R1[\"Season\"]==\"S2\"],AverageCF_R2[AverageCF_R2[\"Season\"]==\"S2\"],AverageCF_R3[AverageCF_R3[\"Season\"]==\"S2\"],AverageCF_Off[AverageCF_Off[\"Season\"]==\"S2\"]]\n",
    "R_S3Data=[AverageCF_R1[AverageCF_R1[\"Season\"]==\"S3\"],AverageCF_R2[AverageCF_R2[\"Season\"]==\"S3\"],AverageCF_R3[AverageCF_R3[\"Season\"]==\"S3\"],AverageCF_Off[AverageCF_Off[\"Season\"]==\"S3\"]]\n",
    "R_S4Data=[AverageCF_R1[AverageCF_R1[\"Season\"]==\"S4\"],AverageCF_R2[AverageCF_R2[\"Season\"]==\"S4\"],AverageCF_R3[AverageCF_R3[\"Season\"]==\"S4\"],AverageCF_Off[AverageCF_Off[\"Season\"]==\"S4\"]]\n",
    "\n",
    "\n",
    "x=AverageCF_Off[\"Hour\"].unique()\n",
    "\n",
    "MinY=0\n",
    "MaxY=0.6\n",
    "\n",
    "axs[0, 0].set_title('S1-Winter')\n",
    "axs[0, 1].set_title('S2-Spring')\n",
    "axs[1, 0].set_title('S3-Summer')\n",
    "axs[1, 1].set_title('S4-Autumn')\n",
    "\n",
    "axs[0, 0].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[0, 1].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[1, 0].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "axs[1, 1].xaxis.set_ticks(np.arange(0, 24+1, 4))\n",
    "\n",
    "\n",
    "\n",
    "Colors=list(mcolors.BASE_COLORS.keys())\n",
    "for i in range(4):\n",
    "    axs[0, 0].plot(x[1:], R_S1Data[i][\"CF\"][1:].values,Colors[i]+\"--\",label=\"Region \"+str(i+1))\n",
    "    axs[0, 1].plot(x[1:], R_S2Data[i][\"CF\"][1:].values,Colors[i]+\"--\",label=\"Region \"+str(i+1))\n",
    "    axs[1, 0].plot(x[1:], R_S3Data[i][\"CF\"][1:].values,Colors[i]+\"--\",label=\"Region \"+str(i+1))\n",
    "    axs[1, 1].plot(x[1:], R_S4Data[i][\"CF\"][1:].values,Colors[i]+\"--\",label=\"Region \"+str(i+1))\n",
    "\n",
    "axs[0, 0].set_ylim(MinY, MaxY)\n",
    "axs[0, 1].set_ylim(MinY, MaxY)\n",
    "axs[1, 0].set_ylim(MinY, MaxY)\n",
    "axs[1, 1].set_ylim(MinY, MaxY)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=\"Local Hour\", ylabel='CF')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average CF for R1: \"+str(AverageCF_R1[\"CF\"].mean()))\n",
    "print(\"Average CF for R2: \"+str(AverageCF_R2[\"CF\"].mean()))\n",
    "print(\"Average CF for R3: \"+str(AverageCF_R3[\"CF\"].mean()))\n",
    "print(\"Average CF for Offshore: \"+str(AverageCF_Off[\"CF\"].mean()))\n",
    "\n",
    "print(\"\\n\\nAverage WS for R1: %.2f \\nQ75 WS for R1: %.2f \\nQ25 WS for R1: %.2f\"%(WS_Statistics_R1[\"Mean\"],WS_Statistics_R1[\"Q75\"],WS_Statistics_R1[\"Q25\"]))\n",
    "print(\"\\n\\nAverage WS for R2: %.2f \\nQ75 WS for R2: %.2f \\nQ25 WS for R2: %.2f\"%(WS_Statistics_R2[\"Mean\"],WS_Statistics_R2[\"Q75\"],WS_Statistics_R2[\"Q25\"]))\n",
    "print(\"\\n\\nAverage WS for R3: %.2f \\nQ75 WS for R3: %.2f \\nQ25 WS for R3: %.2f\"%(WS_Statistics_R3[\"Mean\"],WS_Statistics_R3[\"Q75\"],WS_Statistics_R3[\"Q25\"]))\n",
    "print(\"\\n\\nAverage WS for Offshore: %.2f \\nQ75 WS for Offshore: %.2f \\nQ25 WS for Offshore: %.2f\"%(WS_Statistics_Off[\"Mean\"],WS_Statistics_Off[\"Q75\"],WS_Statistics_Off[\"Q25\"]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The precipitation modulates the probability distribution of the CF. Which yearly values come from ORNL\n",
    "def CF_Hydro(Region):\n",
    "    HydroData_df = pd.read_excel(HydroDataPath,skiprows=1,sheet_name=\"Operational\")\n",
    "    HydroData_df=HydroData_df[HydroData_df[\"State\"]==State] #Hydro Data for NC\n",
    "    HydroData_df=HydroData_df[~(pd.isnull(HydroData_df[\"CH_MWh\"]) + pd.isnull(HydroData_df[\"CH_MW\"]))] #only use valid data\n",
    "\n",
    "    AverageMWYear=HydroData_df[\"CH_MWh\"].sum()/(365.25*24)#Average MW generated\n",
    "    CF_Year=AverageMWYear/(HydroData_df[\"CH_MW\"].sum())#Capacity Factor. all tech is on region 3\n",
    "\n",
    "    HydroData_df = pd.read_excel(HydroDataPath,skiprows=1,sheet_name=\"Operational\")\n",
    "    df_Precipitation = pd.read_excel(SAM_NREL_Path+\"Hydro_USGS/\"+Region + \"/USGS.xlsx\")\n",
    "\n",
    "    df_Precipitation[\"Month\"]\n",
    "    df_Precipitation[\"Precipitation [mm]\"]\n",
    "\n",
    "    AverageCF=pd.DataFrame(columns=[\"Season\",\"Hour\", \"CF\"])\n",
    "\n",
    "    for j in range(len(Seasons)):\n",
    "        Season=Seasons[j]\n",
    "        TotalPrec=0\n",
    "        for month in df_Precipitation[\"Month\"]:\n",
    "            if month in SeasonsMonthRange[j]:\n",
    "                TotalPrec=TotalPrec+df_Precipitation[df_Precipitation[\"Month\"]==month][\"Precipitation [mm]\"].values[0]\n",
    "        RatioTotalPrec=TotalPrec/df_Precipitation[\"Precipitation [mm]\"].sum()\n",
    "\n",
    "        for Hour in range(len(TimeOfDay)):\n",
    "            CF_AVG=CF_Year*RatioTotalPrec*365.25*24/NumHoursSeason[Season]\n",
    "            \n",
    "            AverageCF=pd.concat([AverageCF,pd.DataFrame([{\"Hour\":Hour+1,\"Season\":Season,\"CF\":CF_AVG}])], ignore_index=True)\n",
    "    \n",
    "    return AverageCF\n",
    "\n",
    "AverageCF_R3=CF_Hydro(\"R3\")\n",
    "print(AverageCF_R3[AverageCF_R3[\"Hour\"]==1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write df on sql format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Excel With Data on Yearly CF and technolgies we need to model on CapacityFactorProcess\n",
    "Excel_CapacityFactorProcess = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='CapacityFactor')\n",
    "\n",
    "#Only need to represent technologies at the user evolution level (Advanced, Moderate, Conservative)\n",
    "Excel_CapacityFactorProcess=Excel_CapacityFactorProcess[(Excel_CapacityFactorProcess[\"Stage\"]==\"single\")+(Excel_CapacityFactorProcess[\"Stage\"]==UserScenario)]\n",
    "\n",
    "CF_DistributionDict={\"SOL\":  GetAverageDNI_GHI,\n",
    "                     \"SOL_EIA930\":  EIA930_CFs,\n",
    "                     \"WIND\": CF_Wind,\n",
    "                     \"WATER\": CF_Hydro}\n",
    "\n",
    "TechWithSpecific_CFs=Excel_CapacityFactorProcess[\"tech\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the CF_sql detaframe with the skeleton of the CFs for the technologies\n",
    "# CapacityFactorProcess_sql=pd.DataFrame(columns=[\"regions\",\"season_name\",\"time_of_day_name\",\"tech\",\"vintage\",\"cf_process\",\"cf_process_notes\"])\n",
    "Data=[]\n",
    "for tech in TechWithSpecific_CFs:\n",
    "    df=Efficiency_sql[Efficiency_sql[\"tech\"]==tech]\n",
    "    Regions2Model=df[\"regions\"]#regions we need to model for the technology\n",
    "    vintages2model=df[\"vintage\"]#vintages we need to model for the technology\n",
    "    for region, vintage in zip(Regions2Model,vintages2model):\n",
    "        for season in Seasons:\n",
    "            for hour in TimeOfDay:\n",
    "                Data.append([region,season,hour,tech,vintage,0,\"\"])\n",
    "\n",
    "CapacityFactorProcess_sql=pd.DataFrame(Data,columns=[\"regions\",\"season_name\",\"time_of_day_name\",\"tech\",\"vintage\",\"cf_process\",\"cf_process_notes\"])\n",
    "\n",
    "for tech in TechWithSpecific_CFs:\n",
    "\n",
    "    TechStandardCfs=Excel_CapacityFactorProcess.loc[Excel_CapacityFactorProcess[\"tech\"]==tech].iloc[:,6:]\n",
    "    TechMinCF=TechStandardCfs.iloc[0,0]\n",
    "\n",
    "    input_comm=Excel_CapacityFactorProcess.loc[Excel_CapacityFactorProcess[\"tech\"]==tech,\"input_comm\"].values[0]\n",
    "    NameDistribution=Excel_CapacityFactorProcess.loc[Excel_CapacityFactorProcess[\"tech\"]==tech,\"CF Distribution\"].values[0]\n",
    "\n",
    "    UniqueRegions=CapacityFactorProcess_sql[CapacityFactorProcess_sql[\"tech\"]==tech][\"regions\"].unique()\n",
    "\n",
    "    #Solar energy case\n",
    "    if input_comm==\"SOL\":\n",
    "        for region in UniqueRegions:       \n",
    "            Distribution_S_H, Distribution_S = CF_DistributionDict[input_comm](region)\n",
    "            UniqueVintages=CapacityFactorProcess_sql[(CapacityFactorProcess_sql[\"tech\"]==tech) & (CapacityFactorProcess_sql[\"regions\"]==region)][\"vintage\"].unique()\n",
    "\n",
    "            for vintage in UniqueVintages:\n",
    "                for season in Seasons:\n",
    "                    for time in range(len(TimeOfDay)):\n",
    "                        IdxIn=(CapacityFactorProcess_sql[\"tech\"]==tech) & (CapacityFactorProcess_sql[\"regions\"]==region)  & (CapacityFactorProcess_sql[\"vintage\"]==vintage)\\\n",
    "                            & (CapacityFactorProcess_sql[\"season_name\"]==season) & (CapacityFactorProcess_sql[\"time_of_day_name\"]==TimeOfDay[time])\n",
    "                        \n",
    "                        P_s=Distribution_S.loc[Distribution_S[\"Season\"]==season,\"RatioOf\"+NameDistribution].values[0]\n",
    "                        P_h_s=Distribution_S_H.loc[(Distribution_S_H[\"Season\"]==season)*(Distribution_S_H[\"Hour\"]==time+1), \"RatioOf\"+NameDistribution].values[0]\n",
    "\n",
    "                        TechCF_Ratio=TechStandardCfs[max(vintage,TechStandardCfs.columns[0])].values[0]/TechMinCF\n",
    "                        CapacityFactorProcess_sql.loc[IdxIn,\"cf_process\"]=TechMinCF*365.25*24*P_s*P_h_s/(NumHoursSeason[season]/24)*TechCF_Ratio\n",
    "\n",
    "    #Solar energy case\n",
    "    if input_comm==\"SOL_EIA930\":\n",
    "        Data_CF_SOL = CF_DistributionDict[input_comm](input_comm,1)\n",
    "        for region in UniqueRegions:       \n",
    "            UniqueVintages=CapacityFactorProcess_sql[(CapacityFactorProcess_sql[\"tech\"]==tech) & (CapacityFactorProcess_sql[\"regions\"]==region)][\"vintage\"].unique()\n",
    "\n",
    "            for vintage in UniqueVintages:\n",
    "                for season in Seasons:\n",
    "                    for time in range(len(TimeOfDay)):\n",
    "                        IdxIn=(CapacityFactorProcess_sql[\"tech\"]==tech) & (CapacityFactorProcess_sql[\"regions\"]==region) & (CapacityFactorProcess_sql[\"vintage\"]==vintage) \\\n",
    "                            & (CapacityFactorProcess_sql[\"season_name\"]==season) & (CapacityFactorProcess_sql[\"time_of_day_name\"]==TimeOfDay[time])\n",
    "\n",
    "                        CF=Data_CF_SOL.loc[(Data_CF_SOL[\"Season\"]==season)*(Data_CF_SOL[\"Hour\"]==time+1), \"CF\"].values[0] #Defined as a tech in 2020\n",
    "                        \n",
    "                        TechCF_Ratio=TechStandardCfs[max(vintage,TechStandardCfs.columns[0])].values[0]\n",
    "                        CF=CF*TechCF_Ratio\n",
    "                        if CF<0.005:\n",
    "                            CF=0\n",
    "                        CapacityFactorProcess_sql.loc[IdxIn,\"cf_process\"]=CF\n",
    "                        #The CF of each tech scales as define in the CF presented in the excel file\n",
    "\n",
    "    #Wind energy case\n",
    "    if input_comm==\"WIND\":\n",
    "        for region in UniqueRegions:       \n",
    "            Data_CF_Wind,_ = CF_DistributionDict[input_comm](region, NameDistribution)\n",
    "            UniqueVintages=CapacityFactorProcess_sql[(CapacityFactorProcess_sql[\"tech\"]==tech) & (CapacityFactorProcess_sql[\"regions\"]==region)][\"vintage\"].unique()\n",
    "\n",
    "            for vintage in UniqueVintages:\n",
    "                for season in Seasons:\n",
    "                    for time in range(len(TimeOfDay)):\n",
    "                        IdxIn=(CapacityFactorProcess_sql[\"tech\"]==tech) & (CapacityFactorProcess_sql[\"regions\"]==region) & (CapacityFactorProcess_sql[\"vintage\"]==vintage) \\\n",
    "                            & (CapacityFactorProcess_sql[\"season_name\"]==season) & (CapacityFactorProcess_sql[\"time_of_day_name\"]==TimeOfDay[time])\n",
    "                        CF=Data_CF_Wind.loc[(Data_CF_Wind[\"Season\"]==season)*(Data_CF_Wind[\"Hour\"]==time+1), \"CF\"].values[0] #Defined as a tech in 2020\n",
    "                        \n",
    "                        \n",
    "                        TechCF_Ratio=TechStandardCfs[max(vintage,TechStandardCfs.columns[0])].values[0]/TechMinCF\n",
    "                        CapacityFactorProcess_sql.loc[IdxIn,\"cf_process\"]=CF*TechCF_Ratio\n",
    "                        #The CF of each tech scales as define in the CF presented in the excel file\n",
    "\n",
    "    #Hydro energy case\n",
    "    if input_comm==\"WATER\":\n",
    "        for region in UniqueRegions:       \n",
    "            Data_CF_Hydro=CF_DistributionDict[input_comm](region)\n",
    "            UniqueVintages=CapacityFactorProcess_sql[(CapacityFactorProcess_sql[\"tech\"]==tech) & (CapacityFactorProcess_sql[\"regions\"]==region)][\"vintage\"].unique()\n",
    "            for vintage in UniqueVintages:\n",
    "                for season in Seasons:\n",
    "                    for time in range(len(TimeOfDay)):\n",
    "                        IdxIn=(CapacityFactorProcess_sql[\"tech\"]==tech) & (CapacityFactorProcess_sql[\"regions\"]==region) & (CapacityFactorProcess_sql[\"vintage\"]==vintage)\\\n",
    "                             & (CapacityFactorProcess_sql[\"season_name\"]==season) & (CapacityFactorProcess_sql[\"time_of_day_name\"]==TimeOfDay[time])\n",
    "                        CF=Data_CF_Hydro.loc[(Data_CF_Hydro[\"Season\"]==season)*(Data_CF_Hydro[\"Hour\"]==time+1), \"CF\"].values[0]\n",
    "                        CapacityFactorProcess_sql.loc[IdxIn,\"cf_process\"]=CF #Capacity factor do no depend on the vintage\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify CF consistency (values 0-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify Data consistency\n",
    "if np.sum(CapacityFactorProcess_sql[\"cf_process\"]>=1)>0:\n",
    "    print(\"ERROR: There are some CF values that are larger than one\")\n",
    "    print(CapacityFactorProcess_sql.loc[CapacityFactorProcess_sql[\"cf_process\"]>=1,:])\n",
    "    sys.exit()\n",
    "#CapacityFactorProcess_sql.to_excel(\"Debug.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapacityCredit and tech_reserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapacityCredit_Excel  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='CapacityCredit')\n",
    "#First we build tech reserve\n",
    "tech_reserve_sql=pd.DataFrame({\"tech\": CapacityCredit_Excel[\"tech\"],\n",
    "                                \"notes\": CapacityCredit_Excel[\"cf_tech_notes\"]})\n",
    "\n",
    "#Eliminate _EXISTING TECHNOLOGIES not in ExistingCapacity_sql[\"tech\"]. Some tech may be eliminated if no capacity existing after filtering (related to lifetime)\n",
    "IdxExistingTech=tech_reserve_sql[\"tech\"].str.contains(\"_EXISTING\")\n",
    "IdxExistingNotEliminated=tech_reserve_sql[\"tech\"].isin(ExistingCapacity_sql[\"tech\"].unique())\n",
    "tech_reserve_sql=tech_reserve_sql[~(IdxExistingTech * ~IdxExistingNotEliminated)] #guys need to keep\n",
    "\n",
    "#Build capacity credit:  we use CostVariable to get the skeleton of the capacity credit, however we need to filter for technologies on tech_reserve_sql\n",
    "IdxIn=np.argwhere(np.in1d(CostVariable_sql[\"tech\"],tech_reserve_sql[\"tech\"]))[:,0]\n",
    "dfSkeleton=CostVariable_sql.iloc[IdxIn,:].reset_index(drop=True)\n",
    "\n",
    "CapacityCredit_sql=pd.DataFrame({\"regions\": dfSkeleton[\"regions\"],\n",
    "                                \"periods\": dfSkeleton[\"periods\"],\n",
    "                                \"tech\": dfSkeleton[\"tech\"],\n",
    "                                \"vintage\": dfSkeleton[\"vintage\"],\n",
    "                                \"cf_tech\": -1,\n",
    "                                \"cf_tech_notes\": \"\"})\n",
    "\n",
    "for index, ExcelRowData in CapacityCredit_Excel.iterrows():\n",
    "\n",
    "    capacity_credit=ExcelRowData[\"capacity_credit\"]\n",
    "    tech=ExcelRowData[\"tech\"]\n",
    "    cf_tech_notes=ExcelRowData[\"cf_tech_notes\"]\n",
    "    CapacityCredit_sql.loc[CapacityCredit_sql[\"tech\"]==tech,\"cf_tech\"]=capacity_credit\n",
    "    CapacityCredit_sql.loc[CapacityCredit_sql[\"tech\"]==tech,\"cf_tech_notes\"]=cf_tech_notes\n",
    "\n",
    "#Check if all values were filled\n",
    "if np.sum(CapacityCredit_sql[\"cf_tech\"]==-1)>0:\n",
    "    print(\"ERROR: There are some CF values that are not defined\")\n",
    "    print(CapacityCredit_sql.loc[CapacityCredit_sql[\"cf_tech\"]==-1,:])\n",
    "    sys.exit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DiscountRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DiscountRate_Excel  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='DiscountRate')\n",
    "#Get data structure from CostInvest_sql\n",
    "DiscountRate_sql=pd.DataFrame({\"regions\":CostInvest_sql[\"regions\"],\n",
    "                                \"tech\":CostInvest_sql[\"tech\"],\n",
    "                                \"vintage\":CostInvest_sql[\"vintage\"],\n",
    "                                \"tech_rate\":np.nan,\n",
    "                                \"tech_rate_notes\":\"\"})\n",
    "\n",
    "for index, ExcelRowData in DiscountRate_Excel.iterrows():\n",
    "\n",
    "    region=ExcelRowData[\"regions\"]\n",
    "    tech=ExcelRowData[\"tech\"]\n",
    "    eff_note=ExcelRowData[\"tech_rate_notes\"]\n",
    "\n",
    "    if region==\"single\":#only one row with this technology -all regions same value\n",
    "        for vintage in DiscountRate_sql.loc[DiscountRate_sql[\"tech\"]==tech,\"vintage\"].unique():\n",
    "\n",
    "            tech_rate=ExcelRowData.loc[max(vintage, DiscountRate_Excel.columns[3])] #Get the rate for the proper vintage and tech\n",
    "            DiscountRate_sql.loc[(DiscountRate_sql[\"tech\"]==tech) * (DiscountRate_sql[\"vintage\"]==vintage) ,\"tech_rate\"]=tech_rate\n",
    "\n",
    "    if region!=\"single\":\n",
    "        for vintage in DiscountRate_sql.loc[(DiscountRate_sql[\"tech\"]==tech)*(DiscountRate_sql[\"regions\"]==region),\"vintage\"].unique():#vintages in this region for this tech\n",
    "            tech_rate=ExcelRowData.loc[max(vintage, DiscountRate_Excel.columns[3])]\n",
    "            DiscountRate_sql.loc[(DiscountRate_sql[\"tech\"]==tech) * (DiscountRate_sql[\"regions\"]==region) * (DiscountRate_sql[\"vintage\"]==vintage) ,\"tech_rate\"]=tech_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SegFrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=[]\n",
    "for season in Seasons:\n",
    "    for hour in TimeOfDay:\n",
    "        segfec=NumHoursSeason[season]/(365.25*24)*1/24\n",
    "        Data.append([season,hour,segfec,\"\"])\n",
    "\n",
    "SegFrac_sql=pd.DataFrame(Data, columns=[\"season_name\",\"time_of_day_name\",\"segfrac\",\"segfrac_notes\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapacityToActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can get the name and location of all technologies using Efficiency_sql\n",
    "RegionsAndTechOnCapAct=Efficiency_sql[[\"regions\",\"tech\"]].drop_duplicates().reset_index(drop=True)\n",
    "CapacityToActivity_sql=pd.DataFrame({\"regions\":RegionsAndTechOnCapAct[\"regions\"],\n",
    "                                     \"tech\":RegionsAndTechOnCapAct[\"tech\"],\n",
    "                                     \"c2a\":31.536,\n",
    "                                     \"c2a_notes\": \"1GW* 8760hr/year *3600s/hr=PJ/year\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StorageDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StorageDuration_Excel = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='StorageDuration')\n",
    "\n",
    "#We use Efficiency_sql to get the skeleton of the StorageDuration, however we need to filter for technologies on StorageDuration\n",
    "IdxIn=np.argwhere(np.in1d(Efficiency_sql[\"tech\"],StorageDuration_Excel[\"tech\"]))[:,0]\n",
    "dfSkeleton=Efficiency_sql.iloc[IdxIn,:].reset_index(drop=True)\n",
    "dfSkeleton=dfSkeleton[[\"regions\",\"tech\"]]\n",
    "StorageDuration_sql=dfSkeleton.drop_duplicates(subset=['regions', 'tech']).reset_index(drop=True)\n",
    "StorageDuration_sql[\"duration\"]=-1\n",
    "StorageDuration_sql[\"duration_notes\"]=\"\"\n",
    "\n",
    "for index, ExcelRowData in StorageDuration_Excel.iterrows():\n",
    "    region=ExcelRowData[\"regions\"]\n",
    "    tech=ExcelRowData[\"tech\"]\n",
    "    duration_notes=ExcelRowData[\"duration_notes\"]\n",
    "    duration=ExcelRowData[\"duration\"]\n",
    "\n",
    "    if region==\"single\":#only one row with this technology -all regions same value\n",
    "        StorageDuration_sql.loc[(StorageDuration_sql[\"tech\"]==tech) ,\"duration\"]=duration\n",
    "        StorageDuration_sql.loc[(StorageDuration_sql[\"tech\"]==tech) ,\"duration_notes\"]=duration_notes\n",
    "\n",
    "    if region!=\"single\":\n",
    "        StorageDuration_sql.loc[(StorageDuration_sql[\"tech\"]==tech) * (StorageDuration_sql[\"regions\"]==region) ,\"duration\"]=duration\n",
    "        StorageDuration_sql.loc[(StorageDuration_sql[\"tech\"]==tech) * (StorageDuration_sql[\"regions\"]==region) ,\"duration_notes\"]=duration_notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Convert Excel Table to Sql data Part 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data need to be filtered by the region you are interested in\n",
    "# Ref: https://bioenergykdf.net/executive-summaryoverview?chapterNumber=1&tabNumber=1\n",
    "BiomassDataFromDOE_Excel = pd.read_csv(BiomassDataFromDOE_Path)\n",
    "PriceTarget=60\n",
    "ScenariosDic={\"Agriculture\":\"1% Basecase, all energy crops\",\n",
    "              \"Forest\":\"Medium housing, low energy demands\",\n",
    "              \"Waste\":\"Wastes and other residues\"} #Base scenarios from DOE\n",
    "\n",
    "Conversions2dt={\"dt\":1,\n",
    "                \"lb\":1/2240,\n",
    "                \"bu\":1/45.9296}\n",
    "\n",
    "Conversionsdt2MBTU={\"Agriculture\":0,#assume all agriculture is used on transportation fuel only none for power sector\n",
    "                    \"Forest\":13,\n",
    "                    \"Waste\" :8}#From 2016 billion ton report pg 24\n",
    "\n",
    "Years=list(np.sort(BiomassDataFromDOE_Excel[\"Year\"].unique()))[1:]\n",
    "Resource=[]\n",
    "for case in list(ScenariosDic.keys()):\n",
    "    Resource=Resource+[case]*len(Years)\n",
    "\n",
    "BiomassLimits_df=pd.DataFrame({\"Year\":list(Years)*len(ScenariosDic),\"Resoruce\": Resource,\"Production dt\":0,\"MBTUs\":0})\n",
    "\n",
    "#Filter for the right price\n",
    "BiomassDataFromDOE_Excel=BiomassDataFromDOE_Excel.loc[BiomassDataFromDOE_Excel[\"Biomass Price\"]==PriceTarget,:]\n",
    "\n",
    "#eliminate null values\n",
    "BiomassDataFromDOE_Excel=BiomassDataFromDOE_Excel.loc[~BiomassDataFromDOE_Excel[\"Production\"].isnull()]\n",
    "\n",
    "#Convert all to dt\n",
    "for Unit in list(Conversions2dt.keys()):\n",
    "    BiomassDataFromDOE_Excel.loc[BiomassDataFromDOE_Excel[\"Production Unit\"]==Unit,\"Production\"]=BiomassDataFromDOE_Excel.loc[BiomassDataFromDOE_Excel[\"Production Unit\"]==Unit,\"Production\"]*Conversions2dt[Unit]\n",
    "\n",
    "for year in Years:\n",
    "    for Scenario in list(ScenariosDic.keys()):\n",
    "        dtAvailable=BiomassDataFromDOE_Excel.loc[(BiomassDataFromDOE_Excel[\"Scenario\"]==ScenariosDic[Scenario]) * (BiomassDataFromDOE_Excel[\"Year\"]==year),\"Production\"].sum()\n",
    "        BiomassLimits_df.loc[(BiomassLimits_df[\"Year\"]==year) * (BiomassLimits_df[\"Resoruce\"]==Scenario),\"Production dt\"]= dtAvailable\n",
    "        BiomassLimits_df.loc[(BiomassLimits_df[\"Year\"]==year) * (BiomassLimits_df[\"Resoruce\"]==Scenario),\"MBTUs\"]= dtAvailable*Conversionsdt2MBTU[Scenario] #Convert dt to BTUs\n",
    "\n",
    "\n",
    "SummaryBiomassLimits_df=pd.DataFrame({\"Year\":Years,\"MBTUs\":BiomassLimits_df.groupby(\"Year\")[\"MBTUs\"].sum().values})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Biomass Limits (MBTU)\")\n",
    "plt.plot(SummaryBiomassLimits_df[\"Year\"],SummaryBiomassLimits_df[\"MBTUs\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landfill Gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume no substantial change from current values\n",
    "#EIA 923\n",
    "#Have to enter the value by hand on the efficiency table\n",
    "\n",
    "\n",
    "def EstimateLandfill(Path):\n",
    "    Path=EIA_923_Path\n",
    "    EIA923_df = pd.read_excel(EIA_923_Path,sheet_name ='Page 1 Generation and Fuel Data',skiprows=5)\n",
    "    EIA923_State=EIA923_df[\"Plant State\"]\n",
    "    EIA923_df=EIA923_df[EIA923_State==State]\n",
    "\n",
    "    EIA923_df=EIA923_df[(EIA923_df[\"Net Generation\\n(Megawatthours)\"]>0)] \n",
    "\n",
    "    #On EIA 860 combined cycle tech are divided in CT and CA (steam part)\n",
    "    #We need to combine them as CC: combine cycle\n",
    "    EIA923_Mover=EIA923_df[\"Reported\\nPrime Mover\"]\n",
    "    EIA923_FuelType=EIA923_df[\"Reported\\nFuel Type Code\"]\n",
    "    EIA923_EnergyGen=EIA923_df[\"Elec Fuel Consumption\\nMMBtu\"]\n",
    "\n",
    "    UniqueMoverFuel = EIA923_df[[\"Reported\\nFuel Type Code\",\"Reported\\nPrime Mover\"]].drop_duplicates()\n",
    "    UniqueMoverFuel[\"MMBTU\"]=0\n",
    "\n",
    "    for Mover, Fuel in zip(UniqueMoverFuel.iloc[:,1],UniqueMoverFuel.iloc[:,0]):\n",
    "        Idx=(UniqueMoverFuel[\"Reported\\nPrime Mover\"]==Mover) * (UniqueMoverFuel[\"Reported\\nFuel Type Code\"]==Fuel)\n",
    "        UniqueMoverFuel.loc[Idx,\"MMBTU\"]=np.sum(EIA923_EnergyGen[(EIA923_Mover==Mover) * (EIA923_FuelType==Fuel)])\n",
    "\n",
    "    UniqueMoverFuel=UniqueMoverFuel[UniqueMoverFuel[\"Reported\\nFuel Type Code\"]==\"LFG\"] #LFG is landfill gas\n",
    "    print(\"Total LFG MMBTU: \",np.sum(UniqueMoverFuel[\"MMBTU\"]))\n",
    "    return UniqueMoverFuel\n",
    "\n",
    "LandFill_df=EstimateLandfill(EIA_923_Path)#2021 Esimate\n",
    "_=EstimateLandfill(EIA_923_Path_2020)#2020 Esimate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Landfill and other Biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalBiomassANDLFLimits=SummaryBiomassLimits_df.copy()\n",
    "TotalBiomassANDLFLimits[\"MBTUs\"]=SummaryBiomassLimits_df[\"MBTUs\"]+np.sum(LandFill_df[\"MMBTU\"])\n",
    "print(\"Average Limit For Biomass and Landfill MMBTUs: \",np.mean(TotalBiomassANDLFLimits[\"MBTUs\"]))\n",
    "\n",
    "#MMBTUs to PJ\n",
    "print(\"Average Limit For Biomass and Landfill PJ: \",np.mean(TotalBiomassANDLFLimits[\"MBTUs\"]*1.0550559e-6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxCapacity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind\n",
    "#Offshore Wind Energy- File needs to contain only locations of the desired state and depths and distances from shore you want to consider\n",
    "OffshoreWindCapacity_Excel=pd.read_csv(SAM_NREL_Path+\"WindData_Capacity/Offshore_WindNREL.csv\")\n",
    "\n",
    "LandWindCapacity_Excel=pd.read_csv(SAM_NREL_Path+\"WindData_Capacity/LandWind_NREL.csv\")\n",
    "USStates = gpd.read_file(StatesSHP)\n",
    "\n",
    "#Land Wind Energy\n",
    "df=LandWindCapacity_Excel[[\"longitude\",\"latitude\"]]\n",
    "points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\")\n",
    "\n",
    "within_points = gpd.sjoin(points, USStates[USStates[\"stusab\"]==State])\n",
    "IdxIn=within_points.index\n",
    "LandWindCapacity_Excel_Filtered=LandWindCapacity_Excel.iloc[IdxIn,:]\n",
    "\n",
    "print(\"Maxixum Land Wind Capacity %.2f GW\" % (LandWindCapacity_Excel_Filtered[\"capacity_mw\"].sum()/1000))\n",
    "print(\"Average CF from This db for Land Wind %.2f\" % (LandWindCapacity_Excel_Filtered[\"capacity_factor\"].mean()))\n",
    "\n",
    "print(\"\\nMaxixum Offshore Wind Capacity %.2f GW\" % (OffshoreWindCapacity_Excel[\"capacity_mw\"].sum()/1000))\n",
    "print(\"Average CF from This db for Offshore Wind %.2f\" % (OffshoreWindCapacity_Excel[\"capacity_factor\"].mean()))\n",
    "\n",
    "#We will not make any bound on wind or solar capacity as the available capacity is much larger than the demand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Technologies and Define MaxCapacityGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build MaxCapacityGroup_sql, TechGroupWeight_sql, groups_sql, tech_groups_sql\n",
    "\n",
    "Group_Excel=pd.read_excel(UserDataPath+\"UserDataPart3.xlsx\", sheet_name ='groups')\n",
    "\n",
    "techOn_groups=Group_Excel.iloc[:,1:].stack().reset_index(drop=True)\n",
    "techOn_groups=techOn_groups.unique()\n",
    "\n",
    "tech_groups_sql=pd.DataFrame({\"tech\":techOn_groups,\"notes\":\"\"})\n",
    "groups_sql=pd.DataFrame({\"group_name\":Group_Excel[\"Group Name\"],\"notes\":\"\"})\n",
    "MaxCapacityGroup_sql  = pd.read_excel(UserDataPath+\"UserDataPart3.xlsx\", sheet_name ='MaxCapacityGroup')\n",
    "\n",
    "\n",
    "TechGroupWeight_sql=pd.DataFrame(columns=[\"regions\",\"tech\",\"group_name\",\"weight\",\"tech_desc\"])\n",
    "#TechGroupWeight_sql\n",
    "for group_id in range(Group_Excel.shape[0]):\n",
    "    group=Group_Excel.iloc[group_id,0]\n",
    "    for tech_id in range(Group_Excel.shape[1]-1):\n",
    "        tech=Group_Excel.iloc[group_id,tech_id+1]\n",
    "        if isinstance(tech,str)==1:\n",
    "            #Get all istances of regions for these technologies\n",
    "            unique_regions=Efficiency_sql.loc[Efficiency_sql[\"tech\"]==tech,\"regions\"].unique()\n",
    "            TechGroupWeight_tmp_sql=pd.DataFrame({\"regions\":unique_regions,\"tech\":tech,\"group_name\":group,\"weight\":1,\"tech_desc\":\"\"})    \n",
    "\n",
    "            TechGroupWeight_sql=pd.concat([TechGroupWeight_sql,TechGroupWeight_tmp_sql],ignore_index=True)\n",
    "\n",
    "                                           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon Capture Technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geologic sequestration is not economically or technically feasible within North Carolina (Carbon Capture, Pipeline and Storage:A Viable Option for North Carolina Utilities?)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empty or very small dfs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty dataframes\n",
    "CapacityFactorTech_sql=pd.DataFrame(columns=[\"regions\",\"season_name\",\"time_of_day_name\",\"tech\",\"cf_tech\",\"cf_tech_notes\"])\n",
    "GrowthRateMax_sql=pd.DataFrame(columns=[\"regions\",\"tech\",\"growthrate_max\",\"growthrate_max_notes\"])\n",
    "GrowthRateSeed_sql=pd.DataFrame(columns=[\"regions\",\"tech\",\"growthrate_seed\",\"growthrate_seed_units\",\"growthrate_seed_notes\"])\n",
    "LifetimeProcess_sql=pd.DataFrame(columns=[\"regions\",\"tech\",\"vintage\",\"life_process\",\"life_process_notes\"])\n",
    "\n",
    "tech_new_cluster_sql=pd.DataFrame(columns=[\"regions\",\"tech\",\"cap_size\"])\n",
    "MinGenGroupTarget_sql=pd.DataFrame(columns=[\"periods\",\"group_name\",\"min_act_g\",\"notes\"])\n",
    "MinGenGroupWeight_sql=pd.DataFrame(columns=[\"regions\",\"tech\",\"group_name\",\"act_fraction\",\"tech_desc\"])\n",
    "MyopicBaseyear_sql=pd.DataFrame(columns=[\"year\"])\n",
    "TechOutputSplit_sql=pd.DataFrame(columns=[\"regions\",\"periods\",\"tech\",\"output_comm\",\"to_split\",\"to_split_notes\"])\n",
    "time_renewable_sql=pd.DataFrame(columns=[\"Field1\"])\n",
    "tech_variable_sql=pd.DataFrame(columns=[\"tech\",\"notes\"])\n",
    "tech_flex_sql=pd.DataFrame(columns=[\"tech\",\"notes\"])\n",
    "tech_annual_sql=pd.DataFrame(columns=[\"tech\",\"notes\"])\n",
    "StorageInit_sql=pd.DataFrame(columns=[\"storage_tech\",\"storage_tech_note\"])\n",
    "tech_ramping_sql=pd.DataFrame(columns=[\"tech\"]) #According to REeds we have a ramping cap above 100%/hr for all technologies. So this term will be ignored\n",
    "RampDown_sql=pd.DataFrame(columns=[\"regions\",\"tech\",\"ramp_down\"])\n",
    "RampUp_sql=pd.DataFrame(columns=[\"regions\",\"tech\",\"ramp_up\"])\n",
    "TechInputSplit_sql=pd.DataFrame(columns=[\"regions\",\"periods\",\"input_comm\",\"tech\",\"ti_split\",\"ti_split_notes\"])\n",
    "TechInputSplitAverage_sql=pd.DataFrame(columns=[\"regions\",\"periods\",\"input_comm\",\"tech\",\"ti_split\",\"ti_split_notes\"])\n",
    "LinkedTechs_sql=pd.DataFrame(columns=[\"primary_region\",\"primary_tech\",\"emis_comm\",\"linked_tech\",\"linked_tech_notes\"])\n",
    "\n",
    "#outputs\n",
    "Output_CapacityByPeriodAndTech_sql=pd.DataFrame(columns=[\"regions\",\"scenario\",\"sector\",\"t_periods\",\"tech\",\"capacity\"])\n",
    "Output_Costs_sql=pd.DataFrame(columns=[\"regions\",\"scenario\",\"sector\",\"output_name\",\"tech\",\"vintage\",\"output_cost\"])\n",
    "Output_Curtailment_sql=pd.DataFrame(columns=[\"regions\",\"scenario\",\"sector\",\"t_periods\",\"t_season\",\"t_day\",\"input_comm\",\"tech\",\"vintage\",\"output_comm\",\"curtailment\"])\n",
    "Output_Duals_sql=pd.DataFrame(columns=[\"constraint_name\",\"scenario\",\"dual\"])\n",
    "Output_Emissions_sql=pd.DataFrame(columns=[\"regions\",\"scenario\",\"sector\",\"t_periods\",\"emissions_comm\",\"tech\",\"vintage\",\"emissions\"])\n",
    "Output_Objective_sql=pd.DataFrame(columns=[\"scenario\",\"objective_name\",\"total_system_cost\"])\n",
    "Output_VFlow_In_sql=pd.DataFrame(columns=[\"regions\",\"scenario\",\"sector\",\"t_periods\",\"t_season\",\"t_day\",\"input_comm\",\"tech\",\"vintage\",\"output_comm\",\"vflow_in\"])\n",
    "Output_VFlow_Out_sql=pd.DataFrame(columns=[\"regions\",\"scenario\",\"sector\",\"t_periods\",\"t_season\",\"t_day\",\"input_comm\",\"tech\",\"vintage\",\"output_comm\",\"vflow_out\"])\n",
    "Output_V_Capacity_sql=pd.DataFrame(columns=[\"regions\",\"scenario\",\"sector\",\"tech\",\"vintage\",\"capacity\"])\n",
    "\n",
    "\n",
    "#Very simple dataframes\n",
    "GlobalDiscountRate_sql=pd.DataFrame([0.05],columns=[\"rate\"])\n",
    "regions_sql=pd.DataFrame({\"regions\": RegionsDefinition, \"region_note\":RegionNotes})\n",
    "\n",
    "#Directly from the excel file\n",
    "EmissionLimit_sql  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='EmissionLimit')\n",
    "PlanningReserveMargin_sql  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='PlanningReserveMargin',usecols='A,B')\n",
    "tech_curtailment_sql  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='tech_curtailment')\n",
    "MaxResource_sql  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='MaxResource')\n",
    "\n",
    "MaxCapacity_sql  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='MaxCapacity')\n",
    "MinCapacity_sql  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='MinCapacity')\n",
    "MaxActivity_sql  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='MaxActivity')\n",
    "MinActivity_sql  = pd.read_excel(UserDataPath+\"UserDataPart2.xlsx\", sheet_name ='MinActivity')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For df coming directly from excel remove years not in the model\n",
    "MaxCapacityGroup_sql=MaxCapacityGroup_sql[MaxCapacityGroup_sql[\"periods\"].isin(FutureYears[0:-1])]\n",
    "EmissionLimit_sql=EmissionLimit_sql[EmissionLimit_sql[\"periods\"].isin(FutureYears[0:-1])]\n",
    "MaxActivity_sql=MaxActivity_sql[MaxActivity_sql[\"periods\"].isin(FutureYears[0:-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajust time_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control the number of hours in the day to reduce memory usage\n",
    "\n",
    "#if NumHoursAjust!=24:\n",
    "time_of_day_sql=pd.DataFrame({\"t_day\":list(NumHoursAjust.keys())})\n",
    "SegFrac_sql_old=SegFrac_sql.copy(deep=True)\n",
    "SegFrac_sql=pd.DataFrame(columns=[\"season_name\",\"time_of_day_name\",\"segfrac\",\"segfrac_notes\"])\n",
    "\n",
    "DemandSpecificDistribution_sql_old=DemandSpecificDistribution_sql.copy(deep=True)\n",
    "DemandSpecificDistribution_sql=pd.DataFrame(columns=DemandSpecificDistribution_sql_old.columns)\n",
    "\n",
    "CapacityFactorProcess_sql_old=CapacityFactorProcess_sql.copy(deep=True)\n",
    "CapacityFactorProcess_sql=pd.DataFrame(columns=CapacityFactorProcess_sql_old.columns)\n",
    "\n",
    "DataSegFrac_sql=[]\n",
    "\n",
    "for time_name in list(NumHoursAjust.keys()):\n",
    "    TimesToAggregate=NumHoursAjust[time_name]\n",
    "\n",
    "# SegFrac_sql\n",
    "    for s in SegFrac_sql_old[\"season_name\"].unique():\n",
    "        new_frac=SegFrac_sql_old[(SegFrac_sql_old[\"season_name\"]==s)*([t in TimesToAggregate for t in SegFrac_sql_old[\"time_of_day_name\"]])][\"segfrac\"].sum()\n",
    "        DataSegFrac_sql.append([s,time_name,new_frac,\"\"])\n",
    "\n",
    "#DemandSpecificDistribution_sql\n",
    "    Dsd_tmp=DemandSpecificDistribution_sql_old[[t in TimesToAggregate for t in DemandSpecificDistribution_sql_old[\"time_of_day_name\"]]].groupby([\"regions\",\"season_name\",\"demand_name\"])[\"dds\"].sum().reset_index()\n",
    "    Dsd_tmp[\"time_of_day_name\"]=time_name\n",
    "    Dsd_tmp[\"dds_notes\"]=DemandSpecificDistribution_sql_old[[t in TimesToAggregate for t in DemandSpecificDistribution_sql_old[\"time_of_day_name\"]]][\"dds_notes\"].values[0]\n",
    "    DemandSpecificDistribution_sql=pd.concat([DemandSpecificDistribution_sql,Dsd_tmp],ignore_index=True)\n",
    "   \n",
    "    #CapacityFactorTech_sql (not used in the model)\n",
    "\n",
    "    #CapacityFactorProcess_sql\n",
    "    CFP_tmp=CapacityFactorProcess_sql_old[[t in TimesToAggregate for t in CapacityFactorProcess_sql_old[\"time_of_day_name\"]]].groupby([\"regions\",\"season_name\",\"tech\",\"vintage\"])[\"cf_process\"].mean().reset_index()\n",
    "    CFP_tmp[\"time_of_day_name\"]=time_name\n",
    "    CapacityFactorProcess_sql=pd.concat([CapacityFactorProcess_sql,CFP_tmp],ignore_index=True)\n",
    "\n",
    "SegFrac_sql=pd.DataFrame(DataSegFrac_sql,columns=[\"season_name\",\"time_of_day_name\",\"segfrac\",\"segfrac_notes\"]).sort_values([\"season_name\",\"time_of_day_name\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust Variables for Numerical Stability and CPI Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust for CPI\n",
    "CostInvest_sql[\"cost_invest\"]=CostInvest_sql[\"cost_invest\"]*ConversionCPIRate\n",
    "CostFixed_sql[\"cost_fixed\"]=CostFixed_sql[\"cost_fixed\"]*ConversionCPIRate\n",
    "CostVariable_sql[\"cost_variable\"]=CostVariable_sql[\"cost_variable\"]*ConversionCPIRate\n",
    "\n",
    "#round values to ensure numerical stability\n",
    "CapacityCredit_sql[\"cf_tech\"]=round(CapacityCredit_sql[\"cf_tech\"],3)\n",
    "CapacityFactorProcess_sql[\"cf_process\"]=round(CapacityFactorProcess_sql[\"cf_process\"],3)\n",
    "CapacityFactorTech_sql[\"cf_tech\"]=round(CapacityFactorTech_sql[\"cf_tech\"],3)\n",
    "CapacityToActivity_sql[\"c2a\"]=round(CapacityToActivity_sql[\"c2a\"],3)\n",
    "CostFixed_sql[\"cost_fixed\"]=round(CostFixed_sql[\"cost_fixed\"],3)\n",
    "CostInvest_sql[\"cost_invest\"]=round(CostInvest_sql[\"cost_invest\"],3)\n",
    "CostVariable_sql[\"cost_variable\"]=round(CostVariable_sql[\"cost_variable\"],3)\n",
    "Demand_sql[\"demand\"]=round(Demand_sql[\"demand\"],4)\n",
    "DemandSpecificDistribution_sql[\"dds\"]=round(DemandSpecificDistribution_sql[\"dds\"],6)\n",
    "DiscountRate_sql[\"tech_rate\"]=round(DiscountRate_sql[\"tech_rate\"],3)\n",
    "Efficiency_sql[\"efficiency\"]=Efficiency_sql[\"efficiency\"].astype(float).round(3)\n",
    "EmissionActivity_sql[\"emis_act\"]=EmissionActivity_sql[\"emis_act\"].astype(float).round(3)\n",
    "EmissionLimit_sql[\"emis_limit\"]=EmissionLimit_sql[\"emis_limit\"].astype(float).round(3)\n",
    "ExistingCapacity_sql[\"exist_cap\"]=ExistingCapacity_sql[\"exist_cap\"].astype(float).round(4)\n",
    "GlobalDiscountRate_sql[\"rate\"]=GlobalDiscountRate_sql[\"rate\"].astype(float).round(3)\n",
    "GrowthRateMax_sql[\"growthrate_max\"]=GrowthRateMax_sql[\"growthrate_max\"].astype(float).round(4)\n",
    "GrowthRateSeed_sql[\"growthrate_seed\"]=GrowthRateSeed_sql[\"growthrate_seed\"].astype(float).round(4)\n",
    "MaxActivity_sql[\"maxact\"]=MaxActivity_sql[\"maxact\"].astype(float).round(4)\n",
    "MaxCapacity_sql[\"maxcap\"]=MaxCapacity_sql[\"maxcap\"].astype(float).round(4)\n",
    "MaxResource_sql[\"maxres\"]=MaxResource_sql[\"maxres\"].astype(float).round(4)\n",
    "MinActivity_sql[\"minact\"]=MinActivity_sql[\"minact\"].astype(float).round(4)\n",
    "MinCapacity_sql[\"mincap\"]=MinCapacity_sql[\"mincap\"].astype(float).round(4)\n",
    "PlanningReserveMargin_sql[\"reserve_margin\"]=PlanningReserveMargin_sql[\"reserve_margin\"].astype(float).round(3)\n",
    "#SegFrac_sql[\"segfrac\"]=SegFrac_sql[\"segfrac\"].astype(float).round(6)\n",
    "MaxCapacityGroup_sql[\"max_cap_g\"]=MaxCapacityGroup_sql[\"max_cap_g\"].astype(float).round(4)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sql and sqlite files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DicOfDfs={\n",
    "\"CapacityCredit\":CapacityCredit_sql,\n",
    "\"CapacityFactorProcess\":CapacityFactorProcess_sql,\n",
    "\"CapacityFactorTech\":CapacityFactorTech_sql,\n",
    "\"CapacityToActivity\":CapacityToActivity_sql,\n",
    "\"CostFixed\":CostFixed_sql,\n",
    "\"CostInvest\":CostInvest_sql,\n",
    "\"CostVariable\":CostVariable_sql,\n",
    "\"Demand\":Demand_sql, \n",
    "\"DemandSpecificDistribution\":DemandSpecificDistribution_sql,\n",
    "\"DiscountRate\":DiscountRate_sql,\n",
    "\"Efficiency\":Efficiency_sql,\n",
    "\"EmissionActivity\":EmissionActivity_sql,\n",
    "\"EmissionLimit\":EmissionLimit_sql,\n",
    "\"ExistingCapacity\":ExistingCapacity_sql,\n",
    "\"GlobalDiscountRate\":GlobalDiscountRate_sql,\n",
    "\"GrowthRateMax\":GrowthRateMax_sql,\n",
    "\"GrowthRateSeed\":GrowthRateSeed_sql,\n",
    "\"LifetimeLoanTech\":LifetimeLoanTech_sql,\n",
    "\"LifetimeProcess\":LifetimeProcess_sql,\n",
    "\"LifetimeTech\":LifetimeTech_sql,\n",
    "\"LinkedTechs\":LinkedTechs_sql,\n",
    "\"MaxActivity\":MaxActivity_sql,\n",
    "\"MaxCapacity\":MaxCapacity_sql,\n",
    "\"MaxResource\":MaxResource_sql,\n",
    "\"MinActivity\":MinActivity_sql,\n",
    "\"MinCapacity\":MinCapacity_sql,\n",
    "\"MinGenGroupTarget\":MinGenGroupTarget_sql,\n",
    "\"MinGenGroupWeight\":MinGenGroupWeight_sql,\n",
    "\"MyopicBaseyear\":MyopicBaseyear_sql,\n",
    "\"Output_CapacityByPeriodAndTech\":Output_CapacityByPeriodAndTech_sql,\n",
    "\"Output_Costs\":Output_Costs_sql,\n",
    "\"Output_Curtailment\":Output_Curtailment_sql,\n",
    "\"Output_Duals\":Output_Duals_sql,\n",
    "\"Output_Emissions\":Output_Emissions_sql,\n",
    "\"Output_Objective\":Output_Objective_sql,\n",
    "\"Output_VFlow_In\":Output_VFlow_In_sql,\n",
    "\"Output_VFlow_Out\":Output_VFlow_Out_sql,\n",
    "\"Output_V_Capacity\":Output_V_Capacity_sql,\n",
    "\"PlanningReserveMargin\":PlanningReserveMargin_sql,\n",
    "\"RampDown\":RampDown_sql,\n",
    "\"RampUp\":RampUp_sql,\n",
    "\"SegFrac\":SegFrac_sql,\n",
    "\"StorageDuration\":StorageDuration_sql,\n",
    "\"StorageInit\":StorageInit_sql,\n",
    "\"TechInputSplit\":TechInputSplit_sql,\n",
    "\"TechInputSplitAverage\":TechInputSplitAverage_sql,\n",
    " \"TechOutputSplit\":TechOutputSplit_sql,\n",
    "\"commodities\":commodities_sql,\n",
    "\"commodity_labels\":commodity_labels_sql,\n",
    "\"groups\":groups_sql,\n",
    "\"regions\":regions_sql,\n",
    "\"sector_labels\":sector_labels_sql,\n",
    "\"tech_annual\":tech_annual_sql,\n",
    "\"tech_curtailment\":tech_curtailment_sql,\n",
    "\"tech_exchange\":tech_exchange_sql,\n",
    "\"tech_flex\":tech_flex_sql,\n",
    "\"tech_groups\":tech_groups_sql,\n",
    "\"tech_new_cluster\":tech_new_cluster_sql,\n",
    "\"tech_ramping\":tech_ramping_sql,\n",
    "\"tech_reserve\":tech_reserve_sql,\n",
    "\"tech_variable\":tech_variable_sql,\n",
    "\"technologies\":technologies_sql,\n",
    "\"technology_labels\":technology_labels_sql,\n",
    "\"time_of_day\":time_of_day_sql,\n",
    "\"time_period_labels\":time_period_labels_sql,\n",
    "\"time_periods\":time_periods_sql,\n",
    "\"time_renewable\":time_renewable_sql,\n",
    "\"time_season\":time_season_sql,\n",
    "\"MaxCapacityGroup\": MaxCapacityGroup_sql, \n",
    "\"TechGroupWeight\":TechGroupWeight_sql}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(\"OutputData/\"+SaveSqlName+\".sqlite\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "!sqlite3 OutputData/ReferenceSql.sqlite < InputData/ReferenceSql.sql\n",
    "os.rename(\"OutputData/ReferenceSql.sqlite\", \"./OutputData/\"+SaveSqlName+\".sqlite\")\n",
    "\n",
    "#Insert data\n",
    "engine = create_engine('sqlite:///'+\"OutputData/\"+SaveSqlName+\".sqlite\", echo=False)\n",
    "NamesDic=list(DicOfDfs.keys())\n",
    "for i in range(len(NamesDic)):\n",
    "    DicOfDfs[NamesDic[i]]=DicOfDfs[NamesDic[i]].reset_index(drop=True) #remove index\n",
    "    DicOfDfs[NamesDic[i]].to_sql(NamesDic[i], con=engine, if_exists='append', index=False)\n",
    "    print(NamesDic[i]+\" saved\")\n",
    "\n",
    "con = sqlite3.connect(\"OutputData/\"+SaveSqlName+\".sqlite\")\n",
    "with open(\"OutputData/\"+SaveSqlName+\".sql\", 'w') as f:\n",
    "    for line in con.iterdump():\n",
    "        f.write('%s\\n' % line)\n",
    "con.close()\n",
    "# engine.dispose(close=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save hurricane speed statistics and damage for each technology/region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Speed data from Hazus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ExistingGeneratorData=df_AggregateGen.copy()\n",
    "df_ExistingGeneratorData.drop(columns=[\"NetSummerCapacity(MW)\",\"NetWinterCapacity(MW)\",\"Technology\"],inplace=True)\n",
    "df_ExistingGeneratorData[\"tech\"]=\"\"\n",
    "\n",
    "for i in range(len(df_ExistingGeneratorData)):\n",
    "    if df_ExistingGeneratorData.loc[i,\"MoverCode\"]!=\"BA\":\n",
    "        df_ExistingGeneratorData.loc[i,\"tech\"]=df_ExistingGeneratorData.loc[i,\"SourceCode\"]+\"_\"+df_ExistingGeneratorData.loc[i,\"MoverCode\"]+\"_EXISTING\"\n",
    "    else:\n",
    "        HoursOfOperation=df_ExistingGeneratorData.loc[i,\"NameplateEnergyCapacity(MWh)\"]/df_ExistingGeneratorData.loc[i,\"NameplateCapacity(MW)\"] #h operation of each battery\n",
    "        TmpBrack_L=np.array([0, 1, 2 ,4, 6, 8 ])\n",
    "        TmpBrack_U=np.array([1, 2 ,4, 6, 8 ,10])\n",
    "        \n",
    "        IdxInBatteryH=(HoursOfOperation>TmpBrack_L) * (HoursOfOperation<=TmpBrack_U)\n",
    "        h_battery=TmpBrack_U[IdxInBatteryH][0]\n",
    "        if sum(IdxInBatteryH)!=0:\n",
    "            df_ExistingGeneratorData.loc[i,\"NameplateCapacity(MW)\"]=df_ExistingGeneratorData.loc[i,\"NameplateEnergyCapacity(MWh)\"]/h_battery #MW\n",
    "            df_ExistingGeneratorData.loc[i,\"tech\"]=df_ExistingGeneratorData.loc[i,\"SourceCode\"]+\"_\"+df_ExistingGeneratorData.loc[i,\"MoverCode\"]+str(h_battery)+\"H_EXISTING\"\n",
    "\n",
    "df_ExistingGeneratorData.drop(columns=[\"NameplateEnergyCapacity(MWh)\"],inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existing Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_AggregateGen: Contains information about existing generation location (lat, long and region) and capacity\n",
    "#Assign a HAZUS region to existing each generator\n",
    "ExistingGenLatLong=[Point(df_ExistingGeneratorData.at[i,\"Longitude\"],df_ExistingGeneratorData.at[i,\"Latitude\"]) for i in range(len(df_ExistingGeneratorData))]\n",
    "IdxGenerator2Hazus=[]\n",
    "\n",
    "for i in tqdm(range(len(ExistingGenLatLong))):\n",
    "\n",
    "    flag=0\n",
    "    for p in range (len(shapefile_Hazus)):\n",
    "        \n",
    "        if shapefile_Hazus.iloc[p].geometry.contains(ExistingGenLatLong[i]):\n",
    "            IdxGenerator2Hazus.append(p)\n",
    "            flag=1\n",
    "\n",
    "    if flag==0:\n",
    "        IdxGenerator2Hazus.append(-1) #if no match found, append -1\n",
    "\n",
    "if sum(np.array(IdxGenerator2Hazus)==-1)>0:\n",
    "    print(\"Warning: \"+str(sum(np.array(IdxGenerator2Hazus)==-1))+\" generators are not located in the region of interest\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "for new_column in [\"f10yr\",\"f20yr\",\"f50yr\",\"f100yr\",\"f200yr\",\"f500yr\",\"f1000yr\"]:   \n",
    "    df_ExistingGeneratorData[new_column]=shapefile_Hazus.loc[IdxGenerator2Hazus,new_column].reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FutureGenerators=CostInvest_sql.groupby([\"regions\",\"tech\"]).size().reset_index().rename(columns={0:'Occurances'}) \n",
    "df_FutureGenerators=df_FutureGenerators.drop(columns=[\"Occurances\"])\n",
    "df_FutureGenerators=df_FutureGenerators.reset_index(drop=True)\n",
    "\n",
    "WindSpeed_df={\"R1\": Region1Data, \"R2\": Region2Data, \"R3\": Region3Data}\n",
    "\n",
    "for new_column in [\"f10yr\",\"f20yr\",\"f50yr\",\"f100yr\",\"f200yr\",\"f500yr\",\"f1000yr\"]:   \n",
    "    df_FutureGenerators[new_column]=-1\n",
    "    df_FutureGenerators[new_column]=df_FutureGenerators[new_column].astype(object)\n",
    "    for region in RegionsDefinition:\n",
    "        WS=np.array(WindSpeed_df[region][new_column])\n",
    "\n",
    "        IdxIn=df_FutureGenerators[\"regions\"]==region\n",
    "        for i in np.where(IdxIn)[0]:\n",
    "            df_FutureGenerators.at[i, new_column] = pd.Series(WS)\n",
    "        \n",
    "#Update Technologies With specific locations\n",
    "ExcelLocations=pd.read_excel(FragilityCurvesDataPath+\"SpecialLocationFutureTech.xlsx\")\n",
    "\n",
    "#Centroids of Hazus regions\n",
    "CenLong=shapefile_Hazus.CenLongit\n",
    "CenLat=shapefile_Hazus.CenLat\n",
    "\n",
    "for row in ExcelLocations.iterrows():\n",
    "    region=row[1][\"Region\"]\n",
    "    tech=row[1][\"Tech\"]\n",
    "    \n",
    "    Location=row[1][\"Location\"]\n",
    "    Location=Location.replace('(', '')\n",
    "    Location=Location.replace(')', '')\n",
    "\n",
    "    if Location.find('+')!=-1:\n",
    "        WS=[]\n",
    "        for Concat_region in Location.split('+'):  \n",
    "            #Concatenate speeds of two regions\n",
    "            WS=WS+list(WindSpeed_df[Concat_region][new_column])\n",
    "\n",
    "        IdxIn=(df_FutureGenerators[\"regions\"]==region)*(df_FutureGenerators[\"tech\"]==tech)\n",
    "        for i in np.where(IdxIn)[0]:\n",
    "\n",
    "            for new_column in [\"f10yr\",\"f20yr\",\"f50yr\",\"f100yr\",\"f200yr\",\"f500yr\",\"f1000yr\"]: \n",
    "                df_FutureGenerators.at[i, new_column] = pd.Series(np.array(WS))\n",
    "\n",
    "    else:#Add a series of wind speed from each point reported in the excel file\n",
    "        Location=Location.split(';')\n",
    "        PointsLoc=[[float(Location[i].split(',')[1]),float(Location[i].split(',')[0])] for i in range(len(Location))]\n",
    "\n",
    "        IdxIn=(df_FutureGenerators[\"regions\"]==region)*(df_FutureGenerators[\"tech\"]==tech)\n",
    "        IdxIn=IdxIn.where(IdxIn==True).dropna().index[0]\n",
    "\n",
    "        for new_column in [\"f10yr\",\"f20yr\",\"f50yr\",\"f100yr\",\"f200yr\",\"f500yr\",\"f1000yr\"]: \n",
    "            WS=[]\n",
    "            for LongLat in PointsLoc:\n",
    "                PointLong=LongLat[0]\n",
    "                PointLat=LongLat[1]\n",
    "                IdxClosestRegion=np.argmin(np.sqrt((CenLong-PointLong)**2+(CenLat-PointLat)**2))\n",
    "                WS.append(shapefile_Hazus.iloc[IdxClosestRegion][new_column])\n",
    "\n",
    "            df_FutureGenerators.at[IdxIn, new_column] = pd.Series(np.array(WS))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Probability of Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expected number of events given at least one event happened\n",
    "def ExpectedNumberOfEvents(Probabilities=[1/100, 1/200, 1/500 ,1/1000], NumYears=5):\n",
    "\n",
    "    c=0\n",
    "    for p in Probabilities:\n",
    "        \n",
    "        if c==0:\n",
    "            SampleEvents=np.random.choice(a=[True,False], size=(1000000,NumYears), p=[p, 1-p]) \n",
    "        \n",
    "        if c!=0:\n",
    "            SampleEvents=SampleEvents+np.random.choice(a=[True,False], size=(1000000,NumYears), p=[p, 1-p]) \n",
    "        c=c+1\n",
    "\n",
    "    NumberOfEvent_GivenAtLeastOne=np.sum(SampleEvents[SampleEvents.sum(axis=1)>=1,:], axis=1)\n",
    "    ExpectedNumberOfEvents_GivenAtLeastOne=np.mean(NumberOfEvent_GivenAtLeastOne)\n",
    "\n",
    "    AtLeastOneProbability=1-np.prod(np.power(1-np.array(Probabilities),NumYears))\n",
    "\n",
    "    #AtLeastOneProbability: Probability that at least one event happens in the interval\n",
    "    #ExpectedNumberOfEvents_GivenAtLeastOne: Value to multiply the expected damage per event\n",
    "    return AtLeastOneProbability, ExpectedNumberOfEvents_GivenAtLeastOne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For existing generators is the weighted average (by capacity) of the all technologies in the region\n",
    "# For future generators is the mean of damage caused by all wind speeds assigned to the technology in the region\n",
    "TechFragilityName=pd.read_excel(FragilityCurvesDataPath+\"Tech2FragilityCurve.xlsx\")\n",
    "\n",
    "PercentageHurricaneDamage=pd.DataFrame(columns=[\"regions\",\"tech\",\"f10yr\",\"f20yr\",\"f50yr\",\"f100yr\",\"f200yr\",\"f500yr\",\"f1000yr\"])\n",
    "\n",
    "SkeletonOfRegionsandTech=Efficiency_sql.groupby([\"regions\",\"tech\"]).size().reset_index().rename(columns={0:'Occurances'}) \n",
    "SkeletonOfRegionsandTech=SkeletonOfRegionsandTech.drop(columns=[\"Occurances\"])\n",
    "\n",
    "PercentageHurricaneDamage[\"regions\"]=SkeletonOfRegionsandTech[\"regions\"]\n",
    "PercentageHurricaneDamage[\"tech\"]=SkeletonOfRegionsandTech[\"tech\"]\n",
    "\n",
    "#existing generators\n",
    "for tech in df_ExistingGeneratorData[\"tech\"].unique():\n",
    "    FCName=TechFragilityName.loc[TechFragilityName[\"Technologies\"]==tech,\"Fragility Curve Name\"].values[0] #name of the fragility curve\n",
    "    IdxIn=df_ExistingGeneratorData[\"tech\"]==tech\n",
    "\n",
    "    #Get all generators with the same tech name in the same region\n",
    "    for region in df_ExistingGeneratorData.loc[IdxIn,\"Region\"].unique():\n",
    "        IdxIn2=(df_ExistingGeneratorData[\"tech\"]==tech)*(df_ExistingGeneratorData[\"Region\"]==region)\n",
    "        df_temp=df_ExistingGeneratorData.loc[IdxIn2,:]\n",
    "\n",
    "        IdxInProb_df=(PercentageHurricaneDamage[\"tech\"]==tech)*(PercentageHurricaneDamage[\"regions\"]==region)\n",
    "        for recurrency in [\"f10yr\",\"f20yr\",\"f50yr\",\"f100yr\",\"f200yr\",\"f500yr\",\"f1000yr\"]:\n",
    "            P_damage_eachCapacity=fragility(df_temp[recurrency].values,FCName)\n",
    "            WeightedAverage=np.sum(P_damage_eachCapacity*df_temp[\"NameplateCapacity(MW)\"].values)/np.sum(df_temp[\"NameplateCapacity(MW)\"].values)\n",
    "            PercentageHurricaneDamage.loc[IdxInProb_df,recurrency]=WeightedAverage\n",
    "            \n",
    "#future generators\n",
    "for row in df_FutureGenerators.iterrows():\n",
    "    region=row[1][\"regions\"]\n",
    "    tech=row[1][\"tech\"]\n",
    "    FCName=TechFragilityName.loc[TechFragilityName[\"Technologies\"]==tech,\"Fragility Curve Name\"].values[0] #name of the fragility curve\n",
    "\n",
    "    IdxInProb_df=(PercentageHurricaneDamage[\"tech\"]==tech)*(PercentageHurricaneDamage[\"regions\"]==region)\n",
    "    for recurrency in [\"f10yr\",\"f20yr\",\"f50yr\",\"f100yr\",\"f200yr\",\"f500yr\",\"f1000yr\"]:\n",
    "        WSpeed=row[1][recurrency]\n",
    "        AverageDamage=np.average(fragility(WSpeed,FCName))\n",
    "        PercentageHurricaneDamage.loc[IdxInProb_df,recurrency]=AverageDamage\n",
    "\n",
    "\n",
    "\n",
    "#PercentageHurricaneDamage.to_excel(FragilityCurvesDataPath+\"PercentageHurricaneDamage.xlsx\",index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign damage to appropriate periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenarios were defined at the beginning of the script\n",
    "Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YearsOptimization=FutureYears[:-1]\n",
    "ProbScenario=pd.DataFrame(columns=[\"t_periods\"]+list(Scenarios.keys()))\n",
    "ProbScenario[\"t_periods\"]=YearsOptimization\n",
    "\n",
    "AvgNumEvents=pd.DataFrame(columns=[\"t_periods\"]+list(Scenarios.keys()))\n",
    "AvgNumEvents[\"t_periods\"]=YearsOptimization\n",
    "\n",
    "NumYearsEachPeriod=np.array(FutureYears[1:])-np.array(FutureYears[:-1])\n",
    "\n",
    "ProbHazus=np.array([1/10, 1/20, 1/50, 1/100, 1/200, 1/500, 1/1000])\n",
    "ProbHazus_tags=[\"f10yr\",\"f20yr\",\"f50yr\",\"f100yr\",\"f200yr\",\"f500yr\",\"f1000yr\"]\n",
    "\n",
    "for i_scenario in list(Scenarios.keys()): \n",
    "    \n",
    "    PercentageHurricaneDamage[i_scenario]=-1 # Placeholder\n",
    "    HazusCodes=Scenarios[i_scenario][\"HazusCodes\"] # Hazus codes inside the scenario\n",
    "    Idx=[ProbHazus_tags.index(HazusCodes[i]) for i in range(len(HazusCodes))]\n",
    "    ProbHazus_Scenario=ProbHazus[Idx]#Array with the probabilities of each Hazus code in the scenario\n",
    "    df_tmp=PercentageHurricaneDamage.iloc[:,2:9] #F10yr, F20yr, F50yr, F100yr, F200yr, F500yr, F1000yr\n",
    "    df_tmp=df_tmp.iloc[:,Idx]#Filter for the scenarios of interest\n",
    "    \n",
    "    #To get the weight average\n",
    "    df_tmp=df_tmp*ProbHazus_Scenario #Multiply by the probability of each Hazus code in the scenario (damage x probability)\n",
    "    DenominatorProb=np.sum(ProbHazus_Scenario)\n",
    "\n",
    "    PercentageHurricaneDamage.loc[:, i_scenario]=np.sum(df_tmp,axis=1)/DenominatorProb\n",
    "    \n",
    "    for i in range(len(YearsOptimization)):\n",
    "        ProbAtScenario, NumEvents=ExpectedNumberOfEvents(Probabilities=ProbHazus_Scenario, NumYears=NumYearsEachPeriod[i])\n",
    "        ProbScenario.at[i,i_scenario]=ProbAtScenario\n",
    "        AvgNumEvents.at[i,i_scenario]=NumEvents.round(2)\n",
    "\n",
    "#Check if the sum of the probabilities is >1\n",
    "\n",
    "if np.sum(np.sum(ProbScenario.iloc[:,1:-1],axis=1)>1):\n",
    "    print(\"Due to the length of simulation peridos and High probability of low recurrency events (e.g 10y) the sum of the probabilities is >1 for at least on sim.period\")\n",
    "    print(\"Remove small recurrencies 10y, 20y and consider tehm as part of no damage scenario if appropriate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in list(PercentageHurricaneDamage.columns[2:]):#round to 4 decimals\n",
    "    PercentageHurricaneDamage[l]=PercentageHurricaneDamage[l].astype(float).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PercentageHurricaneDamage.to_excel(FragilityCurvesDataPath+\"PercentageHurricaneDamage.xlsx\",index=False)\n",
    "ProbScenario.to_excel(FragilityCurvesDataPath+\"ProbabilityEachScenario.xlsx\",index=False)\n",
    "AvgNumEvents.to_excel(FragilityCurvesDataPath+\"AverageNumberOfEventsPerScenario.xlsx\",index=False)\n",
    "\n",
    "PercentageHurricaneDamage.to_excel(\"./OutputData/PercentageHurricaneDamage.xlsx\",index=False)\n",
    "ProbScenario.to_excel(\"./OutputData/ProbabilityEachScenario.xlsx\",index=False)\n",
    "AvgNumEvents.to_excel(\"./OutputData/AverageNumberOfEventsPerScenario.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Temoa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
